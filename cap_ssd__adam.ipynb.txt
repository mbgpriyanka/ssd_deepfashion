{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cap_ssd_pferrari_adam_trial_copy.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_icgK6HpxWa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# trials with new annotations - 1,2 or 3 category class ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngkpC1-ZEZE4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import Adam, SGD\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TerminateOnNaN, CSVLogger\n",
        "from keras import backend as K\n",
        "from keras.models import load_model\n",
        "from math import ceil\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IK1NF3s8EjqN",
        "colab_type": "code",
        "outputId": "8dca8b38-d034-49f3-8f03-f60f9c50fd69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foN3letNGicT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''bounding_box_utils.py\n",
        "Includes:\n",
        "* Function to compute the IoU similarity for axis-aligned, rectangular, 2D bounding boxes\n",
        "* Function for coordinate conversion for axis-aligned, rectangular, 2D bounding boxes\n",
        "Copyright (C) 2018 Pierluigi Ferrari\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "   http://www.apache.org/licenses/LICENSE-2.0\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "'''\n",
        "\n",
        "from __future__ import division\n",
        "import numpy as np\n",
        "\n",
        "def convert_coordinates(tensor, start_index, conversion, border_pixels='half'):\n",
        "    '''\n",
        "    Convert coordinates for axis-aligned 2D boxes between two coordinate formats.\n",
        "    Creates a copy of `tensor`, i.e. does not operate in place. Currently there are\n",
        "    three supported coordinate formats that can be converted from and to each other:\n",
        "        1) (xmin, xmax, ymin, ymax) - the 'minmax' format\n",
        "        2) (xmin, ymin, xmax, ymax) - the 'corners' format\n",
        "        2) (cx, cy, w, h) - the 'centroids' format\n",
        "    Arguments:\n",
        "        tensor (array): A Numpy nD array containing the four consecutive coordinates\n",
        "            to be converted somewhere in the last axis.\n",
        "        start_index (int): The index of the first coordinate in the last axis of `tensor`.\n",
        "        conversion (str, optional): The conversion direction. Can be 'minmax2centroids',\n",
        "            'centroids2minmax', 'corners2centroids', 'centroids2corners', 'minmax2corners',\n",
        "            or 'corners2minmax'.\n",
        "        border_pixels (str, optional): How to treat the border pixels of the bounding boxes.\n",
        "            Can be 'include', 'exclude', or 'half'. If 'include', the border pixels belong\n",
        "            to the boxes. If 'exclude', the border pixels do not belong to the boxes.\n",
        "            If 'half', then one of each of the two horizontal and vertical borders belong\n",
        "            to the boxex, but not the other.\n",
        "    Returns:\n",
        "        A Numpy nD array, a copy of the input tensor with the converted coordinates\n",
        "        in place of the original coordinates and the unaltered elements of the original\n",
        "        tensor elsewhere.\n",
        "    '''\n",
        "    if border_pixels == 'half':\n",
        "        d = 0\n",
        "    elif border_pixels == 'include':\n",
        "        d = 1\n",
        "    elif border_pixels == 'exclude':\n",
        "        d = -1\n",
        "\n",
        "    ind = start_index\n",
        "    tensor1 = np.copy(tensor).astype(np.float)\n",
        "    if conversion == 'minmax2centroids':\n",
        "        tensor1[..., ind] = (tensor[..., ind] + tensor[..., ind+1]) / 2.0 # Set cx\n",
        "        tensor1[..., ind+1] = (tensor[..., ind+2] + tensor[..., ind+3]) / 2.0 # Set cy\n",
        "        tensor1[..., ind+2] = tensor[..., ind+1] - tensor[..., ind] + d # Set w\n",
        "        tensor1[..., ind+3] = tensor[..., ind+3] - tensor[..., ind+2] + d # Set h\n",
        "    elif conversion == 'centroids2minmax':\n",
        "        tensor1[..., ind] = tensor[..., ind] - tensor[..., ind+2] / 2.0 # Set xmin\n",
        "        tensor1[..., ind+1] = tensor[..., ind] + tensor[..., ind+2] / 2.0 # Set xmax\n",
        "        tensor1[..., ind+2] = tensor[..., ind+1] - tensor[..., ind+3] / 2.0 # Set ymin\n",
        "        tensor1[..., ind+3] = tensor[..., ind+1] + tensor[..., ind+3] / 2.0 # Set ymax\n",
        "    elif conversion == 'corners2centroids':\n",
        "        tensor1[..., ind] = (tensor[..., ind] + tensor[..., ind+2]) / 2.0 # Set cx\n",
        "        tensor1[..., ind+1] = (tensor[..., ind+1] + tensor[..., ind+3]) / 2.0 # Set cy\n",
        "        tensor1[..., ind+2] = tensor[..., ind+2] - tensor[..., ind] + d # Set w\n",
        "        tensor1[..., ind+3] = tensor[..., ind+3] - tensor[..., ind+1] + d # Set h\n",
        "    elif conversion == 'centroids2corners':\n",
        "        tensor1[..., ind] = tensor[..., ind] - tensor[..., ind+2] / 2.0 # Set xmin\n",
        "        tensor1[..., ind+1] = tensor[..., ind+1] - tensor[..., ind+3] / 2.0 # Set ymin\n",
        "        tensor1[..., ind+2] = tensor[..., ind] + tensor[..., ind+2] / 2.0 # Set xmax\n",
        "        tensor1[..., ind+3] = tensor[..., ind+1] + tensor[..., ind+3] / 2.0 # Set ymax\n",
        "    elif (conversion == 'minmax2corners') or (conversion == 'corners2minmax'):\n",
        "        tensor1[..., ind+1] = tensor[..., ind+2]\n",
        "        tensor1[..., ind+2] = tensor[..., ind+1]\n",
        "    else:\n",
        "        raise ValueError(\"Unexpected conversion value. Supported values are 'minmax2centroids', 'centroids2minmax', 'corners2centroids', 'centroids2corners', 'minmax2corners', and 'corners2minmax'.\")\n",
        "\n",
        "    return tensor1\n",
        "\n",
        "def convert_coordinates2(tensor, start_index, conversion):\n",
        "    '''\n",
        "    A matrix multiplication implementation of `convert_coordinates()`.\n",
        "    Supports only conversion between the 'centroids' and 'minmax' formats.\n",
        "    This function is marginally slower on average than `convert_coordinates()`,\n",
        "    probably because it involves more (unnecessary) arithmetic operations (unnecessary\n",
        "    because the two matrices are sparse).\n",
        "    For details please refer to the documentation of `convert_coordinates()`.\n",
        "    '''\n",
        "    ind = start_index\n",
        "    tensor1 = np.copy(tensor).astype(np.float)\n",
        "    if conversion == 'minmax2centroids':\n",
        "        M = np.array([[0.5, 0. , -1.,  0.],\n",
        "                      [0.5, 0. ,  1.,  0.],\n",
        "                      [0. , 0.5,  0., -1.],\n",
        "                      [0. , 0.5,  0.,  1.]])\n",
        "        tensor1[..., ind:ind+4] = np.dot(tensor1[..., ind:ind+4], M)\n",
        "    elif conversion == 'centroids2minmax':\n",
        "        M = np.array([[ 1. , 1. ,  0. , 0. ],\n",
        "                      [ 0. , 0. ,  1. , 1. ],\n",
        "                      [-0.5, 0.5,  0. , 0. ],\n",
        "                      [ 0. , 0. , -0.5, 0.5]]) # The multiplicative inverse of the matrix above\n",
        "        tensor1[..., ind:ind+4] = np.dot(tensor1[..., ind:ind+4], M)\n",
        "    else:\n",
        "        raise ValueError(\"Unexpected conversion value. Supported values are 'minmax2centroids' and 'centroids2minmax'.\")\n",
        "\n",
        "    return tensor1\n",
        "\n",
        "def intersection_area(boxes1, boxes2, coords='centroids', mode='outer_product', border_pixels='half'):\n",
        "    '''\n",
        "    Computes the intersection areas of two sets of axis-aligned 2D rectangular boxes.\n",
        "    Let `boxes1` and `boxes2` contain `m` and `n` boxes, respectively.\n",
        "    In 'outer_product' mode, returns an `(m,n)` matrix with the intersection areas for all possible\n",
        "    combinations of the boxes in `boxes1` and `boxes2`.\n",
        "    In 'element-wise' mode, `m` and `n` must be broadcast-compatible. Refer to the explanation\n",
        "    of the `mode` argument for details.\n",
        "    Arguments:\n",
        "        boxes1 (array): Either a 1D Numpy array of shape `(4, )` containing the coordinates for one box in the\n",
        "            format specified by `coords` or a 2D Numpy array of shape `(m, 4)` containing the coordinates for `m` boxes.\n",
        "            If `mode` is set to 'element_wise', the shape must be broadcast-compatible with `boxes2`.\n",
        "        boxes2 (array): Either a 1D Numpy array of shape `(4, )` containing the coordinates for one box in the\n",
        "            format specified by `coords` or a 2D Numpy array of shape `(n, 4)` containing the coordinates for `n` boxes.\n",
        "            If `mode` is set to 'element_wise', the shape must be broadcast-compatible with `boxes1`.\n",
        "        coords (str, optional): The coordinate format in the input arrays. Can be either 'centroids' for the format\n",
        "            `(cx, cy, w, h)`, 'minmax' for the format `(xmin, xmax, ymin, ymax)`, or 'corners' for the format\n",
        "            `(xmin, ymin, xmax, ymax)`.\n",
        "        mode (str, optional): Can be one of 'outer_product' and 'element-wise'. In 'outer_product' mode, returns an\n",
        "            `(m,n)` matrix with the intersection areas for all possible combinations of the `m` boxes in `boxes1` with the\n",
        "            `n` boxes in `boxes2`. In 'element-wise' mode, returns a 1D array and the shapes of `boxes1` and `boxes2`\n",
        "            must be boadcast-compatible. If both `boxes1` and `boxes2` have `m` boxes, then this returns an array of\n",
        "            length `m` where the i-th position contains the intersection area of `boxes1[i]` with `boxes2[i]`.\n",
        "        border_pixels (str, optional): How to treat the border pixels of the bounding boxes.\n",
        "            Can be 'include', 'exclude', or 'half'. If 'include', the border pixels belong\n",
        "            to the boxes. If 'exclude', the border pixels do not belong to the boxes.\n",
        "            If 'half', then one of each of the two horizontal and vertical borders belong\n",
        "            to the boxex, but not the other.\n",
        "    Returns:\n",
        "        A 1D or 2D Numpy array (refer to the `mode` argument for details) of dtype float containing values with\n",
        "        the intersection areas of the boxes in `boxes1` and `boxes2`.\n",
        "    '''\n",
        "\n",
        "    # Make sure the boxes have the right shapes.\n",
        "    if boxes1.ndim > 2: raise ValueError(\"boxes1 must have rank either 1 or 2, but has rank {}.\".format(boxes1.ndim))\n",
        "    if boxes2.ndim > 2: raise ValueError(\"boxes2 must have rank either 1 or 2, but has rank {}.\".format(boxes2.ndim))\n",
        "\n",
        "    if boxes1.ndim == 1: boxes1 = np.expand_dims(boxes1, axis=0)\n",
        "    if boxes2.ndim == 1: boxes2 = np.expand_dims(boxes2, axis=0)\n",
        "\n",
        "    if not (boxes1.shape[1] == boxes2.shape[1] == 4): raise ValueError(\"All boxes must consist of 4 coordinates, but the boxes in `boxes1` and `boxes2` have {} and {} coordinates, respectively.\".format(boxes1.shape[1], boxes2.shape[1]))\n",
        "    if not mode in {'outer_product', 'element-wise'}: raise ValueError(\"`mode` must be one of 'outer_product' and 'element-wise', but got '{}'.\",format(mode))\n",
        "\n",
        "    # Convert the coordinates if necessary.\n",
        "    if coords == 'centroids':\n",
        "        boxes1 = convert_coordinates(boxes1, start_index=0, conversion='centroids2corners')\n",
        "        boxes2 = convert_coordinates(boxes2, start_index=0, conversion='centroids2corners')\n",
        "        coords = 'corners'\n",
        "    elif not (coords in {'minmax', 'corners'}):\n",
        "        raise ValueError(\"Unexpected value for `coords`. Supported values are 'minmax', 'corners' and 'centroids'.\")\n",
        "\n",
        "    m = boxes1.shape[0] # The number of boxes in `boxes1`\n",
        "    n = boxes2.shape[0] # The number of boxes in `boxes2`\n",
        "\n",
        "    # Set the correct coordinate indices for the respective formats.\n",
        "    if coords == 'corners':\n",
        "        xmin = 0\n",
        "        ymin = 1\n",
        "        xmax = 2\n",
        "        ymax = 3\n",
        "    elif coords == 'minmax':\n",
        "        xmin = 0\n",
        "        xmax = 1\n",
        "        ymin = 2\n",
        "        ymax = 3\n",
        "\n",
        "    if border_pixels == 'half':\n",
        "        d = 0\n",
        "    elif border_pixels == 'include':\n",
        "        d = 1 # If border pixels are supposed to belong to the bounding boxes, we have to add one pixel to any difference `xmax - xmin` or `ymax - ymin`.\n",
        "    elif border_pixels == 'exclude':\n",
        "        d = -1 # If border pixels are not supposed to belong to the bounding boxes, we have to subtract one pixel from any difference `xmax - xmin` or `ymax - ymin`.\n",
        "\n",
        "    # Compute the intersection areas.\n",
        "\n",
        "    if mode == 'outer_product':\n",
        "\n",
        "        # For all possible box combinations, get the greater xmin and ymin values.\n",
        "        # This is a tensor of shape (m,n,2).\n",
        "        min_xy = np.maximum(np.tile(np.expand_dims(boxes1[:,[xmin,ymin]], axis=1), reps=(1, n, 1)),\n",
        "                            np.tile(np.expand_dims(boxes2[:,[xmin,ymin]], axis=0), reps=(m, 1, 1)))\n",
        "\n",
        "        # For all possible box combinations, get the smaller xmax and ymax values.\n",
        "        # This is a tensor of shape (m,n,2).\n",
        "        max_xy = np.minimum(np.tile(np.expand_dims(boxes1[:,[xmax,ymax]], axis=1), reps=(1, n, 1)),\n",
        "                            np.tile(np.expand_dims(boxes2[:,[xmax,ymax]], axis=0), reps=(m, 1, 1)))\n",
        "\n",
        "        # Compute the side lengths of the intersection rectangles.\n",
        "        side_lengths = np.maximum(0, max_xy - min_xy + d)\n",
        "\n",
        "        return side_lengths[:,:,0] * side_lengths[:,:,1]\n",
        "\n",
        "    elif mode == 'element-wise':\n",
        "\n",
        "        min_xy = np.maximum(boxes1[:,[xmin,ymin]], boxes2[:,[xmin,ymin]])\n",
        "        max_xy = np.minimum(boxes1[:,[xmax,ymax]], boxes2[:,[xmax,ymax]])\n",
        "\n",
        "        # Compute the side lengths of the intersection rectangles.\n",
        "        side_lengths = np.maximum(0, max_xy - min_xy + d)\n",
        "\n",
        "        return side_lengths[:,0] * side_lengths[:,1]\n",
        "\n",
        "def intersection_area_(boxes1, boxes2, coords='corners', mode='outer_product', border_pixels='half'):\n",
        "    '''\n",
        "    The same as 'intersection_area()' but for internal use, i.e. without all the safety checks.\n",
        "    '''\n",
        "\n",
        "    m = boxes1.shape[0] # The number of boxes in `boxes1`\n",
        "    n = boxes2.shape[0] # The number of boxes in `boxes2`\n",
        "\n",
        "    # Set the correct coordinate indices for the respective formats.\n",
        "    if coords == 'corners':\n",
        "        xmin = 0\n",
        "        ymin = 1\n",
        "        xmax = 2\n",
        "        ymax = 3\n",
        "    elif coords == 'minmax':\n",
        "        xmin = 0\n",
        "        xmax = 1\n",
        "        ymin = 2\n",
        "        ymax = 3\n",
        "\n",
        "    if border_pixels == 'half':\n",
        "        d = 0\n",
        "    elif border_pixels == 'include':\n",
        "        d = 1 # If border pixels are supposed to belong to the bounding boxes, we have to add one pixel to any difference `xmax - xmin` or `ymax - ymin`.\n",
        "    elif border_pixels == 'exclude':\n",
        "        d = -1 # If border pixels are not supposed to belong to the bounding boxes, we have to subtract one pixel from any difference `xmax - xmin` or `ymax - ymin`.\n",
        "\n",
        "    # Compute the intersection areas.\n",
        "\n",
        "    if mode == 'outer_product':\n",
        "\n",
        "        # For all possible box combinations, get the greater xmin and ymin values.\n",
        "        # This is a tensor of shape (m,n,2).\n",
        "        min_xy = np.maximum(np.tile(np.expand_dims(boxes1[:,[xmin,ymin]], axis=1), reps=(1, n, 1)),\n",
        "                            np.tile(np.expand_dims(boxes2[:,[xmin,ymin]], axis=0), reps=(m, 1, 1)))\n",
        "\n",
        "        # For all possible box combinations, get the smaller xmax and ymax values.\n",
        "        # This is a tensor of shape (m,n,2).\n",
        "        max_xy = np.minimum(np.tile(np.expand_dims(boxes1[:,[xmax,ymax]], axis=1), reps=(1, n, 1)),\n",
        "                            np.tile(np.expand_dims(boxes2[:,[xmax,ymax]], axis=0), reps=(m, 1, 1)))\n",
        "\n",
        "        # Compute the side lengths of the intersection rectangles.\n",
        "        side_lengths = np.maximum(0, max_xy - min_xy + d)\n",
        "\n",
        "        return side_lengths[:,:,0] * side_lengths[:,:,1]\n",
        "\n",
        "    elif mode == 'element-wise':\n",
        "\n",
        "        min_xy = np.maximum(boxes1[:,[xmin,ymin]], boxes2[:,[xmin,ymin]])\n",
        "        max_xy = np.minimum(boxes1[:,[xmax,ymax]], boxes2[:,[xmax,ymax]])\n",
        "\n",
        "        # Compute the side lengths of the intersection rectangles.\n",
        "        side_lengths = np.maximum(0, max_xy - min_xy + d)\n",
        "\n",
        "        return side_lengths[:,0] * side_lengths[:,1]\n",
        "\n",
        "\n",
        "def iou(boxes1, boxes2, coords='centroids', mode='outer_product', border_pixels='half'):\n",
        "    '''\n",
        "    Computes the intersection-over-union similarity (also known as Jaccard similarity)\n",
        "    of two sets of axis-aligned 2D rectangular boxes.\n",
        "    Let `boxes1` and `boxes2` contain `m` and `n` boxes, respectively.\n",
        "    In 'outer_product' mode, returns an `(m,n)` matrix with the IoUs for all possible\n",
        "    combinations of the boxes in `boxes1` and `boxes2`.\n",
        "    In 'element-wise' mode, `m` and `n` must be broadcast-compatible. Refer to the explanation\n",
        "    of the `mode` argument for details.\n",
        "    Arguments:\n",
        "        boxes1 (array): Either a 1D Numpy array of shape `(4, )` containing the coordinates for one box in the\n",
        "            format specified by `coords` or a 2D Numpy array of shape `(m, 4)` containing the coordinates for `m` boxes.\n",
        "            If `mode` is set to 'element_wise', the shape must be broadcast-compatible with `boxes2`.\n",
        "        boxes2 (array): Either a 1D Numpy array of shape `(4, )` containing the coordinates for one box in the\n",
        "            format specified by `coords` or a 2D Numpy array of shape `(n, 4)` containing the coordinates for `n` boxes.\n",
        "            If `mode` is set to 'element_wise', the shape must be broadcast-compatible with `boxes1`.\n",
        "        coords (str, optional): The coordinate format in the input arrays. Can be either 'centroids' for the format\n",
        "            `(cx, cy, w, h)`, 'minmax' for the format `(xmin, xmax, ymin, ymax)`, or 'corners' for the format\n",
        "            `(xmin, ymin, xmax, ymax)`.\n",
        "        mode (str, optional): Can be one of 'outer_product' and 'element-wise'. In 'outer_product' mode, returns an\n",
        "            `(m,n)` matrix with the IoU overlaps for all possible combinations of the `m` boxes in `boxes1` with the\n",
        "            `n` boxes in `boxes2`. In 'element-wise' mode, returns a 1D array and the shapes of `boxes1` and `boxes2`\n",
        "            must be boadcast-compatible. If both `boxes1` and `boxes2` have `m` boxes, then this returns an array of\n",
        "            length `m` where the i-th position contains the IoU overlap of `boxes1[i]` with `boxes2[i]`.\n",
        "        border_pixels (str, optional): How to treat the border pixels of the bounding boxes.\n",
        "            Can be 'include', 'exclude', or 'half'. If 'include', the border pixels belong\n",
        "            to the boxes. If 'exclude', the border pixels do not belong to the boxes.\n",
        "            If 'half', then one of each of the two horizontal and vertical borders belong\n",
        "            to the boxex, but not the other.\n",
        "    Returns:\n",
        "        A 1D or 2D Numpy array (refer to the `mode` argument for details) of dtype float containing values in [0,1],\n",
        "        the Jaccard similarity of the boxes in `boxes1` and `boxes2`. 0 means there is no overlap between two given\n",
        "        boxes, 1 means their coordinates are identical.\n",
        "    '''\n",
        "\n",
        "    # Make sure the boxes have the right shapes.\n",
        "    if boxes1.ndim > 2: raise ValueError(\"boxes1 must have rank either 1 or 2, but has rank {}.\".format(boxes1.ndim))\n",
        "    if boxes2.ndim > 2: raise ValueError(\"boxes2 must have rank either 1 or 2, but has rank {}.\".format(boxes2.ndim))\n",
        "\n",
        "    if boxes1.ndim == 1: boxes1 = np.expand_dims(boxes1, axis=0)\n",
        "    if boxes2.ndim == 1: boxes2 = np.expand_dims(boxes2, axis=0)\n",
        "\n",
        "    if not (boxes1.shape[1] == boxes2.shape[1] == 4): raise ValueError(\"All boxes must consist of 4 coordinates, but the boxes in `boxes1` and `boxes2` have {} and {} coordinates, respectively.\".format(boxes1.shape[1], boxes2.shape[1]))\n",
        "    if not mode in {'outer_product', 'element-wise'}: raise ValueError(\"`mode` must be one of 'outer_product' and 'element-wise', but got '{}'.\".format(mode))\n",
        "\n",
        "    # Convert the coordinates if necessary.\n",
        "    if coords == 'centroids':\n",
        "        boxes1 = convert_coordinates(boxes1, start_index=0, conversion='centroids2corners')\n",
        "        boxes2 = convert_coordinates(boxes2, start_index=0, conversion='centroids2corners')\n",
        "        coords = 'corners'\n",
        "    elif not (coords in {'minmax', 'corners'}):\n",
        "        raise ValueError(\"Unexpected value for `coords`. Supported values are 'minmax', 'corners' and 'centroids'.\")\n",
        "\n",
        "    # Compute the IoU.\n",
        "\n",
        "    # Compute the interesection areas.\n",
        "\n",
        "    intersection_areas = intersection_area_(boxes1, boxes2, coords=coords, mode=mode)\n",
        "\n",
        "    m = boxes1.shape[0] # The number of boxes in `boxes1`\n",
        "    n = boxes2.shape[0] # The number of boxes in `boxes2`\n",
        "\n",
        "    # Compute the union areas.\n",
        "\n",
        "    # Set the correct coordinate indices for the respective formats.\n",
        "    if coords == 'corners':\n",
        "        xmin = 0\n",
        "        ymin = 1\n",
        "        xmax = 2\n",
        "        ymax = 3\n",
        "    elif coords == 'minmax':\n",
        "        xmin = 0\n",
        "        xmax = 1\n",
        "        ymin = 2\n",
        "        ymax = 3\n",
        "\n",
        "    if border_pixels == 'half':\n",
        "        d = 0\n",
        "    elif border_pixels == 'include':\n",
        "        d = 1 # If border pixels are supposed to belong to the bounding boxes, we have to add one pixel to any difference `xmax - xmin` or `ymax - ymin`.\n",
        "    elif border_pixels == 'exclude':\n",
        "        d = -1 # If border pixels are not supposed to belong to the bounding boxes, we have to subtract one pixel from any difference `xmax - xmin` or `ymax - ymin`.\n",
        "\n",
        "    if mode == 'outer_product':\n",
        "\n",
        "        boxes1_areas = np.tile(np.expand_dims((boxes1[:,xmax] - boxes1[:,xmin] + d) * (boxes1[:,ymax] - boxes1[:,ymin] + d), axis=1), reps=(1,n))\n",
        "        boxes2_areas = np.tile(np.expand_dims((boxes2[:,xmax] - boxes2[:,xmin] + d) * (boxes2[:,ymax] - boxes2[:,ymin] + d), axis=0), reps=(m,1))\n",
        "\n",
        "    elif mode == 'element-wise':\n",
        "\n",
        "        boxes1_areas = (boxes1[:,xmax] - boxes1[:,xmin] + d) * (boxes1[:,ymax] - boxes1[:,ymin] + d)\n",
        "        boxes2_areas = (boxes2[:,xmax] - boxes2[:,xmin] + d) * (boxes2[:,ymax] - boxes2[:,ymin] + d)\n",
        "\n",
        "    union_areas = boxes1_areas + boxes2_areas - intersection_areas\n",
        "\n",
        "    return intersection_areas / union_areas"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0c8kwlLcFc9H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "A custom Keras layer to generate anchor boxes.\n",
        "Copyright (C) 2018 Pierluigi Ferrari\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "   http://www.apache.org/licenses/LICENSE-2.0\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "'''\n",
        "\n",
        "from __future__ import division\n",
        "import numpy as np\n",
        "import keras.backend as K\n",
        "from keras.engine.topology import InputSpec\n",
        "from keras.engine.topology import Layer\n",
        "\n",
        "#from bounding_box_utils.bounding_box_utils import convert_coordinates\n",
        "\n",
        "class AnchorBoxes(Layer):\n",
        "    '''\n",
        "    A Keras layer to create an output tensor containing anchor box coordinates\n",
        "    and variances based on the input tensor and the passed arguments.\n",
        "    A set of 2D anchor boxes of different aspect ratios is created for each spatial unit of\n",
        "    the input tensor. The number of anchor boxes created per unit depends on the arguments\n",
        "    `aspect_ratios` and `two_boxes_for_ar1`, in the default case it is 4. The boxes\n",
        "    are parameterized by the coordinate tuple `(xmin, xmax, ymin, ymax)`.\n",
        "    The logic implemented by this layer is identical to the logic in the module\n",
        "    `ssd_box_encode_decode_utils.py`.\n",
        "    The purpose of having this layer in the network is to make the model self-sufficient\n",
        "    at inference time. Since the model is predicting offsets to the anchor boxes\n",
        "    (rather than predicting absolute box coordinates directly), one needs to know the anchor\n",
        "    box coordinates in order to construct the final prediction boxes from the predicted offsets.\n",
        "    If the model's output tensor did not contain the anchor box coordinates, the necessary\n",
        "    information to convert the predicted offsets back to absolute coordinates would be missing\n",
        "    in the model output. The reason why it is necessary to predict offsets to the anchor boxes\n",
        "    rather than to predict absolute box coordinates directly is explained in `README.md`.\n",
        "    Input shape:\n",
        "        4D tensor of shape `(batch, channels, height, width)` if `dim_ordering = 'th'`\n",
        "        or `(batch, height, width, channels)` if `dim_ordering = 'tf'`.\n",
        "    Output shape:\n",
        "        5D tensor of shape `(batch, height, width, n_boxes, 8)`. The last axis contains\n",
        "        the four anchor box coordinates and the four variance values for each box.\n",
        "    '''\n",
        "\n",
        "    def __init__(self,\n",
        "                 img_height,\n",
        "                 img_width,\n",
        "                 this_scale,\n",
        "                 next_scale,\n",
        "                 aspect_ratios=[0.5, 1.0, 2.0],\n",
        "                 two_boxes_for_ar1=True,\n",
        "                 this_steps=None,\n",
        "                 this_offsets=None,\n",
        "                 clip_boxes=False,\n",
        "                 variances=[0.1, 0.1, 0.2, 0.2],\n",
        "              #   coords='centroids', ################changed here for the format `(xmin, ymin, xmax,  ymax)\n",
        "                 coords='corners',\n",
        "                 normalize_coords=False,\n",
        "                 **kwargs):\n",
        "        '''\n",
        "        All arguments need to be set to the same values as in the box encoding process, otherwise the behavior is undefined.\n",
        "        Some of these arguments are explained in more detail in the documentation of the `SSDBoxEncoder` class.\n",
        "        Arguments:\n",
        "            img_height (int): The height of the input images.\n",
        "            img_width (int): The width of the input images.\n",
        "            this_scale (float): A float in [0, 1], the scaling factor for the size of the generated anchor boxes\n",
        "                as a fraction of the shorter side of the input image.\n",
        "            next_scale (float): A float in [0, 1], the next larger scaling factor. Only relevant if\n",
        "                `self.two_boxes_for_ar1 == True`.\n",
        "            aspect_ratios (list, optional): The list of aspect ratios for which default boxes are to be\n",
        "                generated for this layer.\n",
        "            two_boxes_for_ar1 (bool, optional): Only relevant if `aspect_ratios` contains 1.\n",
        "                If `True`, two default boxes will be generated for aspect ratio 1. The first will be generated\n",
        "                using the scaling factor for the respective layer, the second one will be generated using\n",
        "                geometric mean of said scaling factor and next bigger scaling factor.\n",
        "            clip_boxes (bool, optional): If `True`, clips the anchor box coordinates to stay within image boundaries.\n",
        "            variances (list, optional): A list of 4 floats >0. The anchor box offset for each coordinate will be divided by\n",
        "                its respective variance value.\n",
        "            coords (str, optional): The box coordinate format to be used internally in the model (i.e. this is not the input format\n",
        "                of the ground truth labels). Can be either 'centroids' for the format `(cx, cy, w, h)` (box center coordinates, width, and height),\n",
        "                'corners' for the format `(xmin, ymin, xmax,  ymax)`, or 'minmax' for the format `(xmin, xmax, ymin, ymax)`.\n",
        "            normalize_coords (bool, optional): Set to `True` if the model uses relative instead of absolute coordinates,\n",
        "                i.e. if the model predicts box coordinates within [0,1] instead of absolute coordinates.\n",
        "        '''\n",
        "        if K.backend() != 'tensorflow':\n",
        "            raise TypeError(\"This layer only supports TensorFlow at the moment, but you are using the {} backend.\".format(K.backend()))\n",
        "\n",
        "        if (this_scale < 0) or (next_scale < 0) or (this_scale > 1):\n",
        "            raise ValueError(\"`this_scale` must be in [0, 1] and `next_scale` must be >0, but `this_scale` == {}, `next_scale` == {}\".format(this_scale, next_scale))\n",
        "\n",
        "        if len(variances) != 4:\n",
        "            raise ValueError(\"4 variance values must be pased, but {} values were received.\".format(len(variances)))\n",
        "        variances = np.array(variances)\n",
        "        if np.any(variances <= 0):\n",
        "            raise ValueError(\"All variances must be >0, but the variances given are {}\".format(variances))\n",
        "\n",
        "        self.img_height = img_height\n",
        "        self.img_width = img_width\n",
        "        self.this_scale = this_scale\n",
        "        self.next_scale = next_scale\n",
        "        self.aspect_ratios = aspect_ratios\n",
        "        self.two_boxes_for_ar1 = two_boxes_for_ar1\n",
        "        self.this_steps = this_steps\n",
        "        self.this_offsets = this_offsets\n",
        "        self.clip_boxes = clip_boxes\n",
        "        self.variances = variances\n",
        "        self.coords = coords\n",
        "        self.normalize_coords = normalize_coords\n",
        "        # Compute the number of boxes per cell\n",
        "        if (1 in aspect_ratios) and two_boxes_for_ar1:\n",
        "            self.n_boxes = len(aspect_ratios) + 1\n",
        "        else:\n",
        "            self.n_boxes = len(aspect_ratios)\n",
        "        super(AnchorBoxes, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.input_spec = [InputSpec(shape=input_shape)]\n",
        "        super(AnchorBoxes, self).build(input_shape)\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        '''\n",
        "        Return an anchor box tensor based on the shape of the input tensor.\n",
        "        The logic implemented here is identical to the logic in the module `ssd_box_encode_decode_utils.py`.\n",
        "        Note that this tensor does not participate in any graph computations at runtime. It is being created\n",
        "        as a constant once during graph creation and is just being output along with the rest of the model output\n",
        "        during runtime. Because of this, all logic is implemented as Numpy array operations and it is sufficient\n",
        "        to convert the resulting Numpy array into a Keras tensor at the very end before outputting it.\n",
        "        Arguments:\n",
        "            x (tensor): 4D tensor of shape `(batch, channels, height, width)` if `dim_ordering = 'th'`\n",
        "                or `(batch, height, width, channels)` if `dim_ordering = 'tf'`. The input for this\n",
        "                layer must be the output of the localization predictor layer.\n",
        "        '''\n",
        "\n",
        "        # Compute box width and height for each aspect ratio\n",
        "        # The shorter side of the image will be used to compute `w` and `h` using `scale` and `aspect_ratios`.\n",
        "        size = min(self.img_height, self.img_width)\n",
        "        # Compute the box widths and and heights for all aspect ratios\n",
        "        wh_list = []\n",
        "        for ar in self.aspect_ratios:\n",
        "            if (ar == 1):\n",
        "                # Compute the regular anchor box for aspect ratio 1.\n",
        "                box_height = box_width = self.this_scale * size\n",
        "                wh_list.append((box_width, box_height))\n",
        "                if self.two_boxes_for_ar1:\n",
        "                    # Compute one slightly larger version using the geometric mean of this scale value and the next.\n",
        "                    box_height = box_width = np.sqrt(self.this_scale * self.next_scale) * size\n",
        "                    wh_list.append((box_width, box_height))\n",
        "            else:\n",
        "                box_height = self.this_scale * size / np.sqrt(ar)\n",
        "                box_width = self.this_scale * size * np.sqrt(ar)\n",
        "                wh_list.append((box_width, box_height))\n",
        "        wh_list = np.array(wh_list)\n",
        "\n",
        "        # We need the shape of the input tensor\n",
        "        #if K.image_dim_ordering() == 'tf':\n",
        "        if K.image_data_format()  == 'channels_last':  \n",
        "            batch_size, feature_map_height, feature_map_width, feature_map_channels = x._keras_shape\n",
        "        else: # Not yet relevant since TensorFlow is the only supported backend right now, but it can't harm to have this in here for the future\n",
        "            batch_size, feature_map_channels, feature_map_height, feature_map_width = x._keras_shape\n",
        "\n",
        "        # Compute the grid of box center points. They are identical for all aspect ratios.\n",
        "\n",
        "        # Compute the step sizes, i.e. how far apart the anchor box center points will be vertically and horizontally.\n",
        "        if (self.this_steps is None):\n",
        "            step_height = self.img_height / feature_map_height\n",
        "            step_width = self.img_width / feature_map_width\n",
        "        else:\n",
        "            if isinstance(self.this_steps, (list, tuple)) and (len(self.this_steps) == 2):\n",
        "                step_height = self.this_steps[0]\n",
        "                step_width = self.this_steps[1]\n",
        "            elif isinstance(self.this_steps, (int, float)):\n",
        "                step_height = self.this_steps\n",
        "                step_width = self.this_steps\n",
        "        # Compute the offsets, i.e. at what pixel values the first anchor box center point will be from the top and from the left of the image.\n",
        "        if (self.this_offsets is None):\n",
        "            offset_height = 0.5\n",
        "            offset_width = 0.5\n",
        "        else:\n",
        "            if isinstance(self.this_offsets, (list, tuple)) and (len(self.this_offsets) == 2):\n",
        "                offset_height = self.this_offsets[0]\n",
        "                offset_width = self.this_offsets[1]\n",
        "            elif isinstance(self.this_offsets, (int, float)):\n",
        "                offset_height = self.this_offsets\n",
        "                offset_width = self.this_offsets\n",
        "        # Now that we have the offsets and step sizes, compute the grid of anchor box center points.\n",
        "        cy = np.linspace(offset_height * step_height, (offset_height + feature_map_height - 1) * step_height, feature_map_height)\n",
        "        cx = np.linspace(offset_width * step_width, (offset_width + feature_map_width - 1) * step_width, feature_map_width)\n",
        "        cx_grid, cy_grid = np.meshgrid(cx, cy)\n",
        "        cx_grid = np.expand_dims(cx_grid, -1) # This is necessary for np.tile() to do what we want further down\n",
        "        cy_grid = np.expand_dims(cy_grid, -1) # This is necessary for np.tile() to do what we want further down\n",
        "\n",
        "        # Create a 4D tensor template of shape `(feature_map_height, feature_map_width, n_boxes, 4)`\n",
        "        # where the last dimension will contain `(cx, cy, w, h)`\n",
        "        boxes_tensor = np.zeros((feature_map_height, feature_map_width, self.n_boxes, 4))\n",
        "\n",
        "        boxes_tensor[:, :, :, 0] = np.tile(cx_grid, (1, 1, self.n_boxes)) # Set cx\n",
        "        boxes_tensor[:, :, :, 1] = np.tile(cy_grid, (1, 1, self.n_boxes)) # Set cy\n",
        "        boxes_tensor[:, :, :, 2] = wh_list[:, 0] # Set w\n",
        "        boxes_tensor[:, :, :, 3] = wh_list[:, 1] # Set h\n",
        "\n",
        "        # Convert `(cx, cy, w, h)` to `(xmin, xmax, ymin, ymax)`\n",
        "        boxes_tensor = convert_coordinates(boxes_tensor, start_index=0, conversion='centroids2corners')\n",
        "\n",
        "        # If `clip_boxes` is enabled, clip the coordinates to lie within the image boundaries\n",
        "        if self.clip_boxes:\n",
        "            x_coords = boxes_tensor[:,:,:,[0, 2]]\n",
        "            x_coords[x_coords >= self.img_width] = self.img_width - 1\n",
        "            x_coords[x_coords < 0] = 0\n",
        "            boxes_tensor[:,:,:,[0, 2]] = x_coords\n",
        "            y_coords = boxes_tensor[:,:,:,[1, 3]]\n",
        "            y_coords[y_coords >= self.img_height] = self.img_height - 1\n",
        "            y_coords[y_coords < 0] = 0\n",
        "            boxes_tensor[:,:,:,[1, 3]] = y_coords\n",
        "\n",
        "        # If `normalize_coords` is enabled, normalize the coordinates to be within [0,1]\n",
        "        if self.normalize_coords:\n",
        "            boxes_tensor[:, :, :, [0, 2]] /= self.img_width\n",
        "            boxes_tensor[:, :, :, [1, 3]] /= self.img_height\n",
        "\n",
        "        # TODO: Implement box limiting directly for `(cx, cy, w, h)` so that we don't have to unnecessarily convert back and forth.\n",
        "        if self.coords == 'centroids':\n",
        "            # Convert `(xmin, ymin, xmax, ymax)` back to `(cx, cy, w, h)`.\n",
        "            boxes_tensor = convert_coordinates(boxes_tensor, start_index=0, conversion='corners2centroids', border_pixels='half')\n",
        "        elif self.coords == 'minmax':\n",
        "            # Convert `(xmin, ymin, xmax, ymax)` to `(xmin, xmax, ymin, ymax).\n",
        "            boxes_tensor = convert_coordinates(boxes_tensor, start_index=0, conversion='corners2minmax', border_pixels='half')\n",
        "\n",
        "        # Create a tensor to contain the variances and append it to `boxes_tensor`. This tensor has the same shape\n",
        "        # as `boxes_tensor` and simply contains the same 4 variance values for every position in the last axis.\n",
        "        variances_tensor = np.zeros_like(boxes_tensor) # Has shape `(feature_map_height, feature_map_width, n_boxes, 4)`\n",
        "        variances_tensor += self.variances # Long live broadcasting\n",
        "        # Now `boxes_tensor` becomes a tensor of shape `(feature_map_height, feature_map_width, n_boxes, 8)`\n",
        "        boxes_tensor = np.concatenate((boxes_tensor, variances_tensor), axis=-1)\n",
        "\n",
        "        # Now prepend one dimension to `boxes_tensor` to account for the batch size and tile it along\n",
        "        # The result will be a 5D tensor of shape `(batch_size, feature_map_height, feature_map_width, n_boxes, 8)`\n",
        "        boxes_tensor = np.expand_dims(boxes_tensor, axis=0)\n",
        "        boxes_tensor = K.tile(K.constant(boxes_tensor, dtype='float32'), (K.shape(x)[0], 1, 1, 1, 1))\n",
        "\n",
        "        return boxes_tensor\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        #if K.image_dim_ordering() == 'tf':\n",
        "        if K.image_data_format()  == 'channels_last':  \n",
        "            batch_size, feature_map_height, feature_map_width, feature_map_channels = input_shape\n",
        "        else: # Not yet relevant since TensorFlow is the only supported backend right now, but it can't harm to have this in here for the future\n",
        "            batch_size, feature_map_channels, feature_map_height, feature_map_width = input_shape\n",
        "        return (batch_size, feature_map_height, feature_map_width, self.n_boxes, 8)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'img_height': self.img_height,\n",
        "            'img_width': self.img_width,\n",
        "            'this_scale': self.this_scale,\n",
        "            'next_scale': self.next_scale,\n",
        "            'aspect_ratios': list(self.aspect_ratios),\n",
        "            'two_boxes_for_ar1': self.two_boxes_for_ar1,\n",
        "            'clip_boxes': self.clip_boxes,\n",
        "            'variances': list(self.variances),\n",
        "            'coords': self.coords,\n",
        "            'normalize_coords': self.normalize_coords\n",
        "        }\n",
        "        base_config = super(AnchorBoxes, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_zqKjVNFc6P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "A custom Keras layer to decode the raw SSD prediction output. Corresponds to the\n",
        "`DetectionOutput` layer type in the original Caffe implementation of SSD.\n",
        "Copyright (C) 2018 Pierluigi Ferrari\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "   http://www.apache.org/licenses/LICENSE-2.0\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "'''\n",
        "\n",
        "from __future__ import division\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "from keras.engine.topology import InputSpec\n",
        "from keras.engine.topology import Layer\n",
        "\n",
        "class DecodeDetections(Layer):\n",
        "    '''\n",
        "    A Keras layer to decode the raw SSD prediction output.\n",
        "    Input shape:\n",
        "        3D tensor of shape `(batch_size, n_boxes, n_classes + 12)`.\n",
        "    Output shape:\n",
        "        3D tensor of shape `(batch_size, top_k, 6)`.\n",
        "    '''\n",
        "\n",
        "    def __init__(self,\n",
        "                 confidence_thresh=0.01,\n",
        "                 iou_threshold=0.45,\n",
        "                 top_k=200,\n",
        "                 nms_max_output_size=400,\n",
        "                 #coords='centroids',\n",
        "                 coords='corners',\n",
        "                 normalize_coords=True,\n",
        "                 img_height=None,\n",
        "                 img_width=None,\n",
        "                 **kwargs):\n",
        "        '''\n",
        "        All default argument values follow the Caffe implementation.\n",
        "        Arguments:\n",
        "            confidence_thresh (float, optional): A float in [0,1), the minimum classification confidence in a specific\n",
        "                positive class in order to be considered for the non-maximum suppression stage for the respective class.\n",
        "                A lower value will result in a larger part of the selection process being done by the non-maximum suppression\n",
        "                stage, while a larger value will result in a larger part of the selection process happening in the confidence\n",
        "                thresholding stage.\n",
        "            iou_threshold (float, optional): A float in [0,1]. All boxes with a Jaccard similarity of greater than `iou_threshold`\n",
        "                with a locally maximal box will be removed from the set of predictions for a given class, where 'maximal' refers\n",
        "                to the box score.\n",
        "            top_k (int, optional): The number of highest scoring predictions to be kept for each batch item after the\n",
        "                non-maximum suppression stage.\n",
        "            nms_max_output_size (int, optional): The maximum number of predictions that will be left after performing non-maximum\n",
        "                suppression.\n",
        "            coords (str, optional): The box coordinate format that the model outputs. Must be 'centroids'\n",
        "                i.e. the format `(cx, cy, w, h)` (box center coordinates, width, and height). Other coordinate formats are\n",
        "                currently not supported.\n",
        "            normalize_coords (bool, optional): Set to `True` if the model outputs relative coordinates (i.e. coordinates in [0,1])\n",
        "                and you wish to transform these relative coordinates back to absolute coordinates. If the model outputs\n",
        "                relative coordinates, but you do not want to convert them back to absolute coordinates, set this to `False`.\n",
        "                Do not set this to `True` if the model already outputs absolute coordinates, as that would result in incorrect\n",
        "                coordinates. Requires `img_height` and `img_width` if set to `True`.\n",
        "            img_height (int, optional): The height of the input images. Only needed if `normalize_coords` is `True`.\n",
        "            img_width (int, optional): The width of the input images. Only needed if `normalize_coords` is `True`.\n",
        "        '''\n",
        "        if K.backend() != 'tensorflow':\n",
        "            raise TypeError(\"This layer only supports TensorFlow at the moment, but you are using the {} backend.\".format(K.backend()))\n",
        "\n",
        "        if normalize_coords and ((img_height is None) or (img_width is None)):\n",
        "            raise ValueError(\"If relative box coordinates are supposed to be converted to absolute coordinates, the decoder needs the image size in order to decode the predictions, but `img_height == {}` and `img_width == {}`\".format(img_height, img_width))\n",
        "\n",
        "        if coords != 'centroids':\n",
        "            raise ValueError(\"The DetectionOutput layer currently only supports the 'centroids' coordinate format.\")\n",
        "\n",
        "        # We need these members for the config.\n",
        "        self.confidence_thresh = confidence_thresh\n",
        "        self.iou_threshold = iou_threshold\n",
        "        self.top_k = top_k\n",
        "        self.normalize_coords = normalize_coords\n",
        "        self.img_height = img_height\n",
        "        self.img_width = img_width\n",
        "        self.coords = coords\n",
        "        self.nms_max_output_size = nms_max_output_size\n",
        "\n",
        "        # We need these members for TensorFlow.\n",
        "        self.tf_confidence_thresh = tf.constant(self.confidence_thresh, name='confidence_thresh')\n",
        "        self.tf_iou_threshold = tf.constant(self.iou_threshold, name='iou_threshold')\n",
        "        self.tf_top_k = tf.constant(self.top_k, name='top_k')\n",
        "        self.tf_normalize_coords = tf.constant(self.normalize_coords, name='normalize_coords')\n",
        "        self.tf_img_height = tf.constant(self.img_height, dtype=tf.float32, name='img_height')\n",
        "        self.tf_img_width = tf.constant(self.img_width, dtype=tf.float32, name='img_width')\n",
        "        self.tf_nms_max_output_size = tf.constant(self.nms_max_output_size, name='nms_max_output_size')\n",
        "\n",
        "        super(DecodeDetections, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.input_spec = [InputSpec(shape=input_shape)]\n",
        "        super(DecodeDetections, self).build(input_shape)\n",
        "\n",
        "    def call(self, y_pred, mask=None):\n",
        "        '''\n",
        "        Returns:\n",
        "            3D tensor of shape `(batch_size, top_k, 6)`. The second axis is zero-padded\n",
        "            to always yield `top_k` predictions per batch item. The last axis contains\n",
        "            the coordinates for each predicted box in the format\n",
        "            `[class_id, confidence, xmin, ymin, xmax, ymax]`.\n",
        "        '''\n",
        "\n",
        "        #####################################################################################\n",
        "        # 1. Convert the box coordinates from predicted anchor box offsets to predicted\n",
        "        #    absolute coordinates\n",
        "        #####################################################################################\n",
        "\n",
        "        # Convert anchor box offsets to image offsets.\n",
        "        cx = y_pred[...,-12] * y_pred[...,-4] * y_pred[...,-6] + y_pred[...,-8] # cx = cx_pred * cx_variance * w_anchor + cx_anchor\n",
        "        cy = y_pred[...,-11] * y_pred[...,-3] * y_pred[...,-5] + y_pred[...,-7] # cy = cy_pred * cy_variance * h_anchor + cy_anchor\n",
        "        w = tf.exp(y_pred[...,-10] * y_pred[...,-2]) * y_pred[...,-6] # w = exp(w_pred * variance_w) * w_anchor\n",
        "        h = tf.exp(y_pred[...,-9] * y_pred[...,-1]) * y_pred[...,-5] # h = exp(h_pred * variance_h) * h_anchor\n",
        "\n",
        "        # Convert 'centroids' to 'corners'.\n",
        "        xmin = cx - 0.5 * w\n",
        "        ymin = cy - 0.5 * h\n",
        "        xmax = cx + 0.5 * w\n",
        "        ymax = cy + 0.5 * h\n",
        "\n",
        "        # If the model predicts box coordinates relative to the image dimensions and they are supposed\n",
        "        # to be converted back to absolute coordinates, do that.\n",
        "        def normalized_coords():\n",
        "            xmin1 = tf.expand_dims(xmin * self.tf_img_width, axis=-1)\n",
        "            ymin1 = tf.expand_dims(ymin * self.tf_img_height, axis=-1)\n",
        "            xmax1 = tf.expand_dims(xmax * self.tf_img_width, axis=-1)\n",
        "            ymax1 = tf.expand_dims(ymax * self.tf_img_height, axis=-1)\n",
        "            return xmin1, ymin1, xmax1, ymax1\n",
        "        def non_normalized_coords():\n",
        "            return tf.expand_dims(xmin, axis=-1), tf.expand_dims(ymin, axis=-1), tf.expand_dims(xmax, axis=-1), tf.expand_dims(ymax, axis=-1)\n",
        "\n",
        "        xmin, ymin, xmax, ymax = tf.cond(self.tf_normalize_coords, normalized_coords, non_normalized_coords)\n",
        "\n",
        "        # Concatenate the one-hot class confidences and the converted box coordinates to form the decoded predictions tensor.\n",
        "        y_pred = tf.concat(values=[y_pred[...,:-12], xmin, ymin, xmax, ymax], axis=-1)\n",
        "\n",
        "        #####################################################################################\n",
        "        # 2. Perform confidence thresholding, per-class non-maximum suppression, and\n",
        "        #    top-k filtering.\n",
        "        #####################################################################################\n",
        "\n",
        "        batch_size = tf.shape(y_pred)[0] # Output dtype: tf.int32\n",
        "        n_boxes = tf.shape(y_pred)[1]\n",
        "        n_classes = y_pred.shape[2] - 4\n",
        "        class_indices = tf.range(1, n_classes)\n",
        "\n",
        "        # Create a function that filters the predictions for the given batch item. Specifically, it performs:\n",
        "        # - confidence thresholding\n",
        "        # - non-maximum suppression (NMS)\n",
        "        # - top-k filtering\n",
        "        def filter_predictions(batch_item):\n",
        "\n",
        "            # Create a function that filters the predictions for one single class.\n",
        "            def filter_single_class(index):\n",
        "\n",
        "                # From a tensor of shape (n_boxes, n_classes + 4 coordinates) extract\n",
        "                # a tensor of shape (n_boxes, 1 + 4 coordinates) that contains the\n",
        "                # confidnece values for just one class, determined by `index`.\n",
        "                confidences = tf.expand_dims(batch_item[..., index], axis=-1)\n",
        "                class_id = tf.fill(dims=tf.shape(confidences), value=tf.to_float(index))\n",
        "                box_coordinates = batch_item[...,-4:]\n",
        "\n",
        "                single_class = tf.concat([class_id, confidences, box_coordinates], axis=-1)\n",
        "\n",
        "                # Apply confidence thresholding with respect to the class defined by `index`.\n",
        "                threshold_met = single_class[:,1] > self.tf_confidence_thresh\n",
        "                single_class = tf.boolean_mask(tensor=single_class,\n",
        "                                               mask=threshold_met)\n",
        "\n",
        "                # If any boxes made the threshold, perform NMS.\n",
        "                def perform_nms():\n",
        "                    scores = single_class[...,1]\n",
        "\n",
        "                    # `tf.image.non_max_suppression()` needs the box coordinates in the format `(ymin, xmin, ymax, xmax)`.\n",
        "                    xmin = tf.expand_dims(single_class[...,-4], axis=-1)\n",
        "                    ymin = tf.expand_dims(single_class[...,-3], axis=-1)\n",
        "                    xmax = tf.expand_dims(single_class[...,-2], axis=-1)\n",
        "                    ymax = tf.expand_dims(single_class[...,-1], axis=-1)\n",
        "                    boxes = tf.concat(values=[ymin, xmin, ymax, xmax], axis=-1)\n",
        "\n",
        "                    maxima_indices = tf.image.non_max_suppression(boxes=boxes,\n",
        "                                                                  scores=scores,\n",
        "                                                                  max_output_size=self.tf_nms_max_output_size,\n",
        "                                                                  iou_threshold=self.iou_threshold,\n",
        "                                                                  name='non_maximum_suppresion')\n",
        "                    maxima = tf.gather(params=single_class,\n",
        "                                       indices=maxima_indices,\n",
        "                                       axis=0)\n",
        "                    return maxima\n",
        "\n",
        "                def no_confident_predictions():\n",
        "                    return tf.constant(value=0.0, shape=(1,6))\n",
        "\n",
        "                single_class_nms = tf.cond(tf.equal(tf.size(single_class), 0), no_confident_predictions, perform_nms)\n",
        "\n",
        "                # Make sure `single_class` is exactly `self.nms_max_output_size` elements long.\n",
        "                padded_single_class = tf.pad(tensor=single_class_nms,\n",
        "                                             paddings=[[0, self.tf_nms_max_output_size - tf.shape(single_class_nms)[0]], [0, 0]],\n",
        "                                             mode='CONSTANT',\n",
        "                                             constant_values=0.0)\n",
        "\n",
        "                return padded_single_class\n",
        "\n",
        "            # Iterate `filter_single_class()` over all class indices.\n",
        "            filtered_single_classes = tf.map_fn(fn=lambda i: filter_single_class(i),\n",
        "                                                elems=tf.range(1,n_classes),\n",
        "                                                dtype=tf.float32,\n",
        "                                                parallel_iterations=128,\n",
        "                                                back_prop=False,\n",
        "                                                swap_memory=False,\n",
        "                                                infer_shape=True,\n",
        "                                                name='loop_over_classes')\n",
        "\n",
        "            # Concatenate the filtered results for all individual classes to one tensor.\n",
        "            filtered_predictions = tf.reshape(tensor=filtered_single_classes, shape=(-1,6))\n",
        "\n",
        "            # Perform top-k filtering for this batch item or pad it in case there are\n",
        "            # fewer than `self.top_k` boxes left at this point. Either way, produce a\n",
        "            # tensor of length `self.top_k`. By the time we return the final results tensor\n",
        "            # for the whole batch, all batch items must have the same number of predicted\n",
        "            # boxes so that the tensor dimensions are homogenous. If fewer than `self.top_k`\n",
        "            # predictions are left after the filtering process above, we pad the missing\n",
        "            # predictions with zeros as dummy entries.\n",
        "            def top_k():\n",
        "                return tf.gather(params=filtered_predictions,\n",
        "                                 indices=tf.nn.top_k(filtered_predictions[:, 1], k=self.tf_top_k, sorted=True).indices,\n",
        "                                 axis=0)\n",
        "            def pad_and_top_k():\n",
        "                padded_predictions = tf.pad(tensor=filtered_predictions,\n",
        "                                            paddings=[[0, self.tf_top_k - tf.shape(filtered_predictions)[0]], [0, 0]],\n",
        "                                            mode='CONSTANT',\n",
        "                                            constant_values=0.0)\n",
        "                return tf.gather(params=padded_predictions,\n",
        "                                 indices=tf.nn.top_k(padded_predictions[:, 1], k=self.tf_top_k, sorted=True).indices,\n",
        "                                 axis=0)\n",
        "\n",
        "            top_k_boxes = tf.cond(tf.greater_equal(tf.shape(filtered_predictions)[0], self.tf_top_k), top_k, pad_and_top_k)\n",
        "\n",
        "            return top_k_boxes\n",
        "\n",
        "        # Iterate `filter_predictions()` over all batch items.\n",
        "        output_tensor = tf.map_fn(fn=lambda x: filter_predictions(x),\n",
        "                                  elems=y_pred,\n",
        "                                  dtype=None,\n",
        "                                  parallel_iterations=128,\n",
        "                                  back_prop=False,\n",
        "                                  swap_memory=False,\n",
        "                                  infer_shape=True,\n",
        "                                  name='loop_over_batch')\n",
        "\n",
        "        return output_tensor\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        batch_size, n_boxes, last_axis = input_shape\n",
        "        return (batch_size, self.tf_top_k, 6) # Last axis: (class_ID, confidence, 4 box coordinates)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'confidence_thresh': self.confidence_thresh,\n",
        "            'iou_threshold': self.iou_threshold,\n",
        "            'top_k': self.top_k,\n",
        "            'nms_max_output_size': self.nms_max_output_size,\n",
        "            'coords': self.coords,\n",
        "            'normalize_coords': self.normalize_coords,\n",
        "            'img_height': self.img_height,\n",
        "            'img_width': self.img_width,\n",
        "        }\n",
        "        base_config = super(DecodeDetections, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zG_ZobPuFc3V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "A custom Keras layer to decode the raw SSD prediction output. This is a modified\n",
        "and more efficient version of the `DetectionOutput` layer type in the original Caffe\n",
        "implementation of SSD. For a faithful replication of the original layer, please\n",
        "refer to the `DecodeDetections` layer.\n",
        "Copyright (C) 2018 Pierluigi Ferrari\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "   http://www.apache.org/licenses/LICENSE-2.0\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "'''\n",
        "\n",
        "from __future__ import division\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "from keras.engine.topology import InputSpec\n",
        "from keras.engine.topology import Layer\n",
        "\n",
        "class DecodeDetectionsFast(Layer):\n",
        "    '''\n",
        "    A Keras layer to decode the raw SSD prediction output.\n",
        "    Input shape:\n",
        "        3D tensor of shape `(batch_size, n_boxes, n_classes + 12)`.\n",
        "    Output shape:\n",
        "        3D tensor of shape `(batch_size, top_k, 6)`.\n",
        "    '''\n",
        "\n",
        "    def __init__(self,\n",
        "                 confidence_thresh=0.01,\n",
        "                 iou_threshold=0.45,\n",
        "                 top_k=200,\n",
        "                 nms_max_output_size=400,\n",
        "                 coords='centroids',\n",
        "                 normalize_coords=True,\n",
        "                 img_height=None,\n",
        "                 img_width=None,\n",
        "                 **kwargs):\n",
        "        '''\n",
        "        All default argument values follow the Caffe implementation.\n",
        "        Arguments:\n",
        "            confidence_thresh (float, optional): A float in [0,1), the minimum classification confidence in a specific\n",
        "                positive class in order to be considered for the non-maximum suppression stage for the respective class.\n",
        "                A lower value will result in a larger part of the selection process being done by the non-maximum suppression\n",
        "                stage, while a larger value will result in a larger part of the selection process happening in the confidence\n",
        "                thresholding stage.\n",
        "            iou_threshold (float, optional): A float in [0,1]. All boxes with a Jaccard similarity of greater than `iou_threshold`\n",
        "                with a locally maximal box will be removed from the set of predictions for a given class, where 'maximal' refers\n",
        "                to the box score.\n",
        "            top_k (int, optional): The number of highest scoring predictions to be kept for each batch item after the\n",
        "                non-maximum suppression stage.\n",
        "            nms_max_output_size (int, optional): The maximum number of predictions that will be left after performing non-maximum\n",
        "                suppression.\n",
        "            coords (str, optional): The box coordinate format that the model outputs. Must be 'centroids'\n",
        "                i.e. the format `(cx, cy, w, h)` (box center coordinates, width, and height). Other coordinate formats are\n",
        "                currently not supported.\n",
        "            normalize_coords (bool, optional): Set to `True` if the model outputs relative coordinates (i.e. coordinates in [0,1])\n",
        "                and you wish to transform these relative coordinates back to absolute coordinates. If the model outputs\n",
        "                relative coordinates, but you do not want to convert them back to absolute coordinates, set this to `False`.\n",
        "                Do not set this to `True` if the model already outputs absolute coordinates, as that would result in incorrect\n",
        "                coordinates. Requires `img_height` and `img_width` if set to `True`.\n",
        "            img_height (int, optional): The height of the input images. Only needed if `normalize_coords` is `True`.\n",
        "            img_width (int, optional): The width of the input images. Only needed if `normalize_coords` is `True`.\n",
        "        '''\n",
        "        if K.backend() != 'tensorflow':\n",
        "            raise TypeError(\"This layer only supports TensorFlow at the moment, but you are using the {} backend.\".format(K.backend()))\n",
        "\n",
        "        if normalize_coords and ((img_height is None) or (img_width is None)):\n",
        "            raise ValueError(\"If relative box coordinates are supposed to be converted to absolute coordinates, the decoder needs the image size in order to decode the predictions, but `img_height == {}` and `img_width == {}`\".format(img_height, img_width))\n",
        "\n",
        "        if coords != 'centroids':\n",
        "            raise ValueError(\"The DetectionOutput layer currently only supports the 'centroids' coordinate format.\")\n",
        "\n",
        "        # We need these members for the config.\n",
        "        self.confidence_thresh = confidence_thresh\n",
        "        self.iou_threshold = iou_threshold\n",
        "        self.top_k = top_k\n",
        "        self.normalize_coords = normalize_coords\n",
        "        self.img_height = img_height\n",
        "        self.img_width = img_width\n",
        "        self.coords = coords\n",
        "        self.nms_max_output_size = nms_max_output_size\n",
        "\n",
        "        # We need these members for TensorFlow.\n",
        "        self.tf_confidence_thresh = tf.constant(self.confidence_thresh, name='confidence_thresh')\n",
        "        self.tf_iou_threshold = tf.constant(self.iou_threshold, name='iou_threshold')\n",
        "        self.tf_top_k = tf.constant(self.top_k, name='top_k')\n",
        "        self.tf_normalize_coords = tf.constant(self.normalize_coords, name='normalize_coords')\n",
        "        self.tf_img_height = tf.constant(self.img_height, dtype=tf.float32, name='img_height')\n",
        "        self.tf_img_width = tf.constant(self.img_width, dtype=tf.float32, name='img_width')\n",
        "        self.tf_nms_max_output_size = tf.constant(self.nms_max_output_size, name='nms_max_output_size')\n",
        "\n",
        "        super(DecodeDetectionsFast, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.input_spec = [InputSpec(shape=input_shape)]\n",
        "        super(DecodeDetectionsFast, self).build(input_shape)\n",
        "\n",
        "    def call(self, y_pred, mask=None):\n",
        "        '''\n",
        "        Returns:\n",
        "            3D tensor of shape `(batch_size, top_k, 6)`. The second axis is zero-padded\n",
        "            to always yield `top_k` predictions per batch item. The last axis contains\n",
        "            the coordinates for each predicted box in the format\n",
        "            `[class_id, confidence, xmin, ymin, xmax, ymax]`.\n",
        "        '''\n",
        "\n",
        "        #####################################################################################\n",
        "        # 1. Convert the box coordinates from predicted anchor box offsets to predicted\n",
        "        #    absolute coordinates\n",
        "        #####################################################################################\n",
        "\n",
        "        # Extract the predicted class IDs as the indices of the highest confidence values.\n",
        "        class_ids = tf.expand_dims(tf.to_float(tf.argmax(y_pred[...,:-12], axis=-1)), axis=-1)\n",
        "        # Extract the confidences of the maximal classes.\n",
        "        confidences = tf.reduce_max(y_pred[...,:-12], axis=-1, keep_dims=True)\n",
        "\n",
        "        # Convert anchor box offsets to image offsets.\n",
        "        cx = y_pred[...,-12] * y_pred[...,-4] * y_pred[...,-6] + y_pred[...,-8] # cx = cx_pred * cx_variance * w_anchor + cx_anchor\n",
        "        cy = y_pred[...,-11] * y_pred[...,-3] * y_pred[...,-5] + y_pred[...,-7] # cy = cy_pred * cy_variance * h_anchor + cy_anchor\n",
        "        w = tf.exp(y_pred[...,-10] * y_pred[...,-2]) * y_pred[...,-6] # w = exp(w_pred * variance_w) * w_anchor\n",
        "        h = tf.exp(y_pred[...,-9] * y_pred[...,-1]) * y_pred[...,-5] # h = exp(h_pred * variance_h) * h_anchor\n",
        "\n",
        "        # Convert 'centroids' to 'corners'.\n",
        "        xmin = cx - 0.5 * w\n",
        "        ymin = cy - 0.5 * h\n",
        "        xmax = cx + 0.5 * w\n",
        "        ymax = cy + 0.5 * h\n",
        "\n",
        "        # If the model predicts box coordinates relative to the image dimensions and they are supposed\n",
        "        # to be converted back to absolute coordinates, do that.\n",
        "        def normalized_coords():\n",
        "            xmin1 = tf.expand_dims(xmin * self.tf_img_width, axis=-1)\n",
        "            ymin1 = tf.expand_dims(ymin * self.tf_img_height, axis=-1)\n",
        "            xmax1 = tf.expand_dims(xmax * self.tf_img_width, axis=-1)\n",
        "            ymax1 = tf.expand_dims(ymax * self.tf_img_height, axis=-1)\n",
        "            return xmin1, ymin1, xmax1, ymax1\n",
        "        def non_normalized_coords():\n",
        "            return tf.expand_dims(xmin, axis=-1), tf.expand_dims(ymin, axis=-1), tf.expand_dims(xmax, axis=-1), tf.expand_dims(ymax, axis=-1)\n",
        "\n",
        "        xmin, ymin, xmax, ymax = tf.cond(self.tf_normalize_coords, normalized_coords, non_normalized_coords)\n",
        "\n",
        "        # Concatenate the one-hot class confidences and the converted box coordinates to form the decoded predictions tensor.\n",
        "        y_pred = tf.concat(values=[class_ids, confidences, xmin, ymin, xmax, ymax], axis=-1)\n",
        "\n",
        "        #####################################################################################\n",
        "        # 2. Perform confidence thresholding, non-maximum suppression, and top-k filtering.\n",
        "        #####################################################################################\n",
        "\n",
        "        batch_size = tf.shape(y_pred)[0] # Output dtype: tf.int32\n",
        "        n_boxes = tf.shape(y_pred)[1]\n",
        "        n_classes = y_pred.shape[2] - 4\n",
        "        class_indices = tf.range(1, n_classes)\n",
        "\n",
        "        # Create a function that filters the predictions for the given batch item. Specifically, it performs:\n",
        "        # - confidence thresholding\n",
        "        # - non-maximum suppression (NMS)\n",
        "        # - top-k filtering\n",
        "        def filter_predictions(batch_item):\n",
        "\n",
        "            # Keep only the non-background boxes.\n",
        "            positive_boxes = tf.not_equal(batch_item[...,0], 0.0)\n",
        "            predictions = tf.boolean_mask(tensor=batch_item,\n",
        "                                          mask=positive_boxes)\n",
        "\n",
        "            def perform_confidence_thresholding():\n",
        "                # Apply confidence thresholding.\n",
        "                threshold_met = predictions[:,1] > self.tf_confidence_thresh\n",
        "                return tf.boolean_mask(tensor=predictions,\n",
        "                                       mask=threshold_met)\n",
        "            def no_positive_boxes():\n",
        "                return tf.constant(value=0.0, shape=(1,6))\n",
        "\n",
        "            # If there are any positive predictions, perform confidence thresholding.\n",
        "            predictions_conf_thresh = tf.cond(tf.equal(tf.size(predictions), 0), no_positive_boxes, perform_confidence_thresholding)\n",
        "\n",
        "            def perform_nms():\n",
        "                scores = predictions_conf_thresh[...,1]\n",
        "\n",
        "                # `tf.image.non_max_suppression()` needs the box coordinates in the format `(ymin, xmin, ymax, xmax)`.\n",
        "                xmin = tf.expand_dims(predictions_conf_thresh[...,-4], axis=-1)\n",
        "                ymin = tf.expand_dims(predictions_conf_thresh[...,-3], axis=-1)\n",
        "                xmax = tf.expand_dims(predictions_conf_thresh[...,-2], axis=-1)\n",
        "                ymax = tf.expand_dims(predictions_conf_thresh[...,-1], axis=-1)\n",
        "                boxes = tf.concat(values=[ymin, xmin, ymax, xmax], axis=-1)\n",
        "\n",
        "                maxima_indices = tf.image.non_max_suppression(boxes=boxes,\n",
        "                                                              scores=scores,\n",
        "                                                              max_output_size=self.tf_nms_max_output_size,\n",
        "                                                              iou_threshold=self.iou_threshold,\n",
        "                                                              name='non_maximum_suppresion')\n",
        "                maxima = tf.gather(params=predictions_conf_thresh,\n",
        "                                   indices=maxima_indices,\n",
        "                                   axis=0)\n",
        "                return maxima\n",
        "            def no_confident_predictions():\n",
        "                return tf.constant(value=0.0, shape=(1,6))\n",
        "\n",
        "            # If any boxes made the threshold, perform NMS.\n",
        "            predictions_nms = tf.cond(tf.equal(tf.size(predictions_conf_thresh), 0), no_confident_predictions, perform_nms)\n",
        "\n",
        "            # Perform top-k filtering for this batch item or pad it in case there are\n",
        "            # fewer than `self.top_k` boxes left at this point. Either way, produce a\n",
        "            # tensor of length `self.top_k`. By the time we return the final results tensor\n",
        "            # for the whole batch, all batch items must have the same number of predicted\n",
        "            # boxes so that the tensor dimensions are homogenous. If fewer than `self.top_k`\n",
        "            # predictions are left after the filtering process above, we pad the missing\n",
        "            # predictions with zeros as dummy entries.\n",
        "            def top_k():\n",
        "                return tf.gather(params=predictions_nms,\n",
        "                                 indices=tf.nn.top_k(predictions_nms[:, 1], k=self.tf_top_k, sorted=True).indices,\n",
        "                                 axis=0)\n",
        "            def pad_and_top_k():\n",
        "                padded_predictions = tf.pad(tensor=predictions_nms,\n",
        "                                            paddings=[[0, self.tf_top_k - tf.shape(predictions_nms)[0]], [0, 0]],\n",
        "                                            mode='CONSTANT',\n",
        "                                            constant_values=0.0)\n",
        "                return tf.gather(params=padded_predictions,\n",
        "                                 indices=tf.nn.top_k(padded_predictions[:, 1], k=self.tf_top_k, sorted=True).indices,\n",
        "                                 axis=0)\n",
        "\n",
        "            top_k_boxes = tf.cond(tf.greater_equal(tf.shape(predictions_nms)[0], self.tf_top_k), top_k, pad_and_top_k)\n",
        "\n",
        "            return top_k_boxes\n",
        "\n",
        "        # Iterate `filter_predictions()` over all batch items.\n",
        "        output_tensor = tf.map_fn(fn=lambda x: filter_predictions(x),\n",
        "                                  elems=y_pred,\n",
        "                                  dtype=None,\n",
        "                                  parallel_iterations=128,\n",
        "                                  back_prop=False,\n",
        "                                  swap_memory=False,\n",
        "                                  infer_shape=True,\n",
        "                                  name='loop_over_batch')\n",
        "\n",
        "        return output_tensor\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        batch_size, n_boxes, last_axis = input_shape\n",
        "        return (batch_size, self.tf_top_k, 6) # Last axis: (class_ID, confidence, 4 box coordinates)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'confidence_thresh': self.confidence_thresh,\n",
        "            'iou_threshold': self.iou_threshold,\n",
        "            'top_k': self.top_k,\n",
        "            'nms_max_output_size': self.nms_max_output_size,\n",
        "            'coords': self.coords,\n",
        "            'normalize_coords': self.normalize_coords,\n",
        "            'img_height': self.img_height,\n",
        "            'img_width': self.img_width,\n",
        "        }\n",
        "        base_config = super(DecodeDetectionsFast, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mii-OafJFch0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "A custom Keras layer to perform L2-normalization.\n",
        "Copyright (C) 2018 Pierluigi Ferrari\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "   http://www.apache.org/licenses/LICENSE-2.0\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "'''\n",
        "\n",
        "from __future__ import division\n",
        "import numpy as np\n",
        "import keras.backend as K\n",
        "from keras.engine.topology import InputSpec\n",
        "from keras.engine.topology import Layer\n",
        "\n",
        "class L2Normalization(Layer):\n",
        "    '''\n",
        "    Performs L2 normalization on the input tensor with a learnable scaling parameter\n",
        "    as described in the paper \"Parsenet: Looking Wider to See Better\" (see references)\n",
        "    and as used in the original SSD model.\n",
        "    Arguments:\n",
        "        gamma_init (int): The initial scaling parameter. Defaults to 20 following the\n",
        "            SSD paper.\n",
        "    Input shape:\n",
        "        4D tensor of shape `(batch, channels, height, width)` if `dim_ordering = 'th'`\n",
        "        or `(batch, height, width, channels)` if `dim_ordering = 'tf'`.\n",
        "    Returns:\n",
        "        The scaled tensor. Same shape as the input tensor.\n",
        "    References:\n",
        "        http://cs.unc.edu/~wliu/papers/parsenet.pdf\n",
        "    '''\n",
        "\n",
        "    def __init__(self, gamma_init=20, **kwargs):\n",
        "       # if K.image_dim_ordering() == 'tf':\n",
        "        if K.image_data_format()  == 'channels_last':  \n",
        "            self.axis = 3\n",
        "        else:\n",
        "            self.axis = 1\n",
        "        self.gamma_init = gamma_init\n",
        "        super(L2Normalization, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.input_spec = [InputSpec(shape=input_shape)]\n",
        "        gamma = self.gamma_init * np.ones((input_shape[self.axis],))\n",
        "        self.gamma = K.variable(gamma, name='{}_gamma'.format(self.name))\n",
        "        self.trainable_weights = [self.gamma]\n",
        "        super(L2Normalization, self).build(input_shape)\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        output = K.l2_normalize(x, self.axis)\n",
        "        return output * self.gamma\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'gamma_init': self.gamma_init\n",
        "        }\n",
        "        base_config = super(L2Normalization, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdkZN29AEe8m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from models.keras_ssd300 import ssd_300\n",
        "'''\n",
        "A Keras port of the original Caffe SSD300 network.\n",
        "Copyright (C) 2018 Pierluigi Ferrari\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "   http://www.apache.org/licenses/LICENSE-2.0\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "'''\n",
        "\n",
        "from __future__ import division\n",
        "import numpy as np\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Lambda, Activation, Conv2D, MaxPooling2D, ZeroPadding2D, Reshape, Concatenate\n",
        "from keras.regularizers import l2\n",
        "import keras.backend as K\n",
        "\n",
        "#from keras_layers.keras_layer_AnchorBoxes import AnchorBoxes\n",
        "#from keras_layers.keras_layer_L2Normalization import L2Normalization\n",
        "#from keras_layers.keras_layer_DecodeDetections import DecodeDetections\n",
        "#from keras_layers.keras_layer_DecodeDetectionsFast import DecodeDetectionsFast\n",
        "\n",
        "def ssd_300(image_size,\n",
        "            n_classes,\n",
        "            mode='training',\n",
        "            l2_regularization=0.0005,\n",
        "            min_scale=None,\n",
        "            max_scale=None,\n",
        "            scales=None,\n",
        "            aspect_ratios_global=None,\n",
        "            aspect_ratios_per_layer=[[1.0, 2.0, 0.5],\n",
        "                                     [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
        "                                     [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
        "                                     [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
        "                                     [1.0, 2.0, 0.5],\n",
        "                                     [1.0, 2.0, 0.5]],\n",
        "            two_boxes_for_ar1=True,\n",
        "            steps=[8, 16, 32, 64, 100, 300],\n",
        "            offsets=None,\n",
        "            clip_boxes=False,\n",
        "            variances=[0.1, 0.1, 0.2, 0.2],\n",
        "            coords='centroids',\n",
        "            normalize_coords=True,\n",
        "            subtract_mean=[123, 117, 104],\n",
        "            divide_by_stddev=None,\n",
        "            swap_channels=[2, 1, 0],\n",
        "            confidence_thresh=0.01,\n",
        "            iou_threshold=0.45,\n",
        "            top_k=200,\n",
        "            nms_max_output_size=400,\n",
        "            return_predictor_sizes=False):\n",
        "    '''\n",
        "    Build a Keras model with SSD300 architecture, see references.\n",
        "    The base network is a reduced atrous VGG-16, extended by the SSD architecture,\n",
        "    as described in the paper.\n",
        "    Most of the arguments that this function takes are only needed for the anchor\n",
        "    box layers. In case you're training the network, the parameters passed here must\n",
        "    be the same as the ones used to set up `SSDBoxEncoder`. In case you're loading\n",
        "    trained weights, the parameters passed here must be the same as the ones used\n",
        "    to produce the trained weights.\n",
        "    Some of these arguments are explained in more detail in the documentation of the\n",
        "    `SSDBoxEncoder` class.\n",
        "    Note: Requires Keras v2.0 or later. Currently works only with the\n",
        "    TensorFlow backend (v1.0 or later).\n",
        "    Arguments:\n",
        "        image_size (tuple): The input image size in the format `(height, width, channels)`.\n",
        "        n_classes (int): The number of positive classes, e.g. 20 for Pascal VOC, 80 for MS COCO.\n",
        "        mode (str, optional): One of 'training', 'inference' and 'inference_fast'. In 'training' mode,\n",
        "            the model outputs the raw prediction tensor, while in 'inference' and 'inference_fast' modes,\n",
        "            the raw predictions are decoded into absolute coordinates and filtered via confidence thresholding,\n",
        "            non-maximum suppression, and top-k filtering. The difference between latter two modes is that\n",
        "            'inference' follows the exact procedure of the original Caffe implementation, while\n",
        "            'inference_fast' uses a faster prediction decoding procedure.\n",
        "        l2_regularization (float, optional): The L2-regularization rate. Applies to all convolutional layers.\n",
        "            Set to zero to deactivate L2-regularization.\n",
        "        min_scale (float, optional): The smallest scaling factor for the size of the anchor boxes as a fraction\n",
        "            of the shorter side of the input images.\n",
        "        max_scale (float, optional): The largest scaling factor for the size of the anchor boxes as a fraction\n",
        "            of the shorter side of the input images. All scaling factors between the smallest and the\n",
        "            largest will be linearly interpolated. Note that the second to last of the linearly interpolated\n",
        "            scaling factors will actually be the scaling factor for the last predictor layer, while the last\n",
        "            scaling factor is used for the second box for aspect ratio 1 in the last predictor layer\n",
        "            if `two_boxes_for_ar1` is `True`.\n",
        "        scales (list, optional): A list of floats containing scaling factors per convolutional predictor layer.\n",
        "            This list must be one element longer than the number of predictor layers. The first `k` elements are the\n",
        "            scaling factors for the `k` predictor layers, while the last element is used for the second box\n",
        "            for aspect ratio 1 in the last predictor layer if `two_boxes_for_ar1` is `True`. This additional\n",
        "            last scaling factor must be passed either way, even if it is not being used. If a list is passed,\n",
        "            this argument overrides `min_scale` and `max_scale`. All scaling factors must be greater than zero.\n",
        "        aspect_ratios_global (list, optional): The list of aspect ratios for which anchor boxes are to be\n",
        "            generated. This list is valid for all prediction layers.\n",
        "        aspect_ratios_per_layer (list, optional): A list containing one aspect ratio list for each prediction layer.\n",
        "            This allows you to set the aspect ratios for each predictor layer individually, which is the case for the\n",
        "            original SSD300 implementation. If a list is passed, it overrides `aspect_ratios_global`.\n",
        "        two_boxes_for_ar1 (bool, optional): Only relevant for aspect ratio lists that contain 1. Will be ignored otherwise.\n",
        "            If `True`, two anchor boxes will be generated for aspect ratio 1. The first will be generated\n",
        "            using the scaling factor for the respective layer, the second one will be generated using\n",
        "            geometric mean of said scaling factor and next bigger scaling factor.\n",
        "        steps (list, optional): `None` or a list with as many elements as there are predictor layers. The elements can be\n",
        "            either ints/floats or tuples of two ints/floats. These numbers represent for each predictor layer how many\n",
        "            pixels apart the anchor box center points should be vertically and horizontally along the spatial grid over\n",
        "            the image. If the list contains ints/floats, then that value will be used for both spatial dimensions.\n",
        "            If the list contains tuples of two ints/floats, then they represent `(step_height, step_width)`.\n",
        "            If no steps are provided, then they will be computed such that the anchor box center points will form an\n",
        "            equidistant grid within the image dimensions.\n",
        "        offsets (list, optional): `None` or a list with as many elements as there are predictor layers. The elements can be\n",
        "            either floats or tuples of two floats. These numbers represent for each predictor layer how many\n",
        "            pixels from the top and left boarders of the image the top-most and left-most anchor box center points should be\n",
        "            as a fraction of `steps`. The last bit is important: The offsets are not absolute pixel values, but fractions\n",
        "            of the step size specified in the `steps` argument. If the list contains floats, then that value will\n",
        "            be used for both spatial dimensions. If the list contains tuples of two floats, then they represent\n",
        "            `(vertical_offset, horizontal_offset)`. If no offsets are provided, then they will default to 0.5 of the step size.\n",
        "        clip_boxes (bool, optional): If `True`, clips the anchor box coordinates to stay within image boundaries.\n",
        "        variances (list, optional): A list of 4 floats >0. The anchor box offset for each coordinate will be divided by\n",
        "            its respective variance value.\n",
        "        coords (str, optional): The box coordinate format to be used internally by the model (i.e. this is not the input format\n",
        "            of the ground truth labels). Can be either 'centroids' for the format `(cx, cy, w, h)` (box center coordinates, width,\n",
        "            and height), 'minmax' for the format `(xmin, xmax, ymin, ymax)`, or 'corners' for the format `(xmin, ymin, xmax, ymax)`.\n",
        "        normalize_coords (bool, optional): Set to `True` if the model is supposed to use relative instead of absolute coordinates,\n",
        "            i.e. if the model predicts box coordinates within [0,1] instead of absolute coordinates.\n",
        "        subtract_mean (array-like, optional): `None` or an array-like object of integers or floating point values\n",
        "            of any shape that is broadcast-compatible with the image shape. The elements of this array will be\n",
        "            subtracted from the image pixel intensity values. For example, pass a list of three integers\n",
        "            to perform per-channel mean normalization for color images.\n",
        "        divide_by_stddev (array-like, optional): `None` or an array-like object of non-zero integers or\n",
        "            floating point values of any shape that is broadcast-compatible with the image shape. The image pixel\n",
        "            intensity values will be divided by the elements of this array. For example, pass a list\n",
        "            of three integers to perform per-channel standard deviation normalization for color images.\n",
        "        swap_channels (list, optional): Either `False` or a list of integers representing the desired order in which the input\n",
        "            image channels should be swapped.\n",
        "        confidence_thresh (float, optional): A float in [0,1), the minimum classification confidence in a specific\n",
        "            positive class in order to be considered for the non-maximum suppression stage for the respective class.\n",
        "            A lower value will result in a larger part of the selection process being done by the non-maximum suppression\n",
        "            stage, while a larger value will result in a larger part of the selection process happening in the confidence\n",
        "            thresholding stage.\n",
        "        iou_threshold (float, optional): A float in [0,1]. All boxes that have a Jaccard similarity of greater than `iou_threshold`\n",
        "            with a locally maximal box will be removed from the set of predictions for a given class, where 'maximal' refers\n",
        "            to the box's confidence score.\n",
        "        top_k (int, optional): The number of highest scoring predictions to be kept for each batch item after the\n",
        "            non-maximum suppression stage.\n",
        "        nms_max_output_size (int, optional): The maximal number of predictions that will be left over after the NMS stage.\n",
        "        return_predictor_sizes (bool, optional): If `True`, this function not only returns the model, but also\n",
        "            a list containing the spatial dimensions of the predictor layers. This isn't strictly necessary since\n",
        "            you can always get their sizes easily via the Keras API, but it's convenient and less error-prone\n",
        "            to get them this way. They are only relevant for training anyway (SSDBoxEncoder needs to know the\n",
        "            spatial dimensions of the predictor layers), for inference you don't need them.\n",
        "    Returns:\n",
        "        model: The Keras SSD300 model.\n",
        "        predictor_sizes (optional): A Numpy array containing the `(height, width)` portion\n",
        "            of the output tensor shape for each convolutional predictor layer. During\n",
        "            training, the generator function needs this in order to transform\n",
        "            the ground truth labels into tensors of identical structure as the\n",
        "            output tensors of the model, which is in turn needed for the cost\n",
        "            function.\n",
        "    References:\n",
        "        https://arxiv.org/abs/1512.02325v5\n",
        "    '''\n",
        "\n",
        "    n_predictor_layers = 6 # The number of predictor conv layers in the network is 6 for the original SSD300.\n",
        "    n_classes += 1 # Account for the background class.\n",
        "    l2_reg = l2_regularization # Make the internal name shorter.\n",
        "    img_height, img_width, img_channels = image_size[0], image_size[1], image_size[2]\n",
        "\n",
        "    ############################################################################\n",
        "    # Get a few exceptions out of the way.\n",
        "    ############################################################################\n",
        "\n",
        "    if aspect_ratios_global is None and aspect_ratios_per_layer is None:\n",
        "        raise ValueError(\"`aspect_ratios_global` and `aspect_ratios_per_layer` cannot both be None. At least one needs to be specified.\")\n",
        "    if aspect_ratios_per_layer:\n",
        "        if len(aspect_ratios_per_layer) != n_predictor_layers:\n",
        "            raise ValueError(\"It must be either aspect_ratios_per_layer is None or len(aspect_ratios_per_layer) == {}, but len(aspect_ratios_per_layer) == {}.\".format(n_predictor_layers, len(aspect_ratios_per_layer)))\n",
        "\n",
        "    if (min_scale is None or max_scale is None) and scales is None:\n",
        "        raise ValueError(\"Either `min_scale` and `max_scale` or `scales` need to be specified.\")\n",
        "    if scales:\n",
        "        if len(scales) != n_predictor_layers+1:\n",
        "            raise ValueError(\"It must be either scales is None or len(scales) == {}, but len(scales) == {}.\".format(n_predictor_layers+1, len(scales)))\n",
        "    else: # If no explicit list of scaling factors was passed, compute the list of scaling factors from `min_scale` and `max_scale`\n",
        "        scales = np.linspace(min_scale, max_scale, n_predictor_layers+1)\n",
        "\n",
        "    if len(variances) != 4:\n",
        "        raise ValueError(\"4 variance values must be pased, but {} values were received.\".format(len(variances)))\n",
        "    variances = np.array(variances)\n",
        "    if np.any(variances <= 0):\n",
        "        raise ValueError(\"All variances must be >0, but the variances given are {}\".format(variances))\n",
        "\n",
        "    if (not (steps is None)) and (len(steps) != n_predictor_layers):\n",
        "        raise ValueError(\"You must provide at least one step value per predictor layer.\")\n",
        "\n",
        "    if (not (offsets is None)) and (len(offsets) != n_predictor_layers):\n",
        "        raise ValueError(\"You must provide at least one offset value per predictor layer.\")\n",
        "\n",
        "    ############################################################################\n",
        "    # Compute the anchor box parameters.\n",
        "    ############################################################################\n",
        "\n",
        "    # Set the aspect ratios for each predictor layer. These are only needed for the anchor box layers.\n",
        "    if aspect_ratios_per_layer:\n",
        "        aspect_ratios = aspect_ratios_per_layer\n",
        "    else:\n",
        "        aspect_ratios = [aspect_ratios_global] * n_predictor_layers\n",
        "\n",
        "    # Compute the number of boxes to be predicted per cell for each predictor layer.\n",
        "    # We need this so that we know how many channels the predictor layers need to have.\n",
        "    if aspect_ratios_per_layer:\n",
        "        n_boxes = []\n",
        "        for ar in aspect_ratios_per_layer:\n",
        "            if (1 in ar) & two_boxes_for_ar1:\n",
        "                n_boxes.append(len(ar) + 1) # +1 for the second box for aspect ratio 1\n",
        "            else:\n",
        "                n_boxes.append(len(ar))\n",
        "    else: # If only a global aspect ratio list was passed, then the number of boxes is the same for each predictor layer\n",
        "        if (1 in aspect_ratios_global) & two_boxes_for_ar1:\n",
        "            n_boxes = len(aspect_ratios_global) + 1\n",
        "        else:\n",
        "            n_boxes = len(aspect_ratios_global)\n",
        "        n_boxes = [n_boxes] * n_predictor_layers\n",
        "\n",
        "    if steps is None:\n",
        "        steps = [None] * n_predictor_layers\n",
        "    if offsets is None:\n",
        "        offsets = [None] * n_predictor_layers\n",
        "\n",
        "    ############################################################################\n",
        "    # Define functions for the Lambda layers below.\n",
        "    ############################################################################\n",
        "\n",
        "    def identity_layer(tensor):\n",
        "        return tensor\n",
        "\n",
        "    def input_mean_normalization(tensor):\n",
        "        return tensor - np.array(subtract_mean)\n",
        "\n",
        "    def input_stddev_normalization(tensor):\n",
        "        return tensor / np.array(divide_by_stddev)\n",
        "\n",
        "    def input_channel_swap(tensor):\n",
        "        if len(swap_channels) == 3:\n",
        "            return K.stack([tensor[...,swap_channels[0]], tensor[...,swap_channels[1]], tensor[...,swap_channels[2]]], axis=-1)\n",
        "        elif len(swap_channels) == 4:\n",
        "            return K.stack([tensor[...,swap_channels[0]], tensor[...,swap_channels[1]], tensor[...,swap_channels[2]], tensor[...,swap_channels[3]]], axis=-1)\n",
        "\n",
        "    ############################################################################\n",
        "    # Build the network.\n",
        "    ############################################################################\n",
        "\n",
        "    x = Input(shape=(img_height, img_width, img_channels))\n",
        "\n",
        "    # The following identity layer is only needed so that the subsequent lambda layers can be optional.\n",
        "    x1 = Lambda(identity_layer, output_shape=(img_height, img_width, img_channels), name='identity_layer')(x)\n",
        "    if not (subtract_mean is None):\n",
        "        x1 = Lambda(input_mean_normalization, output_shape=(img_height, img_width, img_channels), name='input_mean_normalization')(x1)\n",
        "    if not (divide_by_stddev is None):\n",
        "        x1 = Lambda(input_stddev_normalization, output_shape=(img_height, img_width, img_channels), name='input_stddev_normalization')(x1)\n",
        "    if swap_channels:\n",
        "        x1 = Lambda(input_channel_swap, output_shape=(img_height, img_width, img_channels), name='input_channel_swap')(x1)\n",
        "\n",
        "    conv1_1 = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv1_1')(x1)\n",
        "    conv1_2 = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv1_2')(conv1_1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same', name='pool1')(conv1_2)\n",
        "\n",
        "    conv2_1 = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv2_1')(pool1)\n",
        "    conv2_2 = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv2_2')(conv2_1)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same', name='pool2')(conv2_2)\n",
        "\n",
        "    conv3_1 = Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv3_1')(pool2)\n",
        "    conv3_2 = Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv3_2')(conv3_1)\n",
        "    conv3_3 = Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv3_3')(conv3_2)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same', name='pool3')(conv3_3)\n",
        "\n",
        "    conv4_1 = Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv4_1')(pool3)\n",
        "    conv4_2 = Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv4_2')(conv4_1)\n",
        "    conv4_3 = Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv4_3')(conv4_2)\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same', name='pool4')(conv4_3)\n",
        "\n",
        "    conv5_1 = Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv5_1')(pool4)\n",
        "    conv5_2 = Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv5_2')(conv5_1)\n",
        "    conv5_3 = Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv5_3')(conv5_2)\n",
        "    pool5 = MaxPooling2D(pool_size=(3, 3), strides=(1, 1), padding='same', name='pool5')(conv5_3)\n",
        "\n",
        "    fc6 = Conv2D(1024, (3, 3), dilation_rate=(6, 6), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='fc6')(pool5)\n",
        "\n",
        "    fc7 = Conv2D(1024, (1, 1), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='fc7')(fc6)\n",
        "\n",
        "    conv6_1 = Conv2D(256, (1, 1), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv6_1')(fc7)\n",
        "    conv6_1 = ZeroPadding2D(padding=((1, 1), (1, 1)), name='conv6_padding')(conv6_1)\n",
        "    conv6_2 = Conv2D(512, (3, 3), strides=(2, 2), activation='relu', padding='valid', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv6_2')(conv6_1)\n",
        "\n",
        "    conv7_1 = Conv2D(128, (1, 1), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv7_1')(conv6_2)\n",
        "    conv7_1 = ZeroPadding2D(padding=((1, 1), (1, 1)), name='conv7_padding')(conv7_1)\n",
        "    conv7_2 = Conv2D(256, (3, 3), strides=(2, 2), activation='relu', padding='valid', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv7_2')(conv7_1)\n",
        "\n",
        "    conv8_1 = Conv2D(128, (1, 1), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv8_1')(conv7_2)\n",
        "    conv8_2 = Conv2D(256, (3, 3), strides=(1, 1), activation='relu', padding='valid', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv8_2')(conv8_1)\n",
        "\n",
        "    conv9_1 = Conv2D(128, (1, 1), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv9_1')(conv8_2)\n",
        "    conv9_2 = Conv2D(256, (3, 3), strides=(1, 1), activation='relu', padding='valid', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv9_2')(conv9_1)\n",
        "\n",
        "    # Feed conv4_3 into the L2 normalization layer\n",
        "    conv4_3_norm = L2Normalization(gamma_init=20, name='conv4_3_norm')(conv4_3)\n",
        "\n",
        "    ### Build the convolutional predictor layers on top of the base network\n",
        "\n",
        "    # We precidt `n_classes` confidence values for each box, hence the confidence predictors have depth `n_boxes * n_classes`\n",
        "    # Output shape of the confidence layers: `(batch, height, width, n_boxes * n_classes)`\n",
        "    conv4_3_norm_mbox_conf = Conv2D(n_boxes[0] * n_classes, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv4_3_norm_mbox_conf')(conv4_3_norm)\n",
        "    fc7_mbox_conf = Conv2D(n_boxes[1] * n_classes, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='fc7_mbox_conf')(fc7)\n",
        "    conv6_2_mbox_conf = Conv2D(n_boxes[2] * n_classes, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv6_2_mbox_conf')(conv6_2)\n",
        "    conv7_2_mbox_conf = Conv2D(n_boxes[3] * n_classes, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv7_2_mbox_conf')(conv7_2)\n",
        "    conv8_2_mbox_conf = Conv2D(n_boxes[4] * n_classes, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv8_2_mbox_conf')(conv8_2)\n",
        "    conv9_2_mbox_conf = Conv2D(n_boxes[5] * n_classes, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv9_2_mbox_conf')(conv9_2)\n",
        "    # We predict 4 box coordinates for each box, hence the localization predictors have depth `n_boxes * 4`\n",
        "    # Output shape of the localization layers: `(batch, height, width, n_boxes * 4)`\n",
        "    conv4_3_norm_mbox_loc = Conv2D(n_boxes[0] * 4, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv4_3_norm_mbox_loc')(conv4_3_norm)\n",
        "    fc7_mbox_loc = Conv2D(n_boxes[1] * 4, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='fc7_mbox_loc')(fc7)\n",
        "    conv6_2_mbox_loc = Conv2D(n_boxes[2] * 4, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv6_2_mbox_loc')(conv6_2)\n",
        "    conv7_2_mbox_loc = Conv2D(n_boxes[3] * 4, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv7_2_mbox_loc')(conv7_2)\n",
        "    conv8_2_mbox_loc = Conv2D(n_boxes[4] * 4, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv8_2_mbox_loc')(conv8_2)\n",
        "    conv9_2_mbox_loc = Conv2D(n_boxes[5] * 4, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv9_2_mbox_loc')(conv9_2)\n",
        "\n",
        "    ### Generate the anchor boxes (called \"priors\" in the original Caffe/C++ implementation, so I'll keep their layer names)\n",
        "\n",
        "    # Output shape of anchors: `(batch, height, width, n_boxes, 8)`\n",
        "    conv4_3_norm_mbox_priorbox = AnchorBoxes(img_height, img_width, this_scale=scales[0], next_scale=scales[1], aspect_ratios=aspect_ratios[0],\n",
        "                                             two_boxes_for_ar1=two_boxes_for_ar1, this_steps=steps[0], this_offsets=offsets[0], clip_boxes=clip_boxes,\n",
        "                                             variances=variances, coords=coords, normalize_coords=normalize_coords, name='conv4_3_norm_mbox_priorbox')(conv4_3_norm_mbox_loc)\n",
        "    fc7_mbox_priorbox = AnchorBoxes(img_height, img_width, this_scale=scales[1], next_scale=scales[2], aspect_ratios=aspect_ratios[1],\n",
        "                                    two_boxes_for_ar1=two_boxes_for_ar1, this_steps=steps[1], this_offsets=offsets[1], clip_boxes=clip_boxes,\n",
        "                                    variances=variances, coords=coords, normalize_coords=normalize_coords, name='fc7_mbox_priorbox')(fc7_mbox_loc)\n",
        "    conv6_2_mbox_priorbox = AnchorBoxes(img_height, img_width, this_scale=scales[2], next_scale=scales[3], aspect_ratios=aspect_ratios[2],\n",
        "                                        two_boxes_for_ar1=two_boxes_for_ar1, this_steps=steps[2], this_offsets=offsets[2], clip_boxes=clip_boxes,\n",
        "                                        variances=variances, coords=coords, normalize_coords=normalize_coords, name='conv6_2_mbox_priorbox')(conv6_2_mbox_loc)\n",
        "    conv7_2_mbox_priorbox = AnchorBoxes(img_height, img_width, this_scale=scales[3], next_scale=scales[4], aspect_ratios=aspect_ratios[3],\n",
        "                                        two_boxes_for_ar1=two_boxes_for_ar1, this_steps=steps[3], this_offsets=offsets[3], clip_boxes=clip_boxes,\n",
        "                                        variances=variances, coords=coords, normalize_coords=normalize_coords, name='conv7_2_mbox_priorbox')(conv7_2_mbox_loc)\n",
        "    conv8_2_mbox_priorbox = AnchorBoxes(img_height, img_width, this_scale=scales[4], next_scale=scales[5], aspect_ratios=aspect_ratios[4],\n",
        "                                        two_boxes_for_ar1=two_boxes_for_ar1, this_steps=steps[4], this_offsets=offsets[4], clip_boxes=clip_boxes,\n",
        "                                        variances=variances, coords=coords, normalize_coords=normalize_coords, name='conv8_2_mbox_priorbox')(conv8_2_mbox_loc)\n",
        "    conv9_2_mbox_priorbox = AnchorBoxes(img_height, img_width, this_scale=scales[5], next_scale=scales[6], aspect_ratios=aspect_ratios[5],\n",
        "                                        two_boxes_for_ar1=two_boxes_for_ar1, this_steps=steps[5], this_offsets=offsets[5], clip_boxes=clip_boxes,\n",
        "                                        variances=variances, coords=coords, normalize_coords=normalize_coords, name='conv9_2_mbox_priorbox')(conv9_2_mbox_loc)\n",
        "\n",
        "    ### Reshape\n",
        "\n",
        "    # Reshape the class predictions, yielding 3D tensors of shape `(batch, height * width * n_boxes, n_classes)`\n",
        "    # We want the classes isolated in the last axis to perform softmax on them\n",
        "    conv4_3_norm_mbox_conf_reshape = Reshape((-1, n_classes), name='conv4_3_norm_mbox_conf_reshape')(conv4_3_norm_mbox_conf)\n",
        "    fc7_mbox_conf_reshape = Reshape((-1, n_classes), name='fc7_mbox_conf_reshape')(fc7_mbox_conf)\n",
        "    conv6_2_mbox_conf_reshape = Reshape((-1, n_classes), name='conv6_2_mbox_conf_reshape')(conv6_2_mbox_conf)\n",
        "    conv7_2_mbox_conf_reshape = Reshape((-1, n_classes), name='conv7_2_mbox_conf_reshape')(conv7_2_mbox_conf)\n",
        "    conv8_2_mbox_conf_reshape = Reshape((-1, n_classes), name='conv8_2_mbox_conf_reshape')(conv8_2_mbox_conf)\n",
        "    conv9_2_mbox_conf_reshape = Reshape((-1, n_classes), name='conv9_2_mbox_conf_reshape')(conv9_2_mbox_conf)\n",
        "    # Reshape the box predictions, yielding 3D tensors of shape `(batch, height * width * n_boxes, 4)`\n",
        "    # We want the four box coordinates isolated in the last axis to compute the smooth L1 loss\n",
        "    conv4_3_norm_mbox_loc_reshape = Reshape((-1, 4), name='conv4_3_norm_mbox_loc_reshape')(conv4_3_norm_mbox_loc)\n",
        "    fc7_mbox_loc_reshape = Reshape((-1, 4), name='fc7_mbox_loc_reshape')(fc7_mbox_loc)\n",
        "    conv6_2_mbox_loc_reshape = Reshape((-1, 4), name='conv6_2_mbox_loc_reshape')(conv6_2_mbox_loc)\n",
        "    conv7_2_mbox_loc_reshape = Reshape((-1, 4), name='conv7_2_mbox_loc_reshape')(conv7_2_mbox_loc)\n",
        "    conv8_2_mbox_loc_reshape = Reshape((-1, 4), name='conv8_2_mbox_loc_reshape')(conv8_2_mbox_loc)\n",
        "    conv9_2_mbox_loc_reshape = Reshape((-1, 4), name='conv9_2_mbox_loc_reshape')(conv9_2_mbox_loc)\n",
        "    # Reshape the anchor box tensors, yielding 3D tensors of shape `(batch, height * width * n_boxes, 8)`\n",
        "    conv4_3_norm_mbox_priorbox_reshape = Reshape((-1, 8), name='conv4_3_norm_mbox_priorbox_reshape')(conv4_3_norm_mbox_priorbox)\n",
        "    fc7_mbox_priorbox_reshape = Reshape((-1, 8), name='fc7_mbox_priorbox_reshape')(fc7_mbox_priorbox)\n",
        "    conv6_2_mbox_priorbox_reshape = Reshape((-1, 8), name='conv6_2_mbox_priorbox_reshape')(conv6_2_mbox_priorbox)\n",
        "    conv7_2_mbox_priorbox_reshape = Reshape((-1, 8), name='conv7_2_mbox_priorbox_reshape')(conv7_2_mbox_priorbox)\n",
        "    conv8_2_mbox_priorbox_reshape = Reshape((-1, 8), name='conv8_2_mbox_priorbox_reshape')(conv8_2_mbox_priorbox)\n",
        "    conv9_2_mbox_priorbox_reshape = Reshape((-1, 8), name='conv9_2_mbox_priorbox_reshape')(conv9_2_mbox_priorbox)\n",
        "\n",
        "    ### Concatenate the predictions from the different layers\n",
        "\n",
        "    # Axis 0 (batch) and axis 2 (n_classes or 4, respectively) are identical for all layer predictions,\n",
        "    # so we want to concatenate along axis 1, the number of boxes per layer\n",
        "    # Output shape of `mbox_conf`: (batch, n_boxes_total, n_classes)\n",
        "    mbox_conf = Concatenate(axis=1, name='mbox_conf')([conv4_3_norm_mbox_conf_reshape,\n",
        "                                                       fc7_mbox_conf_reshape,\n",
        "                                                       conv6_2_mbox_conf_reshape,\n",
        "                                                       conv7_2_mbox_conf_reshape,\n",
        "                                                       conv8_2_mbox_conf_reshape,\n",
        "                                                       conv9_2_mbox_conf_reshape])\n",
        "\n",
        "    # Output shape of `mbox_loc`: (batch, n_boxes_total, 4)\n",
        "    mbox_loc = Concatenate(axis=1, name='mbox_loc')([conv4_3_norm_mbox_loc_reshape,\n",
        "                                                     fc7_mbox_loc_reshape,\n",
        "                                                     conv6_2_mbox_loc_reshape,\n",
        "                                                     conv7_2_mbox_loc_reshape,\n",
        "                                                     conv8_2_mbox_loc_reshape,\n",
        "                                                     conv9_2_mbox_loc_reshape])\n",
        "\n",
        "    # Output shape of `mbox_priorbox`: (batch, n_boxes_total, 8)\n",
        "    mbox_priorbox = Concatenate(axis=1, name='mbox_priorbox')([conv4_3_norm_mbox_priorbox_reshape,\n",
        "                                                               fc7_mbox_priorbox_reshape,\n",
        "                                                               conv6_2_mbox_priorbox_reshape,\n",
        "                                                               conv7_2_mbox_priorbox_reshape,\n",
        "                                                               conv8_2_mbox_priorbox_reshape,\n",
        "                                                               conv9_2_mbox_priorbox_reshape])\n",
        "\n",
        "    # The box coordinate predictions will go into the loss function just the way they are,\n",
        "    # but for the class predictions, we'll apply a softmax activation layer first\n",
        "    mbox_conf_softmax = Activation('softmax', name='mbox_conf_softmax')(mbox_conf)\n",
        "\n",
        "    # Concatenate the class and box predictions and the anchors to one large predictions vector\n",
        "    # Output shape of `predictions`: (batch, n_boxes_total, n_classes + 4 + 8)\n",
        "    predictions = Concatenate(axis=2, name='predictions')([mbox_conf_softmax, mbox_loc, mbox_priorbox])\n",
        "\n",
        "    if mode == 'training':\n",
        "        model = Model(inputs=x, outputs=predictions)\n",
        "    elif mode == 'inference':\n",
        "        decoded_predictions = DecodeDetections(confidence_thresh=confidence_thresh,\n",
        "                                               iou_threshold=iou_threshold,\n",
        "                                               top_k=top_k,\n",
        "                                               nms_max_output_size=nms_max_output_size,\n",
        "                                               coords=coords,\n",
        "                                               normalize_coords=normalize_coords,\n",
        "                                               img_height=img_height,\n",
        "                                               img_width=img_width,\n",
        "                                               name='decoded_predictions')(predictions)\n",
        "        model = Model(inputs=x, outputs=decoded_predictions)\n",
        "    elif mode == 'inference_fast':\n",
        "        decoded_predictions = DecodeDetectionsFast(confidence_thresh=confidence_thresh,\n",
        "                                                   iou_threshold=iou_threshold,\n",
        "                                                   top_k=top_k,\n",
        "                                                   nms_max_output_size=nms_max_output_size,\n",
        "                                                   coords=coords,\n",
        "                                                   normalize_coords=normalize_coords,\n",
        "                                                   img_height=img_height,\n",
        "                                                   img_width=img_width,\n",
        "                                                   name='decoded_predictions')(predictions)\n",
        "        model = Model(inputs=x, outputs=decoded_predictions)\n",
        "    else:\n",
        "        raise ValueError(\"`mode` must be one of 'training', 'inference' or 'inference_fast', but received '{}'.\".format(mode))\n",
        "\n",
        "    if return_predictor_sizes:\n",
        "        predictor_sizes = np.array([conv4_3_norm_mbox_conf._keras_shape[1:3],\n",
        "                                     fc7_mbox_conf._keras_shape[1:3],\n",
        "                                     conv6_2_mbox_conf._keras_shape[1:3],\n",
        "                                     conv7_2_mbox_conf._keras_shape[1:3],\n",
        "                                     conv8_2_mbox_conf._keras_shape[1:3],\n",
        "                                     conv9_2_mbox_conf._keras_shape[1:3]])\n",
        "        return model, predictor_sizes\n",
        "    else:\n",
        "        return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mw53UqGFuQTp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Utilities to match ground truth boxes to anchor boxes.\n",
        "Copyright (C) 2018 Pierluigi Ferrari\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "   http://www.apache.org/licenses/LICENSE-2.0\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "'''\n",
        "\n",
        "from __future__ import division\n",
        "import numpy as np\n",
        "\n",
        "def match_bipartite_greedy(weight_matrix):\n",
        "    '''\n",
        "    Returns a bipartite matching according to the given weight matrix.\n",
        "    The algorithm works as follows:\n",
        "    Let the first axis of `weight_matrix` represent ground truth boxes\n",
        "    and the second axis anchor boxes.\n",
        "    The ground truth box that has the greatest similarity with any\n",
        "    anchor box will be matched first, then out of the remaining ground\n",
        "    truth boxes, the ground truth box that has the greatest similarity\n",
        "    with any of the remaining anchor boxes will be matched second, and\n",
        "    so on. That is, the ground truth boxes will be matched in descending\n",
        "    order by maximum similarity with any of the respectively remaining\n",
        "    anchor boxes.\n",
        "    The runtime complexity is O(m^2 * n), where `m` is the number of\n",
        "    ground truth boxes and `n` is the number of anchor boxes.\n",
        "    Arguments:\n",
        "        weight_matrix (array): A 2D Numpy array that represents the weight matrix\n",
        "            for the matching process. If `(m,n)` is the shape of the weight matrix,\n",
        "            it must be `m <= n`. The weights can be integers or floating point\n",
        "            numbers. The matching process will maximize, i.e. larger weights are\n",
        "            preferred over smaller weights.\n",
        "    Returns:\n",
        "        A 1D Numpy array of length `weight_matrix.shape[0]` that represents\n",
        "        the matched index along the second axis of `weight_matrix` for each index\n",
        "        along the first axis.\n",
        "    '''\n",
        "\n",
        "    weight_matrix = np.copy(weight_matrix) # We'll modify this array.\n",
        "    num_ground_truth_boxes = weight_matrix.shape[0]\n",
        "    all_gt_indices = list(range(num_ground_truth_boxes)) # Only relevant for fancy-indexing below.\n",
        "\n",
        "    # This 1D array will contain for each ground truth box the index of\n",
        "    # the matched anchor box.\n",
        "    matches = np.zeros(num_ground_truth_boxes, dtype=np.int)\n",
        "\n",
        "    # In each iteration of the loop below, exactly one ground truth box\n",
        "    # will be matched to one anchor box.\n",
        "    for _ in range(num_ground_truth_boxes):\n",
        "\n",
        "        # Find the maximal anchor-ground truth pair in two steps: First, reduce\n",
        "        # over the anchor boxes and then reduce over the ground truth boxes.\n",
        "        anchor_indices = np.argmax(weight_matrix, axis=1) # Reduce along the anchor box axis.\n",
        "        overlaps = weight_matrix[all_gt_indices, anchor_indices]\n",
        "        ground_truth_index = np.argmax(overlaps) # Reduce along the ground truth box axis.\n",
        "        anchor_index = anchor_indices[ground_truth_index]\n",
        "        matches[ground_truth_index] = anchor_index # Set the match.\n",
        "\n",
        "        # Set the row of the matched ground truth box and the column of the matched\n",
        "        # anchor box to all zeros. This ensures that those boxes will not be matched again,\n",
        "        # because they will never be the best matches for any other boxes.\n",
        "        weight_matrix[ground_truth_index] = 0\n",
        "        weight_matrix[:,anchor_index] = 0\n",
        "\n",
        "    return matches\n",
        "\n",
        "def match_multi(weight_matrix, threshold):\n",
        "    '''\n",
        "    Matches all elements along the second axis of `weight_matrix` to their best\n",
        "    matches along the first axis subject to the constraint that the weight of a match\n",
        "    must be greater than or equal to `threshold` in order to produce a match.\n",
        "    If the weight matrix contains elements that should be ignored, the row or column\n",
        "    representing the respective elemet should be set to a value below `threshold`.\n",
        "    Arguments:\n",
        "        weight_matrix (array): A 2D Numpy array that represents the weight matrix\n",
        "            for the matching process. If `(m,n)` is the shape of the weight matrix,\n",
        "            it must be `m <= n`. The weights can be integers or floating point\n",
        "            numbers. The matching process will maximize, i.e. larger weights are\n",
        "            preferred over smaller weights.\n",
        "        threshold (float): A float that represents the threshold (i.e. lower bound)\n",
        "            that must be met by a pair of elements to produce a match.\n",
        "    Returns:\n",
        "        Two 1D Numpy arrays of equal length that represent the matched indices. The first\n",
        "        array contains the indices along the first axis of `weight_matrix`, the second array\n",
        "        contains the indices along the second axis.\n",
        "    '''\n",
        "\n",
        "    num_anchor_boxes = weight_matrix.shape[1]\n",
        "    all_anchor_indices = list(range(num_anchor_boxes)) # Only relevant for fancy-indexing below.\n",
        "\n",
        "    # Find the best ground truth match for every anchor box.\n",
        "    ground_truth_indices = np.argmax(weight_matrix, axis=0) # Array of shape (weight_matrix.shape[1],)\n",
        "    overlaps = weight_matrix[ground_truth_indices, all_anchor_indices] # Array of shape (weight_matrix.shape[1],)\n",
        "\n",
        "    # Filter out the matches with a weight below the threshold.\n",
        "    anchor_indices_thresh_met = np.nonzero(overlaps >= threshold)[0]\n",
        "    gt_indices_thresh_met = ground_truth_indices[anchor_indices_thresh_met]\n",
        "\n",
        "    return gt_indices_thresh_met, anchor_indices_thresh_met"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwpO1Af3Ee5H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ssd encoder decoder\n",
        "'''\n",
        "An encoder that converts ground truth annotations to SSD-compatible training targets.\n",
        "Copyright (C) 2018 Pierluigi Ferrari\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "   http://www.apache.org/licenses/LICENSE-2.0\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "'''\n",
        "\n",
        "from __future__ import division\n",
        "import numpy as np\n",
        "\n",
        "#from bounding_box_utils.bounding_box_utils import iou, convert_coordinates\n",
        "#from ssd_encoder_decoder.matching_utils import match_bipartite_greedy, match_multi\n",
        "\n",
        "class SSDInputEncoder:\n",
        "    '''\n",
        "    Transforms ground truth labels for object detection in images\n",
        "    (2D bounding box coordinates and class labels) to the format required for\n",
        "    training an SSD model.\n",
        "    In the process of encoding the ground truth labels, a template of anchor boxes\n",
        "    is being built, which are subsequently matched to the ground truth boxes\n",
        "    via an intersection-over-union threshold criterion.\n",
        "    '''\n",
        "\n",
        "    def __init__(self,\n",
        "                 img_height,\n",
        "                 img_width,\n",
        "                 n_classes,\n",
        "                 predictor_sizes,\n",
        "                 min_scale=0.1,\n",
        "                 max_scale=0.9,\n",
        "                 scales=None,\n",
        "                 aspect_ratios_global=[0.5, 1.0, 2.0],\n",
        "                 aspect_ratios_per_layer=None,\n",
        "                 two_boxes_for_ar1=True,\n",
        "                 steps=None,\n",
        "                 offsets=None,\n",
        "                 clip_boxes=False,\n",
        "                 variances=[0.1, 0.1, 0.2, 0.2],\n",
        "                 matching_type='multi',\n",
        "                 pos_iou_threshold=0.5,\n",
        "                 neg_iou_limit=0.3,\n",
        "                 border_pixels='half',\n",
        "                 coords='centroids',\n",
        "                 normalize_coords=True,\n",
        "                 background_id=0):\n",
        "        '''\n",
        "        Arguments:\n",
        "            img_height (int): The height of the input images.\n",
        "            img_width (int): The width of the input images.\n",
        "            n_classes (int): The number of positive classes, e.g. 20 for Pascal VOC, 80 for MS COCO.\n",
        "            predictor_sizes (list): A list of int-tuples of the format `(height, width)`\n",
        "                containing the output heights and widths of the convolutional predictor layers.\n",
        "            min_scale (float, optional): The smallest scaling factor for the size of the anchor boxes as a fraction\n",
        "                of the shorter side of the input images. Note that you should set the scaling factors\n",
        "                such that the resulting anchor box sizes correspond to the sizes of the objects you are trying\n",
        "                to detect. Must be >0.\n",
        "            max_scale (float, optional): The largest scaling factor for the size of the anchor boxes as a fraction\n",
        "                of the shorter side of the input images. All scaling factors between the smallest and the\n",
        "                largest will be linearly interpolated. Note that the second to last of the linearly interpolated\n",
        "                scaling factors will actually be the scaling factor for the last predictor layer, while the last\n",
        "                scaling factor is used for the second box for aspect ratio 1 in the last predictor layer\n",
        "                if `two_boxes_for_ar1` is `True`. Note that you should set the scaling factors\n",
        "                such that the resulting anchor box sizes correspond to the sizes of the objects you are trying\n",
        "                to detect. Must be greater than or equal to `min_scale`.\n",
        "            scales (list, optional): A list of floats >0 containing scaling factors per convolutional predictor layer.\n",
        "                This list must be one element longer than the number of predictor layers. The first `k` elements are the\n",
        "                scaling factors for the `k` predictor layers, while the last element is used for the second box\n",
        "                for aspect ratio 1 in the last predictor layer if `two_boxes_for_ar1` is `True`. This additional\n",
        "                last scaling factor must be passed either way, even if it is not being used. If a list is passed,\n",
        "                this argument overrides `min_scale` and `max_scale`. All scaling factors must be greater than zero.\n",
        "                Note that you should set the scaling factors such that the resulting anchor box sizes correspond to\n",
        "                the sizes of the objects you are trying to detect.\n",
        "            aspect_ratios_global (list, optional): The list of aspect ratios for which anchor boxes are to be\n",
        "                generated. This list is valid for all prediction layers. Note that you should set the aspect ratios such\n",
        "                that the resulting anchor box shapes roughly correspond to the shapes of the objects you are trying to detect.\n",
        "            aspect_ratios_per_layer (list, optional): A list containing one aspect ratio list for each prediction layer.\n",
        "                If a list is passed, it overrides `aspect_ratios_global`. Note that you should set the aspect ratios such\n",
        "                that the resulting anchor box shapes very roughly correspond to the shapes of the objects you are trying to detect.\n",
        "            two_boxes_for_ar1 (bool, optional): Only relevant for aspect ratios lists that contain 1. Will be ignored otherwise.\n",
        "                If `True`, two anchor boxes will be generated for aspect ratio 1. The first will be generated\n",
        "                using the scaling factor for the respective layer, the second one will be generated using\n",
        "                geometric mean of said scaling factor and next bigger scaling factor.\n",
        "            steps (list, optional): `None` or a list with as many elements as there are predictor layers. The elements can be\n",
        "                either ints/floats or tuples of two ints/floats. These numbers represent for each predictor layer how many\n",
        "                pixels apart the anchor box center points should be vertically and horizontally along the spatial grid over\n",
        "                the image. If the list contains ints/floats, then that value will be used for both spatial dimensions.\n",
        "                If the list contains tuples of two ints/floats, then they represent `(step_height, step_width)`.\n",
        "                If no steps are provided, then they will be computed such that the anchor box center points will form an\n",
        "                equidistant grid within the image dimensions.\n",
        "            offsets (list, optional): `None` or a list with as many elements as there are predictor layers. The elements can be\n",
        "                either floats or tuples of two floats. These numbers represent for each predictor layer how many\n",
        "                pixels from the top and left boarders of the image the top-most and left-most anchor box center points should be\n",
        "                as a fraction of `steps`. The last bit is important: The offsets are not absolute pixel values, but fractions\n",
        "                of the step size specified in the `steps` argument. If the list contains floats, then that value will\n",
        "                be used for both spatial dimensions. If the list contains tuples of two floats, then they represent\n",
        "                `(vertical_offset, horizontal_offset)`. If no offsets are provided, then they will default to 0.5 of the step size.\n",
        "            clip_boxes (bool, optional): If `True`, limits the anchor box coordinates to stay within image boundaries.\n",
        "            variances (list, optional): A list of 4 floats >0. The anchor box offset for each coordinate will be divided by\n",
        "                its respective variance value.\n",
        "            matching_type (str, optional): Can be either 'multi' or 'bipartite'. In 'bipartite' mode, each ground truth box will\n",
        "                be matched only to the one anchor box with the highest IoU overlap. In 'multi' mode, in addition to the aforementioned\n",
        "                bipartite matching, all anchor boxes with an IoU overlap greater than or equal to the `pos_iou_threshold` will be\n",
        "                matched to a given ground truth box.\n",
        "            pos_iou_threshold (float, optional): The intersection-over-union similarity threshold that must be\n",
        "                met in order to match a given ground truth box to a given anchor box.\n",
        "            neg_iou_limit (float, optional): The maximum allowed intersection-over-union similarity of an\n",
        "                anchor box with any ground truth box to be labeled a negative (i.e. background) box. If an\n",
        "                anchor box is neither a positive, nor a negative box, it will be ignored during training.\n",
        "            border_pixels (str, optional): How to treat the border pixels of the bounding boxes.\n",
        "                Can be 'include', 'exclude', or 'half'. If 'include', the border pixels belong\n",
        "                to the boxes. If 'exclude', the border pixels do not belong to the boxes.\n",
        "                If 'half', then one of each of the two horizontal and vertical borders belong\n",
        "                to the boxex, but not the other.\n",
        "            coords (str, optional): The box coordinate format to be used internally by the model (i.e. this is not the input format\n",
        "                of the ground truth labels). Can be either 'centroids' for the format `(cx, cy, w, h)` (box center coordinates, width,\n",
        "                and height), 'minmax' for the format `(xmin, xmax, ymin, ymax)`, or 'corners' for the format `(xmin, ymin, xmax, ymax)`.\n",
        "            normalize_coords (bool, optional): If `True`, the encoder uses relative instead of absolute coordinates.\n",
        "                This means instead of using absolute tartget coordinates, the encoder will scale all coordinates to be within [0,1].\n",
        "                This way learning becomes independent of the input image size.\n",
        "            background_id (int, optional): Determines which class ID is for the background class.\n",
        "        '''\n",
        "        predictor_sizes = np.array(predictor_sizes)\n",
        "        if predictor_sizes.ndim == 1:\n",
        "            predictor_sizes = np.expand_dims(predictor_sizes, axis=0)\n",
        "\n",
        "        ##################################################################################\n",
        "        # Handle exceptions.\n",
        "        ##################################################################################\n",
        "\n",
        "        if (min_scale is None or max_scale is None) and scales is None:\n",
        "            raise ValueError(\"Either `min_scale` and `max_scale` or `scales` need to be specified.\")\n",
        "\n",
        "        if scales:\n",
        "            if (len(scales) != predictor_sizes.shape[0] + 1): # Must be two nested `if` statements since `list` and `bool` cannot be combined by `&`\n",
        "                raise ValueError(\"It must be either scales is None or len(scales) == len(predictor_sizes)+1, but len(scales) == {} and len(predictor_sizes)+1 == {}\".format(len(scales), len(predictor_sizes)+1))\n",
        "            scales = np.array(scales)\n",
        "            if np.any(scales <= 0):\n",
        "                raise ValueError(\"All values in `scales` must be greater than 0, but the passed list of scales is {}\".format(scales))\n",
        "        else: # If no list of scales was passed, we need to make sure that `min_scale` and `max_scale` are valid values.\n",
        "            if not 0 < min_scale <= max_scale:\n",
        "                raise ValueError(\"It must be 0 < min_scale <= max_scale, but it is min_scale = {} and max_scale = {}\".format(min_scale, max_scale))\n",
        "\n",
        "        if not (aspect_ratios_per_layer is None):\n",
        "            if (len(aspect_ratios_per_layer) != predictor_sizes.shape[0]): # Must be two nested `if` statements since `list` and `bool` cannot be combined by `&`\n",
        "                raise ValueError(\"It must be either aspect_ratios_per_layer is None or len(aspect_ratios_per_layer) == len(predictor_sizes), but len(aspect_ratios_per_layer) == {} and len(predictor_sizes) == {}\".format(len(aspect_ratios_per_layer), len(predictor_sizes)))\n",
        "            for aspect_ratios in aspect_ratios_per_layer:\n",
        "                if np.any(np.array(aspect_ratios) <= 0):\n",
        "                    raise ValueError(\"All aspect ratios must be greater than zero.\")\n",
        "        else:\n",
        "            if (aspect_ratios_global is None):\n",
        "                raise ValueError(\"At least one of `aspect_ratios_global` and `aspect_ratios_per_layer` must not be `None`.\")\n",
        "            if np.any(np.array(aspect_ratios_global) <= 0):\n",
        "                raise ValueError(\"All aspect ratios must be greater than zero.\")\n",
        "\n",
        "        if len(variances) != 4:\n",
        "            raise ValueError(\"4 variance values must be pased, but {} values were received.\".format(len(variances)))\n",
        "        variances = np.array(variances)\n",
        "        if np.any(variances <= 0):\n",
        "            raise ValueError(\"All variances must be >0, but the variances given are {}\".format(variances))\n",
        "\n",
        "        if not (coords == 'minmax' or coords == 'centroids' or coords == 'corners'):\n",
        "            raise ValueError(\"Unexpected value for `coords`. Supported values are 'minmax', 'corners' and 'centroids'.\")\n",
        "\n",
        "        if (not (steps is None)) and (len(steps) != predictor_sizes.shape[0]):\n",
        "            raise ValueError(\"You must provide at least one step value per predictor layer.\")\n",
        "\n",
        "        if (not (offsets is None)) and (len(offsets) != predictor_sizes.shape[0]):\n",
        "            raise ValueError(\"You must provide at least one offset value per predictor layer.\")\n",
        "\n",
        "        ##################################################################################\n",
        "        # Set or compute members.\n",
        "        ##################################################################################\n",
        "\n",
        "        self.img_height = img_height\n",
        "        self.img_width = img_width\n",
        "        self.n_classes = n_classes + 1 # + 1 for the background class\n",
        "        self.predictor_sizes = predictor_sizes\n",
        "        self.min_scale = min_scale\n",
        "        self.max_scale = max_scale\n",
        "        # If `scales` is None, compute the scaling factors by linearly interpolating between\n",
        "        # `min_scale` and `max_scale`. If an explicit list of `scales` is given, however,\n",
        "        # then it takes precedent over `min_scale` and `max_scale`.\n",
        "        if (scales is None):\n",
        "            self.scales = np.linspace(self.min_scale, self.max_scale, len(self.predictor_sizes)+1)\n",
        "        else:\n",
        "            # If a list of scales is given explicitly, we'll use that instead of computing it from `min_scale` and `max_scale`.\n",
        "            self.scales = scales\n",
        "        # If `aspect_ratios_per_layer` is None, then we use the same list of aspect ratios\n",
        "        # `aspect_ratios_global` for all predictor layers. If `aspect_ratios_per_layer` is given,\n",
        "        # however, then it takes precedent over `aspect_ratios_global`.\n",
        "        if (aspect_ratios_per_layer is None):\n",
        "            self.aspect_ratios = [aspect_ratios_global] * predictor_sizes.shape[0]\n",
        "        else:\n",
        "            # If aspect ratios are given per layer, we'll use those.\n",
        "            self.aspect_ratios = aspect_ratios_per_layer\n",
        "        self.two_boxes_for_ar1 = two_boxes_for_ar1\n",
        "        if not (steps is None):\n",
        "            self.steps = steps\n",
        "        else:\n",
        "            self.steps = [None] * predictor_sizes.shape[0]\n",
        "        if not (offsets is None):\n",
        "            self.offsets = offsets\n",
        "        else:\n",
        "            self.offsets = [None] * predictor_sizes.shape[0]\n",
        "        self.clip_boxes = clip_boxes\n",
        "        self.variances = variances\n",
        "        self.matching_type = matching_type\n",
        "        self.pos_iou_threshold = pos_iou_threshold\n",
        "        self.neg_iou_limit = neg_iou_limit\n",
        "        self.border_pixels = border_pixels\n",
        "        self.coords = coords\n",
        "        self.normalize_coords = normalize_coords\n",
        "        self.background_id = background_id\n",
        "\n",
        "        # Compute the number of boxes per spatial location for each predictor layer.\n",
        "        # For example, if a predictor layer has three different aspect ratios, [1.0, 0.5, 2.0], and is\n",
        "        # supposed to predict two boxes of slightly different size for aspect ratio 1.0, then that predictor\n",
        "        # layer predicts a total of four boxes at every spatial location across the feature map.\n",
        "        if not (aspect_ratios_per_layer is None):\n",
        "            self.n_boxes = []\n",
        "            for aspect_ratios in aspect_ratios_per_layer:\n",
        "                if (1 in aspect_ratios) & two_boxes_for_ar1:\n",
        "                    self.n_boxes.append(len(aspect_ratios) + 1)\n",
        "                else:\n",
        "                    self.n_boxes.append(len(aspect_ratios))\n",
        "        else:\n",
        "            if (1 in aspect_ratios_global) & two_boxes_for_ar1:\n",
        "                self.n_boxes = len(aspect_ratios_global) + 1\n",
        "            else:\n",
        "                self.n_boxes = len(aspect_ratios_global)\n",
        "\n",
        "        ##################################################################################\n",
        "        # Compute the anchor boxes for each predictor layer.\n",
        "        ##################################################################################\n",
        "\n",
        "        # Compute the anchor boxes for each predictor layer. We only have to do this once\n",
        "        # since the anchor boxes depend only on the model configuration, not on the input data.\n",
        "        # For each predictor layer (i.e. for each scaling factor) the tensors for that layer's\n",
        "        # anchor boxes will have the shape `(feature_map_height, feature_map_width, n_boxes, 4)`.\n",
        "\n",
        "        self.boxes_list = [] # This will store the anchor boxes for each predicotr layer.\n",
        "\n",
        "        # The following lists just store diagnostic information. Sometimes it's handy to have the\n",
        "        # boxes' center points, heights, widths, etc. in a list.\n",
        "        self.wh_list_diag = [] # Box widths and heights for each predictor layer\n",
        "        self.steps_diag = [] # Horizontal and vertical distances between any two boxes for each predictor layer\n",
        "        self.offsets_diag = [] # Offsets for each predictor layer\n",
        "        self.centers_diag = [] # Anchor box center points as `(cy, cx)` for each predictor layer\n",
        "\n",
        "        # Iterate over all predictor layers and compute the anchor boxes for each one.\n",
        "        for i in range(len(self.predictor_sizes)):\n",
        "            boxes, center, wh, step, offset = self.generate_anchor_boxes_for_layer(feature_map_size=self.predictor_sizes[i],\n",
        "                                                                                   aspect_ratios=self.aspect_ratios[i],\n",
        "                                                                                   this_scale=self.scales[i],\n",
        "                                                                                   next_scale=self.scales[i+1],\n",
        "                                                                                   this_steps=self.steps[i],\n",
        "                                                                                   this_offsets=self.offsets[i],\n",
        "                                                                                   diagnostics=True)\n",
        "            self.boxes_list.append(boxes)\n",
        "            self.wh_list_diag.append(wh)\n",
        "            self.steps_diag.append(step)\n",
        "            self.offsets_diag.append(offset)\n",
        "            self.centers_diag.append(center)\n",
        "\n",
        "    def __call__(self, ground_truth_labels, diagnostics=False):\n",
        "        '''\n",
        "        Converts ground truth bounding box data into a suitable format to train an SSD model.\n",
        "        Arguments:\n",
        "            ground_truth_labels (list): A python list of length `batch_size` that contains one 2D Numpy array\n",
        "                for each batch image. Each such array has `k` rows for the `k` ground truth bounding boxes belonging\n",
        "                to the respective image, and the data for each ground truth bounding box has the format\n",
        "                `(class_id, xmin, ymin, xmax, ymax)` (i.e. the 'corners' coordinate format), and `class_id` must be\n",
        "                an integer greater than 0 for all boxes as class ID 0 is reserved for the background class.\n",
        "            diagnostics (bool, optional): If `True`, not only the encoded ground truth tensor will be returned,\n",
        "                but also a copy of it with anchor box coordinates in place of the ground truth coordinates.\n",
        "                This can be very useful if you want to visualize which anchor boxes got matched to which ground truth\n",
        "                boxes.\n",
        "        Returns:\n",
        "            `y_encoded`, a 3D numpy array of shape `(batch_size, #boxes, #classes + 4 + 4 + 4)` that serves as the\n",
        "            ground truth label tensor for training, where `#boxes` is the total number of boxes predicted by the\n",
        "            model per image, and the classes are one-hot-encoded. The four elements after the class vecotrs in\n",
        "            the last axis are the box coordinates, the next four elements after that are just dummy elements, and\n",
        "            the last four elements are the variances.\n",
        "        '''\n",
        "\n",
        "        # Mapping to define which indices represent which coordinates in the ground truth.\n",
        "        class_id = 0\n",
        "        xmin = 1\n",
        "        ymin = 2\n",
        "        xmax = 3\n",
        "        ymax = 4\n",
        "\n",
        "        batch_size = len(ground_truth_labels)\n",
        "\n",
        "        ##################################################################################\n",
        "        # Generate the template for y_encoded.\n",
        "        ##################################################################################\n",
        "\n",
        "        y_encoded = self.generate_encoding_template(batch_size=batch_size, diagnostics=False)\n",
        "\n",
        "        ##################################################################################\n",
        "        # Match ground truth boxes to anchor boxes.\n",
        "        ##################################################################################\n",
        "\n",
        "        # Match the ground truth boxes to the anchor boxes. Every anchor box that does not have\n",
        "        # a ground truth match and for which the maximal IoU overlap with any ground truth box is less\n",
        "        # than or equal to `neg_iou_limit` will be a negative (background) box.\n",
        "\n",
        "        \n",
        "        y_encoded[:, :, self.background_id] = 1 # All boxes are background boxes by default.\n",
        "        n_boxes = y_encoded.shape[1] # The total number of boxes that the model predicts per batch item\n",
        "        class_vectors = np.eye(self.n_classes) # An identity matrix that we'll use as one-hot class vectors\n",
        "\n",
        "        for i in range(batch_size): # For each batch item...\n",
        "\n",
        "            if ground_truth_labels[i].size == 0: continue # If there is no ground truth for this batch item, there is nothing to match.\n",
        "            labels = ground_truth_labels[i].astype(np.float) # The labels for this batch item\n",
        "\n",
        "            # Check for degenerate ground truth bounding boxes before attempting any computations.\n",
        "            if np.any(labels[:,[xmax]] - labels[:,[xmin]] <= 0) or np.any(labels[:,[ymax]] - labels[:,[ymin]] <= 0):\n",
        "                raise DegenerateBoxError(\"SSDInputEncoder detected degenerate ground truth bounding boxes for batch item {} with bounding boxes {}, \".format(i, labels) +\n",
        "                                         \"i.e. bounding boxes where xmax <= xmin and/or ymax <= ymin. Degenerate ground truth \" +\n",
        "                                         \"bounding boxes will lead to NaN errors during the training.\")\n",
        "\n",
        "            # Maybe normalize the box coordinates.\n",
        "            if self.normalize_coords:\n",
        "                labels[:,[ymin,ymax]] /= self.img_height # Normalize ymin and ymax relative to the image height\n",
        "                labels[:,[xmin,xmax]] /= self.img_width # Normalize xmin and xmax relative to the image width\n",
        "\n",
        "            # Maybe convert the box coordinate format.\n",
        "            if self.coords == 'centroids':\n",
        "                labels = convert_coordinates(labels, start_index=xmin, conversion='corners2centroids', border_pixels=self.border_pixels)\n",
        "            elif self.coords == 'minmax':\n",
        "                labels = convert_coordinates(labels, start_index=xmin, conversion='corners2minmax')\n",
        "\n",
        "            classes_one_hot = class_vectors[labels[:, class_id].astype(np.int)] # The one-hot class IDs for the ground truth boxes of this batch item\n",
        "            labels_one_hot = np.concatenate([classes_one_hot, labels[:, [xmin,ymin,xmax,ymax]]], axis=-1) # The one-hot version of the labels for this batch item\n",
        "\n",
        "            # Compute the IoU similarities between all anchor boxes and all ground truth boxes for this batch item.\n",
        "            # This is a matrix of shape `(num_ground_truth_boxes, num_anchor_boxes)`.\n",
        "            similarities = iou(labels[:,[xmin,ymin,xmax,ymax]], y_encoded[i,:,-12:-8], coords=self.coords, mode='outer_product', border_pixels=self.border_pixels)\n",
        "\n",
        "            # First: Do bipartite matching, i.e. match each ground truth box to the one anchor box with the highest IoU.\n",
        "            #        This ensures that each ground truth box will have at least one good match.\n",
        "\n",
        "            # For each ground truth box, get the anchor box to match with it.\n",
        "            bipartite_matches = match_bipartite_greedy(weight_matrix=similarities)\n",
        "\n",
        "            # Write the ground truth data to the matched anchor boxes.\n",
        "            y_encoded[i, bipartite_matches, :-8] = labels_one_hot\n",
        "\n",
        "            # Set the columns of the matched anchor boxes to zero to indicate that they were matched.\n",
        "            similarities[:, bipartite_matches] = 0\n",
        "\n",
        "            # Second: Maybe do 'multi' matching, where each remaining anchor box will be matched to its most similar\n",
        "            #         ground truth box with an IoU of at least `pos_iou_threshold`, or not matched if there is no\n",
        "            #         such ground truth box.\n",
        "\n",
        "            if self.matching_type == 'multi':\n",
        "\n",
        "                # Get all matches that satisfy the IoU threshold.\n",
        "                matches = match_multi(weight_matrix=similarities, threshold=self.pos_iou_threshold)\n",
        "\n",
        "                # Write the ground truth data to the matched anchor boxes.\n",
        "                y_encoded[i, matches[1], :-8] = labels_one_hot[matches[0]]\n",
        "\n",
        "                # Set the columns of the matched anchor boxes to zero to indicate that they were matched.\n",
        "                similarities[:, matches[1]] = 0\n",
        "\n",
        "            # Third: Now after the matching is done, all negative (background) anchor boxes that have\n",
        "            #        an IoU of `neg_iou_limit` or more with any ground truth box will be set to netral,\n",
        "            #        i.e. they will no longer be background boxes. These anchors are \"too close\" to a\n",
        "            #        ground truth box to be valid background boxes.\n",
        "\n",
        "            max_background_similarities = np.amax(similarities, axis=0)\n",
        "            neutral_boxes = np.nonzero(max_background_similarities >= self.neg_iou_limit)[0]\n",
        "            y_encoded[i, neutral_boxes, self.background_id] = 0\n",
        "\n",
        "        ##################################################################################\n",
        "        # Convert box coordinates to anchor box offsets.\n",
        "        ##################################################################################\n",
        "\n",
        "        if self.coords == 'centroids':\n",
        "            y_encoded[:,:,[-12,-11]] -= y_encoded[:,:,[-8,-7]] # cx(gt) - cx(anchor), cy(gt) - cy(anchor)\n",
        "            y_encoded[:,:,[-12,-11]] /= y_encoded[:,:,[-6,-5]] * y_encoded[:,:,[-4,-3]] # (cx(gt) - cx(anchor)) / w(anchor) / cx_variance, (cy(gt) - cy(anchor)) / h(anchor) / cy_variance\n",
        "            y_encoded[:,:,[-10,-9]] /= y_encoded[:,:,[-6,-5]] # w(gt) / w(anchor), h(gt) / h(anchor)\n",
        "            y_encoded[:,:,[-10,-9]] = np.log(y_encoded[:\n",
        "                                                       ,:,[-10,-9]]) / y_encoded[:,:,[-2,-1]] # ln(w(gt) / w(anchor)) / w_variance, ln(h(gt) / h(anchor)) / h_variance (ln == natural logarithm)\n",
        "        elif self.coords == 'corners':\n",
        "            y_encoded[:,:,-12:-8] -= y_encoded[:,:,-8:-4] # (gt - anchor) for all four coordinates\n",
        "            y_encoded[:,:,[-12,-10]] /= np.expand_dims(y_encoded[:,:,-6] - y_encoded[:,:,-8], axis=-1) # (xmin(gt) - xmin(anchor)) / w(anchor), (xmax(gt) - xmax(anchor)) / w(anchor)\n",
        "            y_encoded[:,:,[-11,-9]] /= np.expand_dims(y_encoded[:,:,-5] - y_encoded[:,:,-7], axis=-1) # (ymin(gt) - ymin(anchor)) / h(anchor), (ymax(gt) - ymax(anchor)) / h(anchor)\n",
        "            y_encoded[:,:,-12:-8] /= y_encoded[:,:,-4:] # (gt - anchor) / size(anchor) / variance for all four coordinates, where 'size' refers to w and h respectively\n",
        "        elif self.coords == 'minmax':\n",
        "            y_encoded[:,:,-12:-8] -= y_encoded[:,:,-8:-4] # (gt - anchor) for all four coordinates\n",
        "            y_encoded[:,:,[-12,-11]] /= np.expand_dims(y_encoded[:,:,-7] - y_encoded[:,:,-8], axis=-1) # (xmin(gt) - xmin(anchor)) / w(anchor), (xmax(gt) - xmax(anchor)) / w(anchor)\n",
        "            y_encoded[:,:,[-10,-9]] /= np.expand_dims(y_encoded[:,:,-5] - y_encoded[:,:,-6], axis=-1) # (ymin(gt) - ymin(anchor)) / h(anchor), (ymax(gt) - ymax(anchor)) / h(anchor)\n",
        "            y_encoded[:,:,-12:-8] /= y_encoded[:,:,-4:] # (gt - anchor) / size(anchor) / variance for all four coordinates, where 'size' refers to w and h respectively\n",
        "\n",
        "        if diagnostics:\n",
        "            # Here we'll save the matched anchor boxes (i.e. anchor boxes that were matched to a ground truth box, but keeping the anchor box coordinates).\n",
        "            y_matched_anchors = np.copy(y_encoded)\n",
        "            y_matched_anchors[:,:,-12:-8] = 0 # Keeping the anchor box coordinates means setting the offsets to zero.\n",
        "            return y_encoded, y_matched_anchors\n",
        "        else:\n",
        "            return y_encoded\n",
        "\n",
        "    def generate_anchor_boxes_for_layer(self,\n",
        "                                        feature_map_size,\n",
        "                                        aspect_ratios,\n",
        "                                        this_scale,\n",
        "                                        next_scale,\n",
        "                                        this_steps=None,\n",
        "                                        this_offsets=None,\n",
        "                                        diagnostics=False):\n",
        "        '''\n",
        "        Computes an array of the spatial positions and sizes of the anchor boxes for one predictor layer\n",
        "        of size `feature_map_size == [feature_map_height, feature_map_width]`.\n",
        "        Arguments:\n",
        "            feature_map_size (tuple): A list or tuple `[feature_map_height, feature_map_width]` with the spatial\n",
        "                dimensions of the feature map for which to generate the anchor boxes.\n",
        "            aspect_ratios (list): A list of floats, the aspect ratios for which anchor boxes are to be generated.\n",
        "                All list elements must be unique.\n",
        "            this_scale (float): A float in [0, 1], the scaling factor for the size of the generate anchor boxes\n",
        "                as a fraction of the shorter side of the input image.\n",
        "            next_scale (float): A float in [0, 1], the next larger scaling factor. Only relevant if\n",
        "                `self.two_boxes_for_ar1 == True`.\n",
        "            diagnostics (bool, optional): If true, the following additional outputs will be returned:\n",
        "                1) A list of the center point `x` and `y` coordinates for each spatial location.\n",
        "                2) A list containing `(width, height)` for each box aspect ratio.\n",
        "                3) A tuple containing `(step_height, step_width)`\n",
        "                4) A tuple containing `(offset_height, offset_width)`\n",
        "                This information can be useful to understand in just a few numbers what the generated grid of\n",
        "                anchor boxes actually looks like, i.e. how large the different boxes are and how dense\n",
        "                their spatial distribution is, in order to determine whether the box grid covers the input images\n",
        "                appropriately and whether the box sizes are appropriate to fit the sizes of the objects\n",
        "                to be detected.\n",
        "        Returns:\n",
        "            A 4D Numpy tensor of shape `(feature_map_height, feature_map_width, n_boxes_per_cell, 4)` where the\n",
        "            last dimension contains `(xmin, xmax, ymin, ymax)` for each anchor box in each cell of the feature map.\n",
        "        '''\n",
        "        # Compute box width and height for each aspect ratio.\n",
        "\n",
        "        # The shorter side of the image will be used to compute `w` and `h` using `scale` and `aspect_ratios`.\n",
        "        size = min(self.img_height, self.img_width)\n",
        "        # Compute the box widths and and heights for all aspect ratios\n",
        "        wh_list = []\n",
        "        for ar in aspect_ratios:\n",
        "            if (ar == 1):\n",
        "                # Compute the regular anchor box for aspect ratio 1.\n",
        "                box_height = box_width = this_scale * size\n",
        "                wh_list.append((box_width, box_height))\n",
        "                if self.two_boxes_for_ar1:\n",
        "                    # Compute one slightly larger version using the geometric mean of this scale value and the next.\n",
        "                    box_height = box_width = np.sqrt(this_scale * next_scale) * size\n",
        "                    wh_list.append((box_width, box_height))\n",
        "            else:\n",
        "                box_width = this_scale * size * np.sqrt(ar)\n",
        "                box_height = this_scale * size / np.sqrt(ar)\n",
        "                wh_list.append((box_width, box_height))\n",
        "        wh_list = np.array(wh_list)\n",
        "        n_boxes = len(wh_list)\n",
        "\n",
        "        # Compute the grid of box center points. They are identical for all aspect ratios.\n",
        "\n",
        "        # Compute the step sizes, i.e. how far apart the anchor box center points will be vertically and horizontally.\n",
        "        if (this_steps is None):\n",
        "            step_height = self.img_height / feature_map_size[0]\n",
        "            step_width = self.img_width / feature_map_size[1]\n",
        "        else:\n",
        "            if isinstance(this_steps, (list, tuple)) and (len(this_steps) == 2):\n",
        "                step_height = this_steps[0]\n",
        "                step_width = this_steps[1]\n",
        "            elif isinstance(this_steps, (int, float)):\n",
        "                step_height = this_steps\n",
        "                step_width = this_steps\n",
        "        # Compute the offsets, i.e. at what pixel values the first anchor box center point will be from the top and from the left of the image.\n",
        "        if (this_offsets is None):\n",
        "            offset_height = 0.5\n",
        "            offset_width = 0.5\n",
        "        else:\n",
        "            if isinstance(this_offsets, (list, tuple)) and (len(this_offsets) == 2):\n",
        "                offset_height = this_offsets[0]\n",
        "                offset_width = this_offsets[1]\n",
        "            elif isinstance(this_offsets, (int, float)):\n",
        "                offset_height = this_offsets\n",
        "                offset_width = this_offsets\n",
        "        # Now that we have the offsets and step sizes, compute the grid of anchor box center points.\n",
        "        cy = np.linspace(offset_height * step_height, (offset_height + feature_map_size[0] - 1) * step_height, feature_map_size[0])\n",
        "        cx = np.linspace(offset_width * step_width, (offset_width + feature_map_size[1] - 1) * step_width, feature_map_size[1])\n",
        "        cx_grid, cy_grid = np.meshgrid(cx, cy)\n",
        "        cx_grid = np.expand_dims(cx_grid, -1) # This is necessary for np.tile() to do what we want further down\n",
        "        cy_grid = np.expand_dims(cy_grid, -1) # This is necessary for np.tile() to do what we want further down\n",
        "\n",
        "        # Create a 4D tensor template of shape `(feature_map_height, feature_map_width, n_boxes, 4)`\n",
        "        # where the last dimension will contain `(cx, cy, w, h)`\n",
        "        boxes_tensor = np.zeros((feature_map_size[0], feature_map_size[1], n_boxes, 4))\n",
        "\n",
        "        boxes_tensor[:, :, :, 0] = np.tile(cx_grid, (1, 1, n_boxes)) # Set cx\n",
        "        boxes_tensor[:, :, :, 1] = np.tile(cy_grid, (1, 1, n_boxes)) # Set cy\n",
        "        boxes_tensor[:, :, :, 2] = wh_list[:, 0] # Set w\n",
        "        boxes_tensor[:, :, :, 3] = wh_list[:, 1] # Set h\n",
        "\n",
        "        # Convert `(cx, cy, w, h)` to `(xmin, ymin, xmax, ymax)`\n",
        "        boxes_tensor = convert_coordinates(boxes_tensor, start_index=0, conversion='centroids2corners')\n",
        "\n",
        "        # If `clip_boxes` is enabled, clip the coordinates to lie within the image boundaries\n",
        "        if self.clip_boxes:\n",
        "            x_coords = boxes_tensor[:,:,:,[0, 2]]\n",
        "            x_coords[x_coords >= self.img_width] = self.img_width - 1\n",
        "            x_coords[x_coords < 0] = 0\n",
        "            boxes_tensor[:,:,:,[0, 2]] = x_coords\n",
        "            y_coords = boxes_tensor[:,:,:,[1, 3]]\n",
        "            y_coords[y_coords >= self.img_height] = self.img_height - 1\n",
        "            y_coords[y_coords < 0] = 0\n",
        "            boxes_tensor[:,:,:,[1, 3]] = y_coords\n",
        "\n",
        "        # `normalize_coords` is enabled, normalize the coordinates to be within [0,1]\n",
        "        if self.normalize_coords:\n",
        "            boxes_tensor[:, :, :, [0, 2]] /= self.img_width\n",
        "            boxes_tensor[:, :, :, [1, 3]] /= self.img_height\n",
        "\n",
        "        # TODO: Implement box limiting directly for `(cx, cy, w, h)` so that we don't have to unnecessarily convert back and forth.\n",
        "        if self.coords == 'centroids':\n",
        "            # Convert `(xmin, ymin, xmax, ymax)` back to `(cx, cy, w, h)`.\n",
        "            boxes_tensor = convert_coordinates(boxes_tensor, start_index=0, conversion='corners2centroids', border_pixels='half')\n",
        "        elif self.coords == 'minmax':\n",
        "            # Convert `(xmin, ymin, xmax, ymax)` to `(xmin, xmax, ymin, ymax).\n",
        "            boxes_tensor = convert_coordinates(boxes_tensor, start_index=0, conversion='corners2minmax', border_pixels='half')\n",
        "\n",
        "        if diagnostics:\n",
        "            return boxes_tensor, (cy, cx), wh_list, (step_height, step_width), (offset_height, offset_width)\n",
        "        else:\n",
        "            return boxes_tensor\n",
        "\n",
        "    def generate_encoding_template(self, batch_size, diagnostics=False):\n",
        "        '''\n",
        "        Produces an encoding template for the ground truth label tensor for a given batch.\n",
        "        Note that all tensor creation, reshaping and concatenation operations performed in this function\n",
        "        and the sub-functions it calls are identical to those performed inside the SSD model. This, of course,\n",
        "        must be the case in order to preserve the spatial meaning of each box prediction, but it's useful to make\n",
        "        yourself aware of this fact and why it is necessary.\n",
        "        In other words, the boxes in `y_encoded` must have a specific order in order correspond to the right spatial\n",
        "        positions and scales of the boxes predicted by the model. The sequence of operations here ensures that `y_encoded`\n",
        "        has this specific form.\n",
        "        Arguments:\n",
        "            batch_size (int): The batch size.\n",
        "            diagnostics (bool, optional): See the documnentation for `generate_anchor_boxes()`. The diagnostic output\n",
        "                here is similar, just for all predictor conv layers.\n",
        "        Returns:\n",
        "            A Numpy array of shape `(batch_size, #boxes, #classes + 12)`, the template into which to encode\n",
        "            the ground truth labels for training. The last axis has length `#classes + 12` because the model\n",
        "            output contains not only the 4 predicted box coordinate offsets, but also the 4 coordinates for\n",
        "            the anchor boxes and the 4 variance values.\n",
        "        '''\n",
        "        # Tile the anchor boxes for each predictor layer across all batch items.\n",
        "        boxes_batch = []\n",
        "        for boxes in self.boxes_list:\n",
        "            # Prepend one dimension to `self.boxes_list` to account for the batch size and tile it along.\n",
        "            # The result will be a 5D tensor of shape `(batch_size, feature_map_height, feature_map_width, n_boxes, 4)`\n",
        "            boxes = np.expand_dims(boxes, axis=0)\n",
        "            boxes = np.tile(boxes, (batch_size, 1, 1, 1, 1))\n",
        "\n",
        "            # Now reshape the 5D tensor above into a 3D tensor of shape\n",
        "            # `(batch, feature_map_height * feature_map_width * n_boxes, 4)`. The resulting\n",
        "            # order of the tensor content will be identical to the order obtained from the reshaping operation\n",
        "            # in our Keras model (we're using the Tensorflow backend, and tf.reshape() and np.reshape()\n",
        "            # use the same default index order, which is C-like index ordering)\n",
        "            boxes = np.reshape(boxes, (batch_size, -1, 4))\n",
        "            boxes_batch.append(boxes)\n",
        "\n",
        "        # Concatenate the anchor tensors from the individual layers to one.\n",
        "        boxes_tensor = np.concatenate(boxes_batch, axis=1)\n",
        "\n",
        "        # 3: Create a template tensor to hold the one-hot class encodings of shape `(batch, #boxes, #classes)`\n",
        "        #    It will contain all zeros for now, the classes will be set in the matching process that follows\n",
        "        classes_tensor = np.zeros((batch_size, boxes_tensor.shape[1], self.n_classes))\n",
        "\n",
        "        # 4: Create a tensor to contain the variances. This tensor has the same shape as `boxes_tensor` and simply\n",
        "        #    contains the same 4 variance values for every position in the last axis.\n",
        "        variances_tensor = np.zeros_like(boxes_tensor)\n",
        "        variances_tensor += self.variances # Long live broadcasting\n",
        "\n",
        "        # 4: Concatenate the classes, boxes and variances tensors to get our final template for y_encoded. We also need\n",
        "        #    another tensor of the shape of `boxes_tensor` as a space filler so that `y_encoding_template` has the same\n",
        "        #    shape as the SSD model output tensor. The content of this tensor is irrelevant, we'll just use\n",
        "        #    `boxes_tensor` a second time.\n",
        "        y_encoding_template = np.concatenate((classes_tensor, boxes_tensor, boxes_tensor, variances_tensor), axis=2)\n",
        "\n",
        "        if diagnostics:\n",
        "            return y_encoding_template, self.centers_diag, self.wh_list_diag, self.steps_diag, self.offsets_diag\n",
        "        else:\n",
        "            return y_encoding_template\n",
        "\n",
        "class DegenerateBoxError(Exception):\n",
        "    '''\n",
        "    An exception class to be raised if degenerate boxes are being detected.\n",
        "    '''\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXkAb3yjEe2t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "''' ssd output decoder\n",
        "Includes:\n",
        "* Functions to decode and filter raw SSD model output. These are only needed if the\n",
        "  SSD model does not have a `DecodeDetections` layer.\n",
        "* Functions to perform greedy non-maximum suppression\n",
        "Copyright (C) 2018 Pierluigi Ferrari\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "   http://www.apache.org/licenses/LICENSE-2.0\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "'''\n",
        "\n",
        "from __future__ import division\n",
        "import numpy as np\n",
        "\n",
        "#from bounding_box_utils.bounding_box_utils import iou, convert_coordinates\n",
        "\n",
        "def greedy_nms(y_pred_decoded, iou_threshold=0.45, coords='corners', border_pixels='half'):\n",
        "    '''\n",
        "    Perform greedy non-maximum suppression on the input boxes.\n",
        "    Greedy NMS works by selecting the box with the highest score and\n",
        "    removing all boxes around it that are too close to it measured by IoU-similarity.\n",
        "    Out of the boxes that are left over, once again the one with the highest\n",
        "    score is selected and so on, until no boxes with too much overlap are left.\n",
        "    Arguments:\n",
        "        y_pred_decoded (list): A batch of decoded predictions. For a given batch size `n` this\n",
        "            is a list of length `n` where each list element is a 2D Numpy array.\n",
        "            For a batch item with `k` predicted boxes this 2D Numpy array has\n",
        "            shape `(k, 6)`, where each row contains the coordinates of the respective\n",
        "            box in the format `[class_id, score, xmin, xmax, ymin, ymax]`.\n",
        "            Technically, the number of columns doesn't have to be 6, it can be\n",
        "            arbitrary as long as the first four elements of each row are\n",
        "            `xmin`, `xmax`, `ymin`, `ymax` (in this order) and the last element\n",
        "            is the score assigned to the prediction. Note that this function is\n",
        "            agnostic to the scale of the score or what it represents.\n",
        "        iou_threshold (float, optional): All boxes with a Jaccard similarity of\n",
        "            greater than `iou_threshold` with a locally maximal box will be removed\n",
        "            from the set of predictions, where 'maximal' refers to the box score.\n",
        "        coords (str, optional): The coordinate format of `y_pred_decoded`.\n",
        "            Can be one of the formats supported by `iou()`.\n",
        "        border_pixels (str, optional): How to treat the border pixels of the bounding boxes.\n",
        "            Can be 'include', 'exclude', or 'half'. If 'include', the border pixels belong\n",
        "            to the boxes. If 'exclude', the border pixels do not belong to the boxes.\n",
        "            If 'half', then one of each of the two horizontal and vertical borders belong\n",
        "            to the boxex, but not the other.\n",
        "    Returns:\n",
        "        The predictions after removing non-maxima. The format is the same as the input format.\n",
        "    '''\n",
        "    y_pred_decoded_nms = []\n",
        "    for batch_item in y_pred_decoded: # For the labels of each batch item...\n",
        "        boxes_left = np.copy(batch_item)\n",
        "        maxima = [] # This is where we store the boxes that make it through the non-maximum suppression\n",
        "        while boxes_left.shape[0] > 0: # While there are still boxes left to compare...\n",
        "            maximum_index = np.argmax(boxes_left[:,1]) # ...get the index of the next box with the highest confidence...\n",
        "            maximum_box = np.copy(boxes_left[maximum_index]) # ...copy that box and...\n",
        "            maxima.append(maximum_box) # ...append it to `maxima` because we'll definitely keep it\n",
        "            boxes_left = np.delete(boxes_left, maximum_index, axis=0) # Now remove the maximum box from `boxes_left`\n",
        "            if boxes_left.shape[0] == 0: break # If there are no boxes left after this step, break. Otherwise...\n",
        "            similarities = iou(boxes_left[:,2:], maximum_box[2:], coords=coords, mode='element-wise', border_pixels=border_pixels) # ...compare (IoU) the other left over boxes to the maximum box...\n",
        "            boxes_left = boxes_left[similarities <= iou_threshold] # ...so that we can remove the ones that overlap too much with the maximum box\n",
        "        y_pred_decoded_nms.append(np.array(maxima))\n",
        "\n",
        "    return y_pred_decoded_nms\n",
        "\n",
        "def _greedy_nms(predictions, iou_threshold=0.45, coords='corners', border_pixels='half'):\n",
        "    '''\n",
        "    The same greedy non-maximum suppression algorithm as above, but slightly modified for use as an internal\n",
        "    function for per-class NMS in `decode_detections()`.\n",
        "    '''\n",
        "    boxes_left = np.copy(predictions)\n",
        "    maxima = [] # This is where we store the boxes that make it through the non-maximum suppression\n",
        "    while boxes_left.shape[0] > 0: # While there are still boxes left to compare...\n",
        "        maximum_index = np.argmax(boxes_left[:,0]) # ...get the index of the next box with the highest confidence...\n",
        "        maximum_box = np.copy(boxes_left[maximum_index]) # ...copy that box and...\n",
        "        maxima.append(maximum_box) # ...append it to `maxima` because we'll definitely keep it\n",
        "        boxes_left = np.delete(boxes_left, maximum_index, axis=0) # Now remove the maximum box from `boxes_left`\n",
        "        if boxes_left.shape[0] == 0: break # If there are no boxes left after this step, break. Otherwise...\n",
        "        similarities = iou(boxes_left[:,1:], maximum_box[1:], coords=coords, mode='element-wise', border_pixels=border_pixels) # ...compare (IoU) the other left over boxes to the maximum box...\n",
        "        boxes_left = boxes_left[similarities <= iou_threshold] # ...so that we can remove the ones that overlap too much with the maximum box\n",
        "    return np.array(maxima)\n",
        "\n",
        "def _greedy_nms2(predictions, iou_threshold=0.45, coords='corners', border_pixels='half'):\n",
        "    '''\n",
        "    The same greedy non-maximum suppression algorithm as above, but slightly modified for use as an internal\n",
        "    function in `decode_detections_fast()`.\n",
        "    '''\n",
        "    boxes_left = np.copy(predictions)\n",
        "    maxima = [] # This is where we store the boxes that make it through the non-maximum suppression\n",
        "    while boxes_left.shape[0] > 0: # While there are still boxes left to compare...\n",
        "        maximum_index = np.argmax(boxes_left[:,1]) # ...get the index of the next box with the highest confidence...\n",
        "        maximum_box = np.copy(boxes_left[maximum_index]) # ...copy that box and...\n",
        "        maxima.append(maximum_box) # ...append it to `maxima` because we'll definitely keep it\n",
        "        boxes_left = np.delete(boxes_left, maximum_index, axis=0) # Now remove the maximum box from `boxes_left`\n",
        "        if boxes_left.shape[0] == 0: break # If there are no boxes left after this step, break. Otherwise...\n",
        "        similarities = iou(boxes_left[:,2:], maximum_box[2:], coords=coords, mode='element-wise', border_pixels=border_pixels) # ...compare (IoU) the other left over boxes to the maximum box...\n",
        "        boxes_left = boxes_left[similarities <= iou_threshold] # ...so that we can remove the ones that overlap too much with the maximum box\n",
        "    return np.array(maxima)\n",
        "\n",
        "def decode_detections(y_pred,\n",
        "                      confidence_thresh=0.01,\n",
        "                      iou_threshold=0.45,\n",
        "                      top_k=200,\n",
        "                      input_coords='centroids',\n",
        "                      normalize_coords=True,\n",
        "                      img_height=None,\n",
        "                      img_width=None,\n",
        "                      border_pixels='half'):\n",
        "    '''\n",
        "    Convert model prediction output back to a format that contains only the positive box predictions\n",
        "    (i.e. the same format that `SSDInputEncoder` takes as input).\n",
        "    After the decoding, two stages of prediction filtering are performed for each class individually:\n",
        "    First confidence thresholding, then greedy non-maximum suppression. The filtering results for all\n",
        "    classes are concatenated and the `top_k` overall highest confidence results constitute the final\n",
        "    predictions for a given batch item. This procedure follows the original Caffe implementation.\n",
        "    For a slightly different and more efficient alternative to decode raw model output that performs\n",
        "    non-maximum suppresion globally instead of per class, see `decode_detections_fast()` below.\n",
        "    Arguments:\n",
        "        y_pred (array): The prediction output of the SSD model, expected to be a Numpy array\n",
        "            of shape `(batch_size, #boxes, #classes + 4 + 4 + 4)`, where `#boxes` is the total number of\n",
        "            boxes predicted by the model per image and the last axis contains\n",
        "            `[one-hot vector for the classes, 4 predicted coordinate offsets, 4 anchor box coordinates, 4 variances]`.\n",
        "        confidence_thresh (float, optional): A float in [0,1), the minimum classification confidence in a specific\n",
        "            positive class in order to be considered for the non-maximum suppression stage for the respective class.\n",
        "            A lower value will result in a larger part of the selection process being done by the non-maximum suppression\n",
        "            stage, while a larger value will result in a larger part of the selection process happening in the confidence\n",
        "            thresholding stage.\n",
        "        iou_threshold (float, optional): A float in [0,1]. All boxes with a Jaccard similarity of greater than `iou_threshold`\n",
        "            with a locally maximal box will be removed from the set of predictions for a given class, where 'maximal' refers\n",
        "            to the box score.\n",
        "        top_k (int, optional): The number of highest scoring predictions to be kept for each batch item after the\n",
        "            non-maximum suppression stage.\n",
        "        input_coords (str, optional): The box coordinate format that the model outputs. Can be either 'centroids'\n",
        "            for the format `(cx, cy, w, h)` (box center coordinates, width, and height), 'minmax' for the format\n",
        "            `(xmin, xmax, ymin, ymax)`, or 'corners' for the format `(xmin, ymin, xmax, ymax)`.\n",
        "        normalize_coords (bool, optional): Set to `True` if the model outputs relative coordinates (i.e. coordinates in [0,1])\n",
        "            and you wish to transform these relative coordinates back to absolute coordinates. If the model outputs\n",
        "            relative coordinates, but you do not want to convert them back to absolute coordinates, set this to `False`.\n",
        "            Do not set this to `True` if the model already outputs absolute coordinates, as that would result in incorrect\n",
        "            coordinates. Requires `img_height` and `img_width` if set to `True`.\n",
        "        img_height (int, optional): The height of the input images. Only needed if `normalize_coords` is `True`.\n",
        "        img_width (int, optional): The width of the input images. Only needed if `normalize_coords` is `True`.\n",
        "        border_pixels (str, optional): How to treat the border pixels of the bounding boxes.\n",
        "            Can be 'include', 'exclude', or 'half'. If 'include', the border pixels belong\n",
        "            to the boxes. If 'exclude', the border pixels do not belong to the boxes.\n",
        "            If 'half', then one of each of the two horizontal and vertical borders belong\n",
        "            to the boxex, but not the other.\n",
        "    Returns:\n",
        "        A python list of length `batch_size` where each list element represents the predicted boxes\n",
        "        for one image and contains a Numpy array of shape `(boxes, 6)` where each row is a box prediction for\n",
        "        a non-background class for the respective image in the format `[class_id, confidence, xmin, ymin, xmax, ymax]`.\n",
        "    '''\n",
        "    if normalize_coords and ((img_height is None) or (img_width is None)):\n",
        "        raise ValueError(\"If relative box coordinates are supposed to be converted to absolute coordinates, the decoder needs the image size in order to decode the predictions, but `img_height == {}` and `img_width == {}`\".format(img_height, img_width))\n",
        "\n",
        "    # 1: Convert the box coordinates from the predicted anchor box offsets to predicted absolute coordinates\n",
        "\n",
        "    y_pred_decoded_raw = np.copy(y_pred[:,:,:-8]) # Slice out the classes and the four offsets, throw away the anchor coordinates and variances, resulting in a tensor of shape `[batch, n_boxes, n_classes + 4 coordinates]`\n",
        "\n",
        "    if input_coords == 'centroids':\n",
        "        y_pred_decoded_raw[:,:,[-2,-1]] = np.exp(y_pred_decoded_raw[:,:,[-2,-1]] * y_pred[:,:,[-2,-1]]) # exp(ln(w(pred)/w(anchor)) / w_variance * w_variance) == w(pred) / w(anchor), exp(ln(h(pred)/h(anchor)) / h_variance * h_variance) == h(pred) / h(anchor)\n",
        "        y_pred_decoded_raw[:,:,[-2,-1]] *= y_pred[:,:,[-6,-5]] # (w(pred) / w(anchor)) * w(anchor) == w(pred), (h(pred) / h(anchor)) * h(anchor) == h(pred)\n",
        "        y_pred_decoded_raw[:,:,[-4,-3]] *= y_pred[:,:,[-4,-3]] * y_pred[:,:,[-6,-5]] # (delta_cx(pred) / w(anchor) / cx_variance) * cx_variance * w(anchor) == delta_cx(pred), (delta_cy(pred) / h(anchor) / cy_variance) * cy_variance * h(anchor) == delta_cy(pred)\n",
        "        y_pred_decoded_raw[:,:,[-4,-3]] += y_pred[:,:,[-8,-7]] # delta_cx(pred) + cx(anchor) == cx(pred), delta_cy(pred) + cy(anchor) == cy(pred)\n",
        "        y_pred_decoded_raw = convert_coordinates(y_pred_decoded_raw, start_index=-4, conversion='centroids2corners')\n",
        "    elif input_coords == 'minmax':\n",
        "        y_pred_decoded_raw[:,:,-4:] *= y_pred[:,:,-4:] # delta(pred) / size(anchor) / variance * variance == delta(pred) / size(anchor) for all four coordinates, where 'size' refers to w or h, respectively\n",
        "        y_pred_decoded_raw[:,:,[-4,-3]] *= np.expand_dims(y_pred[:,:,-7] - y_pred[:,:,-8], axis=-1) # delta_xmin(pred) / w(anchor) * w(anchor) == delta_xmin(pred), delta_xmax(pred) / w(anchor) * w(anchor) == delta_xmax(pred)\n",
        "        y_pred_decoded_raw[:,:,[-2,-1]] *= np.expand_dims(y_pred[:,:,-5] - y_pred[:,:,-6], axis=-1) # delta_ymin(pred) / h(anchor) * h(anchor) == delta_ymin(pred), delta_ymax(pred) / h(anchor) * h(anchor) == delta_ymax(pred)\n",
        "        y_pred_decoded_raw[:,:,-4:] += y_pred[:,:,-8:-4] # delta(pred) + anchor == pred for all four coordinates\n",
        "        y_pred_decoded_raw = convert_coordinates(y_pred_decoded_raw, start_index=-4, conversion='minmax2corners')\n",
        "    elif input_coords == 'corners':\n",
        "        y_pred_decoded_raw[:,:,-4:] *= y_pred[:,:,-4:] # delta(pred) / size(anchor) / variance * variance == delta(pred) / size(anchor) for all four coordinates, where 'size' refers to w or h, respectively\n",
        "        y_pred_decoded_raw[:,:,[-4,-2]] *= np.expand_dims(y_pred[:,:,-6] - y_pred[:,:,-8], axis=-1) # delta_xmin(pred) / w(anchor) * w(anchor) == delta_xmin(pred), delta_xmax(pred) / w(anchor) * w(anchor) == delta_xmax(pred)\n",
        "        y_pred_decoded_raw[:,:,[-3,-1]] *= np.expand_dims(y_pred[:,:,-5] - y_pred[:,:,-7], axis=-1) # delta_ymin(pred) / h(anchor) * h(anchor) == delta_ymin(pred), delta_ymax(pred) / h(anchor) * h(anchor) == delta_ymax(pred)\n",
        "        y_pred_decoded_raw[:,:,-4:] += y_pred[:,:,-8:-4] # delta(pred) + anchor == pred for all four coordinates\n",
        "    else:\n",
        "        raise ValueError(\"Unexpected value for `input_coords`. Supported input coordinate formats are 'minmax', 'corners' and 'centroids'.\")\n",
        "\n",
        "    # 2: If the model predicts normalized box coordinates and they are supposed to be converted back to absolute coordinates, do that\n",
        "\n",
        "    if normalize_coords:\n",
        "        y_pred_decoded_raw[:,:,[-4,-2]] *= img_width # Convert xmin, xmax back to absolute coordinates\n",
        "        y_pred_decoded_raw[:,:,[-3,-1]] *= img_height # Convert ymin, ymax back to absolute coordinates\n",
        "\n",
        "    # 3: Apply confidence thresholding and non-maximum suppression per class\n",
        "\n",
        "    n_classes = y_pred_decoded_raw.shape[-1] - 4 # The number of classes is the length of the last axis minus the four box coordinates\n",
        "\n",
        "    y_pred_decoded = [] # Store the final predictions in this list\n",
        "    for batch_item in y_pred_decoded_raw: # `batch_item` has shape `[n_boxes, n_classes + 4 coords]`\n",
        "        pred = [] # Store the final predictions for this batch item here\n",
        "        for class_id in range(1, n_classes): # For each class except the background class (which has class ID 0)...\n",
        "            single_class = batch_item[:,[class_id, -4, -3, -2, -1]] # ...keep only the confidences for that class, making this an array of shape `[n_boxes, 5]` and...\n",
        "            threshold_met = single_class[single_class[:,0] > confidence_thresh] # ...keep only those boxes with a confidence above the set threshold.\n",
        "            if threshold_met.shape[0] > 0: # If any boxes made the threshold...\n",
        "                maxima = _greedy_nms(threshold_met, iou_threshold=iou_threshold, coords='corners', border_pixels=border_pixels) # ...perform NMS on them.\n",
        "                maxima_output = np.zeros((maxima.shape[0], maxima.shape[1] + 1)) # Expand the last dimension by one element to have room for the class ID. This is now an arrray of shape `[n_boxes, 6]`\n",
        "                maxima_output[:,0] = class_id # Write the class ID to the first column...\n",
        "                maxima_output[:,1:] = maxima # ...and write the maxima to the other columns...\n",
        "                pred.append(maxima_output) # ...and append the maxima for this class to the list of maxima for this batch item.\n",
        "        # Once we're through with all classes, keep only the `top_k` maxima with the highest scores\n",
        "        if pred: # If there are any predictions left after confidence-thresholding...\n",
        "            pred = np.concatenate(pred, axis=0)\n",
        "            if top_k != 'all' and pred.shape[0] > top_k: # If we have more than `top_k` results left at this point, otherwise there is nothing to filter,...\n",
        "                top_k_indices = np.argpartition(pred[:,1], kth=pred.shape[0]-top_k, axis=0)[pred.shape[0]-top_k:] # ...get the indices of the `top_k` highest-score maxima...\n",
        "                pred = pred[top_k_indices] # ...and keep only those entries of `pred`...\n",
        "        else:\n",
        "            pred = np.array(pred) # Even if empty, `pred` must become a Numpy array.\n",
        "        y_pred_decoded.append(pred) # ...and now that we're done, append the array of final predictions for this batch item to the output list\n",
        "\n",
        "    return y_pred_decoded\n",
        "\n",
        "def decode_detections_fast(y_pred,\n",
        "                           confidence_thresh=0.5,\n",
        "                           iou_threshold=0.45,\n",
        "                           top_k='all',\n",
        "                           input_coords='centroids',\n",
        "                           normalize_coords=True,\n",
        "                           img_height=None,\n",
        "                           img_width=None,\n",
        "                           border_pixels='half'):\n",
        "    '''\n",
        "    Convert model prediction output back to a format that contains only the positive box predictions\n",
        "    (i.e. the same format that `enconde_y()` takes as input).\n",
        "    Optionally performs confidence thresholding and greedy non-maximum suppression after the decoding stage.\n",
        "    Note that the decoding procedure used here is not the same as the procedure used in the original Caffe implementation.\n",
        "    For each box, the procedure used here assigns the box's highest confidence as its predicted class. Then it removes\n",
        "    all boxes for which the highest confidence is the background class. This results in less work for the subsequent\n",
        "    non-maximum suppression, because the vast majority of the predictions will be filtered out just by the fact that\n",
        "    their highest confidence is for the background class. It is much more efficient than the procedure of the original\n",
        "    implementation, but the results may also differ.\n",
        "    Arguments:\n",
        "        y_pred (array): The prediction output of the SSD model, expected to be a Numpy array\n",
        "            of shape `(batch_size, #boxes, #classes + 4 + 4 + 4)`, where `#boxes` is the total number of\n",
        "            boxes predicted by the model per image and the last axis contains\n",
        "            `[one-hot vector for the classes, 4 predicted coordinate offsets, 4 anchor box coordinates, 4 variances]`.\n",
        "        confidence_thresh (float, optional): A float in [0,1), the minimum classification confidence in any positive\n",
        "            class required for a given box to be considered a positive prediction. A lower value will result\n",
        "            in better recall, while a higher value will result in better precision. Do not use this parameter with the\n",
        "            goal to combat the inevitably many duplicates that an SSD will produce, the subsequent non-maximum suppression\n",
        "            stage will take care of those.\n",
        "        iou_threshold (float, optional): `None` or a float in [0,1]. If `None`, no non-maximum suppression will be\n",
        "            performed. If not `None`, greedy NMS will be performed after the confidence thresholding stage, meaning\n",
        "            all boxes with a Jaccard similarity of greater than `iou_threshold` with a locally maximal box will be removed\n",
        "            from the set of predictions, where 'maximal' refers to the box score.\n",
        "        top_k (int, optional): 'all' or an integer with number of highest scoring predictions to be kept for each batch item\n",
        "            after the non-maximum suppression stage. If 'all', all predictions left after the NMS stage will be kept.\n",
        "        input_coords (str, optional): The box coordinate format that the model outputs. Can be either 'centroids'\n",
        "            for the format `(cx, cy, w, h)` (box center coordinates, width, and height), 'minmax' for the format\n",
        "            `(xmin, xmax, ymin, ymax)`, or 'corners' for the format `(xmin, ymin, xmax, ymax)`.\n",
        "        normalize_coords (bool, optional): Set to `True` if the model outputs relative coordinates (i.e. coordinates in [0,1])\n",
        "            and you wish to transform these relative coordinates back to absolute coordinates. If the model outputs\n",
        "            relative coordinates, but you do not want to convert them back to absolute coordinates, set this to `False`.\n",
        "            Do not set this to `True` if the model already outputs absolute coordinates, as that would result in incorrect\n",
        "            coordinates. Requires `img_height` and `img_width` if set to `True`.\n",
        "        img_height (int, optional): The height of the input images. Only needed if `normalize_coords` is `True`.\n",
        "        img_width (int, optional): The width of the input images. Only needed if `normalize_coords` is `True`.\n",
        "        border_pixels (str, optional): How to treat the border pixels of the bounding boxes.\n",
        "            Can be 'include', 'exclude', or 'half'. If 'include', the border pixels belong\n",
        "            to the boxes. If 'exclude', the border pixels do not belong to the boxes.\n",
        "            If 'half', then one of each of the two horizontal and vertical borders belong\n",
        "            to the boxex, but not the other.\n",
        "    Returns:\n",
        "        A python list of length `batch_size` where each list element represents the predicted boxes\n",
        "        for one image and contains a Numpy array of shape `(boxes, 6)` where each row is a box prediction for\n",
        "        a non-background class for the respective image in the format `[class_id, confidence, xmin, xmax, ymin, ymax]`.\n",
        "    '''\n",
        "    if normalize_coords and ((img_height is None) or (img_width is None)):\n",
        "        raise ValueError(\"If relative box coordinates are supposed to be converted to absolute coordinates, the decoder needs the image size in order to decode the predictions, but `img_height == {}` and `img_width == {}`\".format(img_height, img_width))\n",
        "\n",
        "    # 1: Convert the classes from one-hot encoding to their class ID\n",
        "    y_pred_converted = np.copy(y_pred[:,:,-14:-8]) # Slice out the four offset predictions plus two elements whereto we'll write the class IDs and confidences in the next step\n",
        "    y_pred_converted[:,:,0] = np.argmax(y_pred[:,:,:-12], axis=-1) # The indices of the highest confidence values in the one-hot class vectors are the class ID\n",
        "    y_pred_converted[:,:,1] = np.amax(y_pred[:,:,:-12], axis=-1) # Store the confidence values themselves, too\n",
        "\n",
        "    # 2: Convert the box coordinates from the predicted anchor box offsets to predicted absolute coordinates\n",
        "    if input_coords == 'centroids':\n",
        "        y_pred_converted[:,:,[4,5]] = np.exp(y_pred_converted[:,:,[4,5]] * y_pred[:,:,[-2,-1]]) # exp(ln(w(pred)/w(anchor)) / w_variance * w_variance) == w(pred) / w(anchor), exp(ln(h(pred)/h(anchor)) / h_variance * h_variance) == h(pred) / h(anchor)\n",
        "        y_pred_converted[:,:,[4,5]] *= y_pred[:,:,[-6,-5]] # (w(pred) / w(anchor)) * w(anchor) == w(pred), (h(pred) / h(anchor)) * h(anchor) == h(pred)\n",
        "        y_pred_converted[:,:,[2,3]] *= y_pred[:,:,[-4,-3]] * y_pred[:,:,[-6,-5]] # (delta_cx(pred) / w(anchor) / cx_variance) * cx_variance * w(anchor) == delta_cx(pred), (delta_cy(pred) / h(anchor) / cy_variance) * cy_variance * h(anchor) == delta_cy(pred)\n",
        "        y_pred_converted[:,:,[2,3]] += y_pred[:,:,[-8,-7]] # delta_cx(pred) + cx(anchor) == cx(pred), delta_cy(pred) + cy(anchor) == cy(pred)\n",
        "        y_pred_converted = convert_coordinates(y_pred_converted, start_index=-4, conversion='centroids2corners')\n",
        "    elif input_coords == 'minmax':\n",
        "        y_pred_converted[:,:,2:] *= y_pred[:,:,-4:] # delta(pred) / size(anchor) / variance * variance == delta(pred) / size(anchor) for all four coordinates, where 'size' refers to w or h, respectively\n",
        "        y_pred_converted[:,:,[2,3]] *= np.expand_dims(y_pred[:,:,-7] - y_pred[:,:,-8], axis=-1) # delta_xmin(pred) / w(anchor) * w(anchor) == delta_xmin(pred), delta_xmax(pred) / w(anchor) * w(anchor) == delta_xmax(pred)\n",
        "        y_pred_converted[:,:,[4,5]] *= np.expand_dims(y_pred[:,:,-5] - y_pred[:,:,-6], axis=-1) # delta_ymin(pred) / h(anchor) * h(anchor) == delta_ymin(pred), delta_ymax(pred) / h(anchor) * h(anchor) == delta_ymax(pred)\n",
        "        y_pred_converted[:,:,2:] += y_pred[:,:,-8:-4] # delta(pred) + anchor == pred for all four coordinates\n",
        "        y_pred_converted = convert_coordinates(y_pred_converted, start_index=-4, conversion='minmax2corners')\n",
        "    elif input_coords == 'corners':\n",
        "        y_pred_converted[:,:,2:] *= y_pred[:,:,-4:] # delta(pred) / size(anchor) / variance * variance == delta(pred) / size(anchor) for all four coordinates, where 'size' refers to w or h, respectively\n",
        "        y_pred_converted[:,:,[2,4]] *= np.expand_dims(y_pred[:,:,-6] - y_pred[:,:,-8], axis=-1) # delta_xmin(pred) / w(anchor) * w(anchor) == delta_xmin(pred), delta_xmax(pred) / w(anchor) * w(anchor) == delta_xmax(pred)\n",
        "        y_pred_converted[:,:,[3,5]] *= np.expand_dims(y_pred[:,:,-5] - y_pred[:,:,-7], axis=-1) # delta_ymin(pred) / h(anchor) * h(anchor) == delta_ymin(pred), delta_ymax(pred) / h(anchor) * h(anchor) == delta_ymax(pred)\n",
        "        y_pred_converted[:,:,2:] += y_pred[:,:,-8:-4] # delta(pred) + anchor == pred for all four coordinates\n",
        "    else:\n",
        "        raise ValueError(\"Unexpected value for `coords`. Supported values are 'minmax', 'corners' and 'centroids'.\")\n",
        "\n",
        "    # 3: If the model predicts normalized box coordinates and they are supposed to be converted back to absolute coordinates, do that\n",
        "    if normalize_coords:\n",
        "        y_pred_converted[:,:,[2,4]] *= img_width # Convert xmin, xmax back to absolute coordinates\n",
        "        y_pred_converted[:,:,[3,5]] *= img_height # Convert ymin, ymax back to absolute coordinates\n",
        "\n",
        "    # 4: Decode our huge `(batch, #boxes, 6)` tensor into a list of length `batch` where each list entry is an array containing only the positive predictions\n",
        "    y_pred_decoded = []\n",
        "    for batch_item in y_pred_converted: # For each image in the batch...\n",
        "        boxes = batch_item[np.nonzero(batch_item[:,0])] # ...get all boxes that don't belong to the background class,...\n",
        "        boxes = boxes[boxes[:,1] >= confidence_thresh] # ...then filter out those positive boxes for which the prediction confidence is too low and after that...\n",
        "        if iou_threshold: # ...if an IoU threshold is set...\n",
        "            boxes = _greedy_nms2(boxes, iou_threshold=iou_threshold, coords='corners', border_pixels=border_pixels) # ...perform NMS on the remaining boxes.\n",
        "        if top_k != 'all' and boxes.shape[0] > top_k: # If we have more than `top_k` results left at this point...\n",
        "            top_k_indices = np.argpartition(boxes[:,1], kth=boxes.shape[0]-top_k, axis=0)[boxes.shape[0]-top_k:] # ...get the indices of the `top_k` highest-scoring boxes...\n",
        "            boxes = boxes[top_k_indices] # ...and keep only those boxes...\n",
        "        y_pred_decoded.append(boxes) # ...and now that we're done, append the array of final predictions for this batch item to the output list\n",
        "\n",
        "    return y_pred_decoded\n",
        "\n",
        "################################################################################################\n",
        "# Debugging tools, not relevant for normal use\n",
        "################################################################################################\n",
        "\n",
        "# The functions below are for debugging, so you won't normally need them. That is,\n",
        "# unless you need to debug your model, of course.\n",
        "\n",
        "def decode_detections_debug(y_pred,\n",
        "                            confidence_thresh=0.01,\n",
        "                            iou_threshold=0.45,\n",
        "                            top_k=200,\n",
        "                            input_coords='centroids',\n",
        "                            normalize_coords=True,\n",
        "                            img_height=None,\n",
        "                            img_width=None,\n",
        "                            variance_encoded_in_target=False,\n",
        "                            border_pixels='half'):\n",
        "    '''\n",
        "    This decoder performs the same processing as `decode_detections()`, but the output format for each left-over\n",
        "    predicted box is `[box_id, class_id, confidence, xmin, ymin, xmax, ymax]`.\n",
        "    That is, in addition to the usual data, each predicted box has the internal index of that box within\n",
        "    the model (`box_id`) prepended to it. This allows you to know exactly which part of the model made a given\n",
        "    box prediction; in particular, it allows you to know which predictor layer made a given prediction.\n",
        "    This can be useful for debugging.\n",
        "    Arguments:\n",
        "        y_pred (array): The prediction output of the SSD model, expected to be a Numpy array\n",
        "            of shape `(batch_size, #boxes, #classes + 4 + 4 + 4)`, where `#boxes` is the total number of\n",
        "            boxes predicted by the model per image and the last axis contains\n",
        "            `[one-hot vector for the classes, 4 predicted coordinate offsets, 4 anchor box coordinates, 4 variances]`.\n",
        "        confidence_thresh (float, optional): A float in [0,1), the minimum classification confidence in a specific\n",
        "            positive class in order to be considered for the non-maximum suppression stage for the respective class.\n",
        "            A lower value will result in a larger part of the selection process being done by the non-maximum suppression\n",
        "            stage, while a larger value will result in a larger part of the selection process happening in the confidence\n",
        "            thresholding stage.\n",
        "        iou_threshold (float, optional): A float in [0,1]. All boxes with a Jaccard similarity of greater than `iou_threshold`\n",
        "            with a locally maximal box will be removed from the set of predictions for a given class, where 'maximal' refers\n",
        "            to the box score.\n",
        "        top_k (int, optional): The number of highest scoring predictions to be kept for each batch item after the\n",
        "            non-maximum suppression stage.\n",
        "        input_coords (str, optional): The box coordinate format that the model outputs. Can be either 'centroids'\n",
        "            for the format `(cx, cy, w, h)` (box center coordinates, width, and height), 'minmax' for the format\n",
        "            `(xmin, xmax, ymin, ymax)`, or 'corners' for the format `(xmin, ymin, xmax, ymax)`.\n",
        "        normalize_coords (bool, optional): Set to `True` if the model outputs relative coordinates (i.e. coordinates in [0,1])\n",
        "            and you wish to transform these relative coordinates back to absolute coordinates. If the model outputs\n",
        "            relative coordinates, but you do not want to convert them back to absolute coordinates, set this to `False`.\n",
        "            Do not set this to `True` if the model already outputs absolute coordinates, as that would result in incorrect\n",
        "            coordinates. Requires `img_height` and `img_width` if set to `True`.\n",
        "        img_height (int, optional): The height of the input images. Only needed if `normalize_coords` is `True`.\n",
        "        img_width (int, optional): The width of the input images. Only needed if `normalize_coords` is `True`.\n",
        "        border_pixels (str, optional): How to treat the border pixels of the bounding boxes.\n",
        "            Can be 'include', 'exclude', or 'half'. If 'include', the border pixels belong\n",
        "            to the boxes. If 'exclude', the border pixels do not belong to the boxes.\n",
        "            If 'half', then one of each of the two horizontal and vertical borders belong\n",
        "            to the boxex, but not the other.\n",
        "    Returns:\n",
        "        A python list of length `batch_size` where each list element represents the predicted boxes\n",
        "        for one image and contains a Numpy array of shape `(boxes, 7)` where each row is a box prediction for\n",
        "        a non-background class for the respective image in the format `[box_id, class_id, confidence, xmin, ymin, xmax, ymax]`.\n",
        "    '''\n",
        "    if normalize_coords and ((img_height is None) or (img_width is None)):\n",
        "        raise ValueError(\"If relative box coordinates are supposed to be converted to absolute coordinates, the decoder needs the image size in order to decode the predictions, but `img_height == {}` and `img_width == {}`\".format(img_height, img_width))\n",
        "\n",
        "    # 1: Convert the box coordinates from the predicted anchor box offsets to predicted absolute coordinates\n",
        "\n",
        "    y_pred_decoded_raw = np.copy(y_pred[:,:,:-8]) # Slice out the classes and the four offsets, throw away the anchor coordinates and variances, resulting in a tensor of shape `[batch, n_boxes, n_classes + 4 coordinates]`\n",
        "\n",
        "    if input_coords == 'centroids':\n",
        "        if variance_encoded_in_target:\n",
        "            # Decode the predicted box center x and y coordinates.\n",
        "            y_pred_decoded_raw[:,:,[-4,-3]] = y_pred_decoded_raw[:,:,[-4,-3]] * y_pred[:,:,[-6,-5]] + y_pred[:,:,[-8,-7]]\n",
        "            # Decode the predicted box width and heigt.\n",
        "            y_pred_decoded_raw[:,:,[-2,-1]] = np.exp(y_pred_decoded_raw[:,:,[-2,-1]]) * y_pred[:,:,[-6,-5]]\n",
        "        else:\n",
        "            # Decode the predicted box center x and y coordinates.\n",
        "            y_pred_decoded_raw[:,:,[-4,-3]] = y_pred_decoded_raw[:,:,[-4,-3]] * y_pred[:,:,[-6,-5]] * y_pred[:,:,[-4,-3]] + y_pred[:,:,[-8,-7]]\n",
        "            # Decode the predicted box width and heigt.\n",
        "            y_pred_decoded_raw[:,:,[-2,-1]] = np.exp(y_pred_decoded_raw[:,:,[-2,-1]] * y_pred[:,:,[-2,-1]]) * y_pred[:,:,[-6,-5]]\n",
        "        y_pred_decoded_raw = convert_coordinates(y_pred_decoded_raw, start_index=-4, conversion='centroids2corners')\n",
        "    elif input_coords == 'minmax':\n",
        "        y_pred_decoded_raw[:,:,-4:] *= y_pred[:,:,-4:] # delta(pred) / size(anchor) / variance * variance == delta(pred) / size(anchor) for all four coordinates, where 'size' refers to w or h, respectively\n",
        "        y_pred_decoded_raw[:,:,[-4,-3]] *= np.expand_dims(y_pred[:,:,-7] - y_pred[:,:,-8], axis=-1) # delta_xmin(pred) / w(anchor) * w(anchor) == delta_xmin(pred), delta_xmax(pred) / w(anchor) * w(anchor) == delta_xmax(pred)\n",
        "        y_pred_decoded_raw[:,:,[-2,-1]] *= np.expand_dims(y_pred[:,:,-5] - y_pred[:,:,-6], axis=-1) # delta_ymin(pred) / h(anchor) * h(anchor) == delta_ymin(pred), delta_ymax(pred) / h(anchor) * h(anchor) == delta_ymax(pred)\n",
        "        y_pred_decoded_raw[:,:,-4:] += y_pred[:,:,-8:-4] # delta(pred) + anchor == pred for all four coordinates\n",
        "        y_pred_decoded_raw = convert_coordinates(y_pred_decoded_raw, start_index=-4, conversion='minmax2corners')\n",
        "    elif input_coords == 'corners':\n",
        "        y_pred_decoded_raw[:,:,-4:] *= y_pred[:,:,-4:] # delta(pred) / size(anchor) / variance * variance == delta(pred) / size(anchor) for all four coordinates, where 'size' refers to w or h, respectively\n",
        "        y_pred_decoded_raw[:,:,[-4,-2]] *= np.expand_dims(y_pred[:,:,-6] - y_pred[:,:,-8], axis=-1) # delta_xmin(pred) / w(anchor) * w(anchor) == delta_xmin(pred), delta_xmax(pred) / w(anchor) * w(anchor) == delta_xmax(pred)\n",
        "        y_pred_decoded_raw[:,:,[-3,-1]] *= np.expand_dims(y_pred[:,:,-5] - y_pred[:,:,-7], axis=-1) # delta_ymin(pred) / h(anchor) * h(anchor) == delta_ymin(pred), delta_ymax(pred) / h(anchor) * h(anchor) == delta_ymax(pred)\n",
        "        y_pred_decoded_raw[:,:,-4:] += y_pred[:,:,-8:-4] # delta(pred) + anchor == pred for all four coordinates\n",
        "    else:\n",
        "        raise ValueError(\"Unexpected value for `input_coords`. Supported input coordinate formats are 'minmax', 'corners' and 'centroids'.\")\n",
        "\n",
        "    # 2: If the model predicts normalized box coordinates and they are supposed to be converted back to absolute coordinates, do that\n",
        "\n",
        "    if normalize_coords:\n",
        "        y_pred_decoded_raw[:,:,[-4,-2]] *= img_width # Convert xmin, xmax back to absolute coordinates\n",
        "        y_pred_decoded_raw[:,:,[-3,-1]] *= img_height # Convert ymin, ymax back to absolute coordinates\n",
        "\n",
        "    # 3: For each batch item, prepend each box's internal index to its coordinates.\n",
        "\n",
        "    y_pred_decoded_raw2 = np.zeros((y_pred_decoded_raw.shape[0], y_pred_decoded_raw.shape[1], y_pred_decoded_raw.shape[2] + 1)) # Expand the last axis by one.\n",
        "    y_pred_decoded_raw2[:,:,1:] = y_pred_decoded_raw\n",
        "    y_pred_decoded_raw2[:,:,0] = np.arange(y_pred_decoded_raw.shape[1]) # Put the box indices as the first element for each box via broadcasting.\n",
        "    y_pred_decoded_raw = y_pred_decoded_raw2\n",
        "\n",
        "    # 4: Apply confidence thresholding and non-maximum suppression per class\n",
        "\n",
        "    n_classes = y_pred_decoded_raw.shape[-1] - 5 # The number of classes is the length of the last axis minus the four box coordinates and minus the index\n",
        "\n",
        "    y_pred_decoded = [] # Store the final predictions in this list\n",
        "    for batch_item in y_pred_decoded_raw: # `batch_item` has shape `[n_boxes, n_classes + 4 coords]`\n",
        "        pred = [] # Store the final predictions for this batch item here\n",
        "        for class_id in range(1, n_classes): # For each class except the background class (which has class ID 0)...\n",
        "            single_class = batch_item[:,[0, class_id + 1, -4, -3, -2, -1]] # ...keep only the confidences for that class, making this an array of shape `[n_boxes, 6]` and...\n",
        "            threshold_met = single_class[single_class[:,1] > confidence_thresh] # ...keep only those boxes with a confidence above the set threshold.\n",
        "            if threshold_met.shape[0] > 0: # If any boxes made the threshold...\n",
        "                maxima = _greedy_nms_debug(threshold_met, iou_threshold=iou_threshold, coords='corners', border_pixels=border_pixels) # ...perform NMS on them.\n",
        "                maxima_output = np.zeros((maxima.shape[0], maxima.shape[1] + 1)) # Expand the last dimension by one element to have room for the class ID. This is now an arrray of shape `[n_boxes, 6]`\n",
        "                maxima_output[:,0] = maxima[:,0] # Write the box index to the first column...\n",
        "                maxima_output[:,1] = class_id # ...and write the class ID to the second column...\n",
        "                maxima_output[:,2:] = maxima[:,1:] # ...and write the rest of the maxima data to the other columns...\n",
        "                pred.append(maxima_output) # ...and append the maxima for this class to the list of maxima for this batch item.\n",
        "        # Once we're through with all classes, keep only the `top_k` maxima with the highest scores\n",
        "        pred = np.concatenate(pred, axis=0)\n",
        "        if pred.shape[0] > top_k: # If we have more than `top_k` results left at this point, otherwise there is nothing to filter,...\n",
        "            top_k_indices = np.argpartition(pred[:,2], kth=pred.shape[0]-top_k, axis=0)[pred.shape[0]-top_k:] # ...get the indices of the `top_k` highest-score maxima...\n",
        "            pred = pred[top_k_indices] # ...and keep only those entries of `pred`...\n",
        "        y_pred_decoded.append(pred) # ...and now that we're done, append the array of final predictions for this batch item to the output list\n",
        "\n",
        "    return y_pred_decoded\n",
        "\n",
        "def _greedy_nms_debug(predictions, iou_threshold=0.45, coords='corners', border_pixels='half'):\n",
        "    '''\n",
        "    The same greedy non-maximum suppression algorithm as above, but slightly modified for use as an internal\n",
        "    function for per-class NMS in `decode_detections_debug()`. The difference is that it keeps the indices of all\n",
        "    left-over boxes for each batch item, which allows you to know which predictor layer predicted a given output\n",
        "    box and is thus useful for debugging.\n",
        "    '''\n",
        "    boxes_left = np.copy(predictions)\n",
        "    maxima = [] # This is where we store the boxes that make it through the non-maximum suppression\n",
        "    while boxes_left.shape[0] > 0: # While there are still boxes left to compare...\n",
        "        maximum_index = np.argmax(boxes_left[:,1]) # ...get the index of the next box with the highest confidence...\n",
        "        maximum_box = np.copy(boxes_left[maximum_index]) # ...copy that box and...\n",
        "        maxima.append(maximum_box) # ...append it to `maxima` because we'll definitely keep it\n",
        "        boxes_left = np.delete(boxes_left, maximum_index, axis=0) # Now remove the maximum box from `boxes_left`\n",
        "        if boxes_left.shape[0] == 0: break # If there are no boxes left after this step, break. Otherwise...\n",
        "        similarities = iou(boxes_left[:,2:], maximum_box[2:], coords=coords, mode='element-wise', border_pixels=border_pixels) # ...compare (IoU) the other left over boxes to the maximum box...\n",
        "        boxes_left = boxes_left[similarities <= iou_threshold] # ...so that we can remove the ones that overlap too much with the maximum box\n",
        "    return np.array(maxima)\n",
        "\n",
        "def get_num_boxes_per_pred_layer(predictor_sizes, aspect_ratios, two_boxes_for_ar1):\n",
        "    '''\n",
        "    Returns a list of the number of boxes that each predictor layer predicts.\n",
        "    `aspect_ratios` must be a nested list, containing a list of aspect ratios\n",
        "    for each predictor layer.\n",
        "    '''\n",
        "    num_boxes_per_pred_layer = []\n",
        "    for i in range(len(predictor_sizes)):\n",
        "        if two_boxes_for_ar1:\n",
        "            num_boxes_per_pred_layer.append(predictor_sizes[i][0] * predictor_sizes[i][1] * (len(aspect_ratios[i]) + 1))\n",
        "        else:\n",
        "            num_boxes_per_pred_layer.append(predictor_sizes[i][0] * predictor_sizes[i][1] * len(aspect_ratios[i]))\n",
        "    return num_boxes_per_pred_layer\n",
        "\n",
        "def get_pred_layers(y_pred_decoded, num_boxes_per_pred_layer):\n",
        "    '''\n",
        "    For a given prediction tensor decoded with `decode_detections_debug()`, returns a list\n",
        "    with the indices of the predictor layers that made each predictions.\n",
        "    That is, this function lets you know which predictor layer is responsible\n",
        "    for a given prediction.\n",
        "    Arguments:\n",
        "        y_pred_decoded (array): The decoded model output tensor. Must have been\n",
        "            decoded with `decode_detections_debug()` so that it contains the internal box index\n",
        "            for each predicted box.\n",
        "        num_boxes_per_pred_layer (list): A list that contains the total number\n",
        "            of boxes that each predictor layer predicts.\n",
        "    '''\n",
        "    pred_layers_all = []\n",
        "    cum_boxes_per_pred_layer = np.cumsum(num_boxes_per_pred_layer)\n",
        "    for batch_item in y_pred_decoded:\n",
        "        pred_layers = []\n",
        "        for prediction in batch_item:\n",
        "            if (prediction[0] < 0) or (prediction[0] >= cum_boxes_per_pred_layer[-1]):\n",
        "                raise ValueError(\"Box index is out of bounds of the possible indices as given by the values in `num_boxes_per_pred_layer`.\")\n",
        "            for i in range(len(cum_boxes_per_pred_layer)):\n",
        "                if prediction[0] < cum_boxes_per_pred_layer[i]:\n",
        "                    pred_layers.append(i)\n",
        "                    break\n",
        "        pred_layers_all.append(pred_layers)\n",
        "    return pred_layers_all"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-106Li_Ee0e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "A data generator for 2D object detection.\n",
        "Copyright (C) 2018 Pierluigi Ferrari\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "   http://www.apache.org/licenses/LICENSE-2.0\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "'''\n",
        "\n",
        "from __future__ import division\n",
        "import numpy as np\n",
        "import inspect\n",
        "from collections import defaultdict\n",
        "import warnings\n",
        "import sklearn.utils\n",
        "from copy import deepcopy\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import csv\n",
        "import os\n",
        "import sys\n",
        "from tqdm import tqdm, trange\n",
        "try:\n",
        "    import h5py\n",
        "except ImportError:\n",
        "    warnings.warn(\"'h5py' module is missing. The fast HDF5 dataset option will be unavailable.\")\n",
        "try:\n",
        "    import json\n",
        "except ImportError:\n",
        "    warnings.warn(\"'json' module is missing. The JSON-parser will be unavailable.\")\n",
        "try:\n",
        "    from bs4 import BeautifulSoup\n",
        "except ImportError:\n",
        "    warnings.warn(\"'BeautifulSoup' module is missing. The XML-parser will be unavailable.\")\n",
        "try:\n",
        "    import pickle\n",
        "except ImportError:\n",
        "    warnings.warn(\"'pickle' module is missing. You won't be able to save parsed file lists and annotations as pickled files.\")\n",
        "\n",
        "#from ssd_encoder_decoder.ssd_input_encoder import SSDInputEncoder\n",
        "#from data_generator.object_detection_2d_image_boxes_validation_utils import BoxFilter\n",
        "\n",
        "class DegenerateBatchError(Exception):\n",
        "    '''\n",
        "    An exception class to be raised if a generated batch ends up being degenerate,\n",
        "    e.g. if a generated batch is empty.\n",
        "    '''\n",
        "    pass\n",
        "\n",
        "class DatasetError(Exception):\n",
        "    '''\n",
        "    An exception class to be raised if a anything is wrong with the dataset,\n",
        "    in particular if you try to generate batches when no dataset was loaded.\n",
        "    '''\n",
        "    pass\n",
        "\n",
        "class DataGenerator:\n",
        "    '''\n",
        "    A generator to generate batches of samples and corresponding labels indefinitely.\n",
        "    Can shuffle the dataset consistently after each complete pass.\n",
        "    Currently provides three methods to parse annotation data: A general-purpose CSV parser,\n",
        "    an XML parser for the Pascal VOC datasets, and a JSON parser for the MS COCO datasets.\n",
        "    If the annotations of your dataset are in a format that is not supported by these parsers,\n",
        "    you could just add another parser method and still use this generator.\n",
        "    Can perform image transformations for data conversion and data augmentation,\n",
        "    for details please refer to the documentation of the `generate()` method.\n",
        "    '''\n",
        "\n",
        "    def __init__(self,\n",
        "                 load_images_into_memory=False,\n",
        "                 hdf5_dataset_path=None,\n",
        "                 filenames=None,\n",
        "                 filenames_type='text',\n",
        "                 images_dir=None,\n",
        "                 labels=None,\n",
        "                 image_ids=None,\n",
        "                 eval_neutral=None,\n",
        "                 labels_output_format=('class_id', 'xmin', 'ymin', 'xmax', 'ymax'),\n",
        "                 verbose=True):\n",
        "        '''\n",
        "        Initializes the data generator. You can either load a dataset directly here in the constructor,\n",
        "        e.g. an HDF5 dataset, or you can use one of the parser methods to read in a dataset.\n",
        "        Arguments:\n",
        "            load_images_into_memory (bool, optional): If `True`, the entire dataset will be loaded into memory.\n",
        "                This enables noticeably faster data generation than loading batches of images into memory ad hoc.\n",
        "                Be sure that you have enough memory before you activate this option.\n",
        "            hdf5_dataset_path (str, optional): The full file path of an HDF5 file that contains a dataset in the\n",
        "                format that the `create_hdf5_dataset()` method produces. If you load such an HDF5 dataset, you\n",
        "                don't need to use any of the parser methods anymore, the HDF5 dataset already contains all relevant\n",
        "                data.\n",
        "            filenames (string or list, optional): `None` or either a Python list/tuple or a string representing\n",
        "                a filepath. If a list/tuple is passed, it must contain the file names (full paths) of the\n",
        "                images to be used. Note that the list/tuple must contain the paths to the images,\n",
        "                not the images themselves. If a filepath string is passed, it must point either to\n",
        "                (1) a pickled file containing a list/tuple as described above. In this case the `filenames_type`\n",
        "                argument must be set to `pickle`.\n",
        "                Or\n",
        "                (2) a text file. Each line of the text file contains the file name (basename of the file only,\n",
        "                not the full directory path) to one image and nothing else. In this case the `filenames_type`\n",
        "                argument must be set to `text` and you must pass the path to the directory that contains the\n",
        "                images in `images_dir`.\n",
        "            filenames_type (string, optional): In case a string is passed for `filenames`, this indicates what\n",
        "                type of file `filenames` is. It can be either 'pickle' for a pickled file or 'text' for a\n",
        "                plain text file.\n",
        "            images_dir (string, optional): In case a text file is passed for `filenames`, the full paths to\n",
        "                the images will be composed from `images_dir` and the names in the text file, i.e. this\n",
        "                should be the directory that contains the images to which the text file refers.\n",
        "                If `filenames_type` is not 'text', then this argument is irrelevant.\n",
        "            labels (string or list, optional): `None` or either a Python list/tuple or a string representing\n",
        "                the path to a pickled file containing a list/tuple. The list/tuple must contain Numpy arrays\n",
        "                that represent the labels of the dataset.\n",
        "            image_ids (string or list, optional): `None` or either a Python list/tuple or a string representing\n",
        "                the path to a pickled file containing a list/tuple. The list/tuple must contain the image\n",
        "                IDs of the images in the dataset.\n",
        "            eval_neutral (string or list, optional): `None` or either a Python list/tuple or a string representing\n",
        "                the path to a pickled file containing a list/tuple. The list/tuple must contain for each image\n",
        "                a list that indicates for each ground truth object in the image whether that object is supposed\n",
        "                to be treated as neutral during an evaluation.\n",
        "            labels_output_format (list, optional): A list of five strings representing the desired order of the five\n",
        "                items class ID, xmin, ymin, xmax, ymax in the generated ground truth data (if any). The expected\n",
        "                strings are 'xmin', 'ymin', 'xmax', 'ymax', 'class_id'.\n",
        "            verbose (bool, optional): If `True`, prints out the progress for some constructor operations that may\n",
        "                take a bit longer.\n",
        "        '''\n",
        "        self.labels_output_format = labels_output_format\n",
        "        self.labels_format={'class_id': labels_output_format.index('class_id'),\n",
        "                            'xmin': labels_output_format.index('xmin'),\n",
        "                            'ymin': labels_output_format.index('ymin'),\n",
        "                            'xmax': labels_output_format.index('xmax'),\n",
        "                            'ymax': labels_output_format.index('ymax')} # This dictionary is for internal use.\n",
        "\n",
        "        self.dataset_size = 0 # As long as we haven't loaded anything yet, the dataset size is zero.\n",
        "        self.load_images_into_memory = load_images_into_memory\n",
        "        self.images = None # The only way that this list will not stay `None` is if `load_images_into_memory == True`.\n",
        "\n",
        "        # `self.filenames` is a list containing all file names of the image samples (full paths).\n",
        "        # Note that it does not contain the actual image files themselves. This list is one of the outputs of the parser methods.\n",
        "        # In case you are loading an HDF5 dataset, this list will be `None`.\n",
        "        if not filenames is None:\n",
        "            if isinstance(filenames, (list, tuple)):\n",
        "                self.filenames = filenames\n",
        "            elif isinstance(filenames, str):\n",
        "                with open(filenames, 'rb') as f:\n",
        "                    if filenames_type == 'pickle':\n",
        "                        self.filenames = pickle.load(f)\n",
        "                    elif filenames_type == 'text':\n",
        "                        #self.filenames = [os.path.join(images_dir, line.strip()) for line in f]\n",
        "                        self.filenames = [line for line in f]\n",
        "                    else:\n",
        "                        raise ValueError(\"`filenames_type` can be either 'text' or 'pickle'.\")\n",
        "            else:\n",
        "                raise ValueError(\"`filenames` must be either a Python list/tuple or a string representing a filepath (to a pickled or text file). The value you passed is neither of the two.\")\n",
        "            self.dataset_size = len(self.filenames)\n",
        "            self.dataset_indices = np.arange(self.dataset_size, dtype=np.int32)\n",
        "            if load_images_into_memory:\n",
        "                self.images = []\n",
        "                if verbose: it = tqdm(self.filenames, desc='Loading images into memory', file=sys.stdout)\n",
        "                else: it = self.filenames\n",
        "                for filename in it:\n",
        "                    with Image.open(filename) as image:\n",
        "                        self.images.append(np.array(image, dtype=np.uint8))\n",
        "        else:\n",
        "            self.filenames = None\n",
        "\n",
        "        # In case ground truth is available, `self.labels` is a list containing for each image a list (or NumPy array)\n",
        "        # of ground truth bounding boxes for that image.\n",
        "        if not labels is None:\n",
        "            if isinstance(labels, str):\n",
        "                with open(labels, 'rb') as f:\n",
        "                    self.labels = pickle.load(f)\n",
        "            elif isinstance(labels, (list, tuple)):\n",
        "                self.labels = labels\n",
        "            else:\n",
        "                raise ValueError(\"`labels` must be either a Python list/tuple or a string representing the path to a pickled file containing a list/tuple. The value you passed is neither of the two.\")\n",
        "        else:\n",
        "            self.labels = None\n",
        "\n",
        "        if not image_ids is None:\n",
        "            if isinstance(image_ids, str):\n",
        "                with open(image_ids, 'rb') as f:\n",
        "                    self.image_ids = pickle.load(f)\n",
        "            elif isinstance(image_ids, (list, tuple)):\n",
        "                self.image_ids = image_ids\n",
        "            else:\n",
        "                raise ValueError(\"`image_ids` must be either a Python list/tuple or a string representing the path to a pickled file containing a list/tuple. The value you passed is neither of the two.\")\n",
        "        else:\n",
        "            self.image_ids = None\n",
        "\n",
        "        if not eval_neutral is None:\n",
        "            if isinstance(eval_neutral, str):\n",
        "                with open(eval_neutral, 'rb') as f:\n",
        "                    self.eval_neutral = pickle.load(f)\n",
        "            elif isinstance(eval_neutral, (list, tuple)):\n",
        "                self.eval_neutral = eval_neutral\n",
        "            else:\n",
        "                raise ValueError(\"`image_ids` must be either a Python list/tuple or a string representing the path to a pickled file containing a list/tuple. The value you passed is neither of the two.\")\n",
        "        else:\n",
        "            self.eval_neutral = None\n",
        "\n",
        "        if not hdf5_dataset_path is None:\n",
        "            self.hdf5_dataset_path = hdf5_dataset_path\n",
        "            self.load_hdf5_dataset(verbose=verbose)\n",
        "        else:\n",
        "            self.hdf5_dataset = None\n",
        "\n",
        "    \"\"\"def load_hdf5_dataset(self, verbose=True):\n",
        "        '''\n",
        "        Loads an HDF5 dataset that is in the format that the `create_hdf5_dataset()` method\n",
        "        produces.\n",
        "        Arguments:\n",
        "            verbose (bool, optional): If `True`, prints out the progress while loading\n",
        "                the dataset.\n",
        "        Returns:\n",
        "            None.\n",
        "        '''\n",
        "\n",
        "        self.hdf5_dataset = h5py.File(self.hdf5_dataset_path, 'r')\n",
        "        self.dataset_size = len(self.hdf5_dataset['images'])\n",
        "        self.dataset_indices = np.arange(self.dataset_size, dtype=np.int32) # Instead of shuffling the HDF5 dataset or images in memory, we will shuffle this index list.\n",
        "\n",
        "        if self.load_images_into_memory:\n",
        "            self.images = []\n",
        "            if verbose: tr = trange(self.dataset_size, desc='Loading images into memory', file=sys.stdout)\n",
        "            else: tr = range(self.dataset_size)\n",
        "            for i in tr:\n",
        "                self.images.append(self.hdf5_dataset['images'][i].reshape(self.hdf5_dataset['image_shapes'][i]))\n",
        "\n",
        "        if self.hdf5_dataset.attrs['has_labels']:\n",
        "            self.labels = []\n",
        "            labels = self.hdf5_dataset['labels']\n",
        "            label_shapes = self.hdf5_dataset['label_shapes']\n",
        "            if verbose: tr = trange(self.dataset_size, desc='Loading labels', file=sys.stdout)\n",
        "            else: tr = range(self.dataset_size)\n",
        "            for i in tr:\n",
        "                self.labels.append(labels[i].reshape(label_shapes[i]))\n",
        "\n",
        "        if self.hdf5_dataset.attrs['has_image_ids']:\n",
        "            self.image_ids = []\n",
        "            image_ids = self.hdf5_dataset['image_ids']\n",
        "            if verbose: tr = trange(self.dataset_size, desc='Loading image IDs', file=sys.stdout)\n",
        "            else: tr = range(self.dataset_size)\n",
        "            for i in tr:\n",
        "                self.image_ids.append(image_ids[i])\n",
        "\n",
        "        if self.hdf5_dataset.attrs['has_eval_neutral']:\n",
        "            self.eval_neutral = []\n",
        "            eval_neutral = self.hdf5_dataset['eval_neutral']\n",
        "            if verbose: tr = trange(self.dataset_size, desc='Loading evaluation-neutrality annotations', file=sys.stdout)\n",
        "            else: tr = range(self.dataset_size)\n",
        "            for i in tr:\n",
        "                self.eval_neutral.append(eval_neutral[i])\"\"\"\n",
        "\n",
        "    def parse_csv(self,\n",
        "                  images_dir,\n",
        "                  labels_filename,\n",
        "                  input_format,\n",
        "                  include_classes='all',\n",
        "                  random_sample=False,\n",
        "                  ret=False,\n",
        "                  verbose=True):\n",
        "        '''\n",
        "        Arguments:\n",
        "            images_dir (str): The path to the directory that contains the images.\n",
        "            labels_filename (str): The filepath to a CSV file that contains one ground truth bounding box per line\n",
        "                and each line contains the following six items: image file name, class ID, xmin, xmax, ymin, ymax.\n",
        "                The six items do not have to be in a specific order, but they must be the first six columns of\n",
        "                each line. The order of these items in the CSV file must be specified in `input_format`.\n",
        "                The class ID is an integer greater than zero. Class ID 0 is reserved for the background class.\n",
        "                `xmin` and `xmax` are the left-most and right-most absolu42te horizontal coordinates of the box,\n",
        "                `ymin` and `ymax` are the top-most and bottom-most absolute vertical coordinates of the box.\n",
        "                The image name is expected to be just the name of the image file without the directory path\n",
        "                at which the image is located.\n",
        "            input_format (list): A list of six strings representing the order of the six items\n",
        "                image file name, class ID, xmin, xmax, ymin, ymax in the input CSV file. The expected strings\n",
        "                are 'image_name', 'xmin', 'xmax', 'ymin', 'ymax', 'class_id'.\n",
        "            include_classes (list, optional): Either 'all' or a list of integers containing the class IDs that\n",
        "                are to be included in the dataset. If 'all', all ground truth boxes will be included in the dataset.\n",
        "            random_sample (float, optional): Either `False` or a float in `[0,1]`. If this is `False`, the\n",
        "                full dataset will be used by the generator. If this is a float in `[0,1]`, a randomly sampled\n",
        "                fraction of the dataset will be used, where `random_sample` is the fraction of the dataset\n",
        "                to be used. For example, if `random_sample = 0.2`, 20 precent of the dataset will be randomly selected,\n",
        "                the rest will be ommitted. The fraction refers to the number of images, not to the number\n",
        "                of boxes, i.e. each image that will be added to the dataset will always be added with all\n",
        "                of its boxes.\n",
        "            ret (bool, optional): Whether or not to return the outputs of the parser.\n",
        "            verbose (bool, optional): If `True`, prints out the progress for operations that may take a bit longer.\n",
        "        Returns:\n",
        "            None by default, optionally lists for whichever are available of images, image filenames, labels, and image IDs.\n",
        "        '''\n",
        "\n",
        "        # Set class members.\n",
        "        self.images_dir = images_dir\n",
        "        self.labels_filename = labels_filename\n",
        "        self.input_format = input_format\n",
        "        self.include_classes = include_classes\n",
        "\n",
        "        # Before we begin, make sure that we have a labels_filename and an input_format\n",
        "        if self.labels_filename is None or self.input_format is None:\n",
        "            raise ValueError(\"`labels_filename` and/or `input_format` have not been set yet. You need to pass them as arguments.\")\n",
        "\n",
        "        # Erase data that might have been parsed before\n",
        "        self.filenames = []\n",
        "        self.image_ids = []\n",
        "        self.labels = []\n",
        "\n",
        "        # First, just read in the CSV file lines and sort them.\n",
        "        print(\"INSIDE PARSE_CSV..............\")\n",
        "        data = []\n",
        "\n",
        "        with open(self.labels_filename, newline='') as csvfile:\n",
        "            csvread = csv.reader(csvfile, delimiter=',')\n",
        "            next(csvread) # Skip the header row.\n",
        "            for row in csvread: # For every line (i.e for every bounding box) in the CSV file...\n",
        "                print(\"Row :\",row)\n",
        "                if self.include_classes == 'all' or int(row[self.input_format.index('class_id')].strip()) in self.include_classes: # If the class_id is among the classes that are to be included in the dataset...\n",
        "                    box = [] # Store the box class and coordinates here\n",
        "                    box.append(row[self.input_format.index('image_name')].strip()) # Select the image name column in the input format and append its content to `box`\n",
        "                    for element in self.labels_output_format: # For each element in the output format (where the elements are the class ID and the four box coordinates)...\n",
        "                        box.append(int(row[self.input_format.index(element)].strip())) # ...select the respective column in the input format and append it to `box`.\n",
        "                    data.append(box)\n",
        "\n",
        "        data = sorted(data) # The data needs to be sorted, otherwise the next step won't give the correct result\n",
        "        print(\"Data after sorting...\",data[0:10])\n",
        "        # Now that we've made sure that the data is sorted by file names,\n",
        "        # we can compile the actual samples and labels lists\n",
        "\n",
        "        current_file = data[0][0] # The current image for which we're collecting the ground truth boxes\n",
        "        current_image_id = data[0][0].split('.')[0] # The image ID will be the portion of the image name before the first dot.\n",
        "        print(\"curr image id...\",current_image_id)\n",
        "        current_labels = [] # The list where we collect all ground truth boxes for a given image\n",
        "        add_to_dataset = False\n",
        "        for i, box in enumerate(data):\n",
        "\n",
        "            if box[0] == current_file: # If this box (i.e. this line of the CSV file) belongs to the current image file\n",
        "                current_labels.append(box[1:])\n",
        "                print(\"current_labels...\",box[1:5]) #box[1:])#   000000000000000000000000000000000000000\n",
        "                if i == len(data)-1: # If this is the last line of the CSV file\n",
        "                    if random_sample: # In case we're not using the full dataset, but a random sample of it.\n",
        "                        p = np.random.uniform(0,1)\n",
        "                        if p >= (1-random_sample):\n",
        "                            self.labels.append(np.stack(current_labels, axis=0))\n",
        "                           ## self.filenames.append(os.path.join(self.images_dir, current_file))\n",
        "                            self.filenames.append(current_file)\n",
        "                            self.image_ids.append(current_image_id)\n",
        "                    else:\n",
        "                        self.labels.append(np.stack(current_labels, axis=0))\n",
        "                        #self.filenames.append(os.path.join(self.images_dir, current_file)) # changed this since the absolute url it has\n",
        "                        self.filenames.append(current_file)\n",
        "                        self.image_ids.append(current_image_id)\n",
        "            else: # If this box belongs to a new image file\n",
        "                if random_sample: # In case we're not using the full dataset, but a random sample of it.\n",
        "                    p = np.random.uniform(0,1)\n",
        "                    if p >= (1-random_sample):\n",
        "                        self.labels.append(np.stack(current_labels, axis=0))\n",
        "                        self.filenames.append(os.path.join(self.images_dir, current_file))\n",
        "                        self.image_ids.append(current_image_id)\n",
        "                else:\n",
        "                    self.labels.append(np.stack(current_labels, axis=0))\n",
        "                    ##self.filenames.append(os.path.join(self.images_dir, current_file))\n",
        "                    self.filenames.append(current_file)\n",
        "                    self.image_ids.append(current_image_id)\n",
        "                current_labels = [] # Reset the labels list because this is a new file.\n",
        "                current_file = box[0]\n",
        "                current_image_id = box[0].split('.')[0]\n",
        "                current_labels.append(box[1:])\n",
        "                if i == len(data)-1: # If this is the last line of the CSV file\n",
        "                    if random_sample: # In case we're not using the full dataset, but a random sample of it.\n",
        "                        p = np.random.uniform(0,1)\n",
        "                        if p >= (1-random_sample):\n",
        "                            self.labels.append(np.stack(current_labels, axis=0))\n",
        "                            #self.filenames.append(os.path.join(self.images_dir, current_file))\n",
        "                            self.filenames.append(current_file)\n",
        "                            self.image_ids.append(current_image_id)\n",
        "                    else:\n",
        "                        self.labels.append(np.stack(current_labels, axis=0))\n",
        "                        #self.filenames.append(os.path.join(self.images_dir, current_file))\n",
        "                        self.filenames.append(current_file)\n",
        "                        self.image_ids.append(current_image_id)\n",
        "\n",
        "        self.dataset_size = len(self.filenames)\n",
        "        self.dataset_indices = np.arange(self.dataset_size, dtype=np.int32)\n",
        "        if self.load_images_into_memory:\n",
        "            self.images = []\n",
        "            if verbose: it = tqdm(self.filenames, desc='Loading images into memory', file=sys.stdout)\n",
        "            else: it = self.filenames\n",
        "            for filename in it:\n",
        "                with Image.open(filename) as image:\n",
        "                    self.images.append(np.array(image, dtype=np.uint8))\n",
        "        allimageids=self.image_ids\n",
        "        print(\"self.image_ids.............\",allimageids)#000000000000000000\n",
        "        if ret: # In case we want to return these\n",
        "            return self.images, self.filenames, self.labels, self.image_ids\n",
        "\n",
        "    \"\"\"def create_hdf5_dataset(self,\n",
        "                            file_path='dataset.h5',\n",
        "                            resize=False,\n",
        "                            variable_image_size=True,\n",
        "                            verbose=True):\n",
        "        '''\n",
        "        Converts the currently loaded dataset into a HDF5 file. This HDF5 file contains all\n",
        "        images as uncompressed arrays in a contiguous block of memory, which allows for them\n",
        "        to be loaded faster. Such an uncompressed dataset, however, may take up considerably\n",
        "        more space on your hard drive than the sum of the source images in a compressed format\n",
        "        such as JPG or PNG.\n",
        "        It is recommended that you always convert the dataset into an HDF5 dataset if you\n",
        "        have enugh hard drive space since loading from an HDF5 dataset accelerates the data\n",
        "        generation noticeably.\n",
        "        Note that you must load a dataset (e.g. via one of the parser methods) before creating\n",
        "        an HDF5 dataset from it.\n",
        "        The created HDF5 dataset will remain open upon its creation so that it can be used right\n",
        "        away.\n",
        "        Arguments:\n",
        "            file_path (str, optional): The full file path under which to store the HDF5 dataset.\n",
        "                You can load this output file via the `DataGenerator` constructor in the future.\n",
        "            resize (tuple, optional): `False` or a 2-tuple `(height, width)` that represents the\n",
        "                target size for the images. All images in the dataset will be resized to this\n",
        "                target size before they will be written to the HDF5 file. If `False`, no resizing\n",
        "                will be performed.\n",
        "            variable_image_size (bool, optional): The only purpose of this argument is that its\n",
        "                value will be stored in the HDF5 dataset in order to be able to quickly find out\n",
        "                whether the images in the dataset all have the same size or not.\n",
        "            verbose (bool, optional): Whether or not prit out the progress of the dataset creation.\n",
        "        Returns:\n",
        "            None.\n",
        "        '''\n",
        "\n",
        "        self.hdf5_dataset_path = file_path\n",
        "\n",
        "        dataset_size = len(self.filenames)\n",
        "\n",
        "        # Create the HDF5 file.\n",
        "        hdf5_dataset = h5py.File(file_path, 'w')\n",
        "\n",
        "        # Create a few attributes that tell us what this dataset contains.\n",
        "        # The dataset will obviously always contain images, but maybe it will\n",
        "        # also contain labels, image IDs, etc.\n",
        "        hdf5_dataset.attrs.create(name='has_labels', data=False, shape=None, dtype=np.bool_)\n",
        "        hdf5_dataset.attrs.create(name='has_image_ids', data=False, shape=None, dtype=np.bool_)\n",
        "        hdf5_dataset.attrs.create(name='has_eval_neutral', data=False, shape=None, dtype=np.bool_)\n",
        "        # It's useful to be able to quickly check whether the images in a dataset all\n",
        "        # have the same size or not, so add a boolean attribute for that.\n",
        "        if variable_image_size and not resize:\n",
        "            hdf5_dataset.attrs.create(name='variable_image_size', data=True, shape=None, dtype=np.bool_)\n",
        "        else:\n",
        "            hdf5_dataset.attrs.create(name='variable_image_size', data=False, shape=None, dtype=np.bool_)\n",
        "\n",
        "        # Create the dataset in which the images will be stored as flattened arrays.\n",
        "        # This allows us, among other things, to store images of variable size.\n",
        "        hdf5_images = hdf5_dataset.create_dataset(name='images',\n",
        "                                                  shape=(dataset_size,),\n",
        "                                                  maxshape=(None),\n",
        "                                                  dtype=h5py.special_dtype(vlen=np.uint8))\n",
        "\n",
        "        # Create the dataset that will hold the image heights, widths and channels that\n",
        "        # we need in order to reconstruct the images from the flattened arrays later.\n",
        "        hdf5_image_shapes = hdf5_dataset.create_dataset(name='image_shapes',\n",
        "                                                        shape=(dataset_size, 3),\n",
        "                                                        maxshape=(None, 3),\n",
        "                                                        dtype=np.int32)\n",
        "\n",
        "        if not (self.labels is None):\n",
        "\n",
        "            # Create the dataset in which the labels will be stored as flattened arrays.\n",
        "            hdf5_labels = hdf5_dataset.create_dataset(name='labels',\n",
        "                                                      shape=(dataset_size,),\n",
        "                                                      maxshape=(None),\n",
        "                                                      dtype=h5py.special_dtype(vlen=np.int32))\n",
        "\n",
        "            # Create the dataset that will hold the dimensions of the labels arrays for\n",
        "            # each image so that we can restore the labels from the flattened arrays later.\n",
        "            hdf5_label_shapes = hdf5_dataset.create_dataset(name='label_shapes',\n",
        "                                                            shape=(dataset_size, 2),\n",
        "                                                            maxshape=(None, 2),\n",
        "                                                            dtype=np.int32)\n",
        "\n",
        "            hdf5_dataset.attrs.modify(name='has_labels', value=True)\n",
        "\n",
        "        if not (self.image_ids is None):\n",
        "\n",
        "            hdf5_image_ids = hdf5_dataset.create_dataset(name='image_ids',\n",
        "                                                         shape=(dataset_size,),\n",
        "                                                         maxshape=(None),\n",
        "                                                         dtype=h5py.special_dtype(vlen=str))\n",
        "\n",
        "            hdf5_dataset.attrs.modify(name='has_image_ids', value=True)\n",
        "\n",
        "        if not (self.eval_neutral is None):\n",
        "\n",
        "            # Create the dataset in which the labels will be stored as flattened arrays.\n",
        "            hdf5_eval_neutral = hdf5_dataset.create_dataset(name='eval_neutral',\n",
        "                                                            shape=(dataset_size,),\n",
        "                                                            maxshape=(None),\n",
        "                                                            dtype=h5py.special_dtype(vlen=np.bool_))\n",
        "\n",
        "            hdf5_dataset.attrs.modify(name='has_eval_neutral', value=True)\n",
        "\n",
        "        if verbose:\n",
        "            tr = trange(dataset_size, desc='Creating HDF5 dataset', file=sys.stdout)\n",
        "        else:\n",
        "            tr = range(dataset_size)\n",
        "\n",
        "        # Iterate over all images in the dataset.\n",
        "        for i in tr:\n",
        "\n",
        "            # Store the image.\n",
        "            with Image.open(self.filenames[i]) as image:\n",
        "\n",
        "                image = np.asarray(image, dtype=np.uint8)\n",
        "\n",
        "                # Make sure all images end up having three channels.\n",
        "                if image.ndim == 2:\n",
        "                    image = np.stack([image] * 3, axis=-1)\n",
        "                elif image.ndim == 3:\n",
        "                    if image.shape[2] == 1:\n",
        "                        image = np.concatenate([image] * 3, axis=-1)\n",
        "                    elif image.shape[2] == 4:\n",
        "                        image = image[:,:,:3]\n",
        "\n",
        "                if resize:\n",
        "                    image = cv2.resize(image, dsize=(resize[1], resize[0]))\n",
        "\n",
        "                # Flatten the image array and write it to the images dataset.\n",
        "                hdf5_images[i] = image.reshape(-1)\n",
        "                # Write the image's shape to the image shapes dataset.\n",
        "                hdf5_image_shapes[i] = image.shape\n",
        "\n",
        "            # Store the ground truth if we have any.\n",
        "            if not (self.labels is None):\n",
        "\n",
        "                labels = np.asarray(self.labels[i])\n",
        "                # Flatten the labels array and write it to the labels dataset.\n",
        "                hdf5_labels[i] = labels.reshape(-1)\n",
        "                # Write the labels' shape to the label shapes dataset.\n",
        "                hdf5_label_shapes[i] = labels.shape\n",
        "\n",
        "            # Store the image ID if we have one.\n",
        "            if not (self.image_ids is None):\n",
        "\n",
        "                hdf5_image_ids[i] = self.image_ids[i]\n",
        "\n",
        "            # Store the evaluation-neutrality annotations if we have any.\n",
        "            if not (self.eval_neutral is None):\n",
        "\n",
        "                hdf5_eval_neutral[i] = self.eval_neutral[i]\n",
        "\n",
        "        hdf5_dataset.close()\n",
        "        self.hdf5_dataset = h5py.File(file_path, 'r')\n",
        "        self.hdf5_dataset_path = file_path\n",
        "        self.dataset_size = len(self.hdf5_dataset['images'])\n",
        "        self.dataset_indices = np.arange(self.dataset_size, dtype=np.int32) # Instead of shuffling the HDF5 dataset, we will shuffle this index list.\n",
        "          \"\"\"\n",
        "    def generate(self,\n",
        "                 batch_size=10,\n",
        "                 shuffle=True,\n",
        "                 transformations=[],\n",
        "                 label_encoder=None,\n",
        "                 returns={'processed_images', 'encoded_labels'},\n",
        "                 keep_images_without_gt=False,\n",
        "                 degenerate_box_handling='remove'):\n",
        "        '''\n",
        "        Generates batches of samples and (optionally) corresponding labels indefinitely.\n",
        "        Can shuffle the samples consistently after each complete pass.\n",
        "        Optionally takes a list of arbitrary image transformations to apply to the\n",
        "        samples ad hoc.\n",
        "        Arguments:\n",
        "            batch_size (int, optional): The size of the batches to be generated.\n",
        "            shuffle (bool, optional): Whether or not to shuffle the dataset before each pass.\n",
        "                This option should always be `True` during training, but it can be useful to turn shuffling off\n",
        "                for debugging or if you're using the generator for prediction.\n",
        "            transformations (list, optional): A list of transformations that will be applied to the images and labels\n",
        "                in the given order. Each transformation is a callable that takes as input an image (as a Numpy array)\n",
        "                and optionally labels (also as a Numpy array) and returns an image and optionally labels in the same\n",
        "                format.\n",
        "            label_encoder (callable, optional): Only relevant if labels are given. A callable that takes as input the\n",
        "                labels of a batch (as a list of Numpy arrays) and returns some structure that represents those labels.\n",
        "                The general use case for this is to convert labels from their input format to a format that a given object\n",
        "                detection model needs as its training targets.\n",
        "            returns (set, optional): A set of strings that determines what outputs the generator yields. The generator's output\n",
        "                is always a tuple that contains the outputs specified in this set and only those. If an output is not available,\n",
        "                it will be `None`. The output tuple can contain the following outputs according to the specified keyword strings:\n",
        "                * 'processed_images': An array containing the processed images. Will always be in the outputs, so it doesn't\n",
        "                    matter whether or not you include this keyword in the set.\n",
        "                * 'encoded_labels': The encoded labels tensor. Will always be in the outputs if a label encoder is given,\n",
        "                    so it doesn't matter whether or not you include this keyword in the set if you pass a label encoder.\n",
        "                * 'matched_anchors': Only available if `labels_encoder` is an `SSDInputEncoder` object. The same as 'encoded_labels',\n",
        "                    but containing anchor box coordinates for all matched anchor boxes instead of ground truth coordinates.\n",
        "                    This can be useful to visualize what anchor boxes are being matched to each ground truth box. Only available\n",
        "                    in training mode.\n",
        "                * 'processed_labels': The processed, but not yet encoded labels. This is a list that contains for each\n",
        "                    batch image a Numpy array with all ground truth boxes for that image. Only available if ground truth is available.\n",
        "                * 'filenames': A list containing the file names (full paths) of the images in the batch.\n",
        "                * 'image_ids': A list containing the integer IDs of the images in the batch. Only available if there\n",
        "                    are image IDs available.\n",
        "                * 'evaluation-neutral': A nested list of lists of booleans. Each list contains `True` or `False` for every ground truth\n",
        "                    bounding box of the respective image depending on whether that bounding box is supposed to be evaluation-neutral (`True`)\n",
        "                    or not (`False`). May return `None` if there exists no such concept for a given dataset. An example for\n",
        "                    evaluation-neutrality are the ground truth boxes annotated as \"difficult\" in the Pascal VOC datasets, which are\n",
        "                    usually treated to be neutral in a model evaluation.\n",
        "                * 'inverse_transform': A nested list that contains a list of \"inverter\" functions for each item in the batch.\n",
        "                    These inverter functions take (predicted) labels for an image as input and apply the inverse of the transformations\n",
        "                    that were applied to the original image to them. This makes it possible to let the model make predictions on a\n",
        "                    transformed image and then convert these predictions back to the original image. This is mostly relevant for\n",
        "                    evaluation: If you want to evaluate your model on a dataset with varying image sizes, then you are forced to\n",
        "                    transform the images somehow (e.g. by resizing or cropping) to make them all the same size. Your model will then\n",
        "                    predict boxes for those transformed images, but for the evaluation you will need predictions with respect to the\n",
        "                    original images, not with respect to the transformed images. This means you will have to transform the predicted\n",
        "                    box coordinates back to the original image sizes. Note that for each image, the inverter functions for that\n",
        "                    image need to be applied in the order in which they are given in the respective list for that image.\n",
        "                * 'original_images': A list containing the original images in the batch before any processing.\n",
        "                * 'original_labels': A list containing the original ground truth boxes for the images in this batch before any\n",
        "                    processing. Only available if ground truth is available.\n",
        "                The order of the outputs in the tuple is the order of the list above. If `returns` contains a keyword for an\n",
        "                output that is unavailable, that output omitted in the yielded tuples and a warning will be raised.\n",
        "            keep_images_without_gt (bool, optional): If `False`, images for which there aren't any ground truth boxes before\n",
        "                any transformations have been applied will be removed from the batch. If `True`, such images will be kept\n",
        "                in the batch.\n",
        "            degenerate_box_handling (str, optional): How to handle degenerate boxes, which are boxes that have `xmax <= xmin` and/or\n",
        "                `ymax <= ymin`. Degenerate boxes can sometimes be in the dataset, or non-degenerate boxes can become degenerate\n",
        "                after they were processed by transformations. Note that the generator checks for degenerate boxes after all\n",
        "                transformations have been applied (if any), but before the labels were passed to the `label_encoder` (if one was given).\n",
        "                Can be one of 'warn' or 'remove'. If 'warn', the generator will merely print a warning to let you know that there\n",
        "                are degenerate boxes in a batch. If 'remove', the generator will remove degenerate boxes from the batch silently.\n",
        "        Yields:\n",
        "            The next batch as a tuple of items as defined by the `returns` argument.\n",
        "        '''\n",
        "\n",
        "        if self.dataset_size == 0:\n",
        "            raise DatasetError(\"Cannot generate batches because you did not load a dataset.\")\n",
        "\n",
        "        #############################################################################################\n",
        "        # Warn if any of the set returns aren't possible.\n",
        "        #############################################################################################\n",
        "\n",
        "        if self.labels is None:\n",
        "            if any([ret in returns for ret in ['original_labels', 'processed_labels', 'encoded_labels', 'matched_anchors', 'evaluation-neutral']]):\n",
        "                warnings.warn(\"Since no labels were given, none of 'original_labels', 'processed_labels', 'evaluation-neutral', 'encoded_labels', and 'matched_anchors' \" +\n",
        "                              \"are possible returns, but you set `returns = {}`. The impossible returns will be `None`.\".format(returns))\n",
        "        elif label_encoder is None:\n",
        "            if any([ret in returns for ret in ['encoded_labels', 'matched_anchors']]):\n",
        "                warnings.warn(\"Since no label encoder was given, 'encoded_labels' and 'matched_anchors' aren't possible returns, \" +\n",
        "                              \"but you set `returns = {}`. The impossible returns will be `None`.\".format(returns))\n",
        "        elif not isinstance(label_encoder, SSDInputEncoder):\n",
        "            if 'matched_anchors' in returns:\n",
        "                warnings.warn(\"`label_encoder` is not an `SSDInputEncoder` object, therefore 'matched_anchors' is not a possible return, \" +\n",
        "                              \"but you set `returns = {}`. The impossible returns will be `None`.\".format(returns))\n",
        "\n",
        "        #############################################################################################\n",
        "        # Do a few preparatory things like maybe shuffling the dataset initially.\n",
        "        #############################################################################################\n",
        "\n",
        "        if shuffle:\n",
        "            objects_to_shuffle = [self.dataset_indices]\n",
        "            if not (self.filenames is None):\n",
        "                objects_to_shuffle.append(self.filenames)\n",
        "            if not (self.labels is None):\n",
        "                objects_to_shuffle.append(self.labels)\n",
        "            if not (self.image_ids is None):\n",
        "                objects_to_shuffle.append(self.image_ids)\n",
        "            if not (self.eval_neutral is None):\n",
        "                objects_to_shuffle.append(self.eval_neutral)\n",
        "            shuffled_objects = sklearn.utils.shuffle(*objects_to_shuffle)\n",
        "            for i in range(len(objects_to_shuffle)):\n",
        "                objects_to_shuffle[i][:] = shuffled_objects[i]\n",
        "\n",
        "        if degenerate_box_handling == 'remove':\n",
        "            box_filter = BoxFilter(check_overlap=False,\n",
        "                                   check_min_area=False,\n",
        "                                   check_degenerate=True,\n",
        "                                   labels_format=self.labels_format)\n",
        "\n",
        "        # Override the labels formats of all the transformations to make sure they are set correctly.\n",
        "        if not (self.labels is None):\n",
        "            for transform in transformations:\n",
        "                transform.labels_format = self.labels_format\n",
        "\n",
        "        #############################################################################################\n",
        "        # Generate mini batches.\n",
        "        #############################################################################################\n",
        "\n",
        "        current = 0\n",
        "\n",
        "        while True:\n",
        "\n",
        "            batch_X, batch_y = [], []\n",
        "\n",
        "            if current >= self.dataset_size:\n",
        "                current = 0\n",
        "\n",
        "            #########################################################################################\n",
        "            # Maybe shuffle the dataset if a full pass over the dataset has finished.\n",
        "            #########################################################################################\n",
        "\n",
        "                if shuffle:\n",
        "                    objects_to_shuffle = [self.dataset_indices]\n",
        "                    if not (self.filenames is None):\n",
        "                        objects_to_shuffle.append(self.filenames)\n",
        "                    if not (self.labels is None):\n",
        "                        objects_to_shuffle.append(self.labels)\n",
        "                    if not (self.image_ids is None):\n",
        "                        objects_to_shuffle.append(self.image_ids)\n",
        "                    if not (self.eval_neutral is None):\n",
        "                        objects_to_shuffle.append(self.eval_neutral)\n",
        "                    shuffled_objects = sklearn.utils.shuffle(*objects_to_shuffle)\n",
        "                    for i in range(len(objects_to_shuffle)):\n",
        "                        objects_to_shuffle[i][:] = shuffled_objects[i]\n",
        "\n",
        "            #########################################################################################\n",
        "            # Get the images, (maybe) image IDs, (maybe) labels, etc. for this batch.\n",
        "            #########################################################################################\n",
        "\n",
        "            # We prioritize our options in the following order:\n",
        "            # 1) If we have the images already loaded in memory, get them from there.\n",
        "            # 2) Else, if we have an HDF5 dataset, get the images from there.\n",
        "            # 3) Else, if we have neither of the above, we'll have to load the individual image\n",
        "            #    files from disk.\n",
        "            batch_indices = self.dataset_indices[current:current+batch_size]\n",
        "            if not (self.images is None):\n",
        "                for i in batch_indices:\n",
        "                    batch_X.append(self.images[i])\n",
        "                if not (self.filenames is None):\n",
        "                    batch_filenames = self.filenames[current:current+batch_size]\n",
        "                else:\n",
        "                    batch_filenames = None\n",
        "            elif not (self.hdf5_dataset is None):\n",
        "                for i in batch_indices:\n",
        "                    batch_X.append(self.hdf5_dataset['images'][i].reshape(self.hdf5_dataset['image_shapes'][i]))\n",
        "                if not (self.filenames is None):\n",
        "                    batch_filenames = self.filenames[current:current+batch_size]\n",
        "                else:\n",
        "                    batch_filenames = None\n",
        "            else:\n",
        "                batch_filenames = self.filenames[current:current+batch_size]\n",
        "                for filename in batch_filenames:\n",
        "                    with Image.open(filename) as image:  \n",
        "                        batch_X.append(np.array(image, dtype=np.uint8))\n",
        "\n",
        "            # Get the labels for this batch (if there are any).\n",
        "            if not (self.labels is None):\n",
        "                batch_y = deepcopy(self.labels[current:current+batch_size])\n",
        "            else:\n",
        "                batch_y = None\n",
        "\n",
        "            if not (self.eval_neutral is None):\n",
        "                batch_eval_neutral = self.eval_neutral[current:current+batch_size]\n",
        "            else:\n",
        "                batch_eval_neutral = None\n",
        "\n",
        "            # Get the image IDs for this batch (if there are any).\n",
        "            if not (self.image_ids is None):\n",
        "                batch_image_ids = self.image_ids[current:current+batch_size]\n",
        "            else:\n",
        "                batch_image_ids = None\n",
        "\n",
        "            if 'original_images' in returns:\n",
        "                batch_original_images = deepcopy(batch_X) # The original, unaltered images\n",
        "            if 'original_labels' in returns:\n",
        "                batch_original_labels = deepcopy(batch_y) # The original, unaltered labels\n",
        "\n",
        "            current += batch_size\n",
        "\n",
        "            #########################################################################################\n",
        "            # Maybe perform image transformations.\n",
        "            #########################################################################################\n",
        "\n",
        "            batch_items_to_remove = [] # In case we need to remove any images from the batch, store their indices in this list.\n",
        "            batch_inverse_transforms = []\n",
        "\n",
        "            for i in range(len(batch_X)):\n",
        "\n",
        "                if not (self.labels is None):\n",
        "                    # Convert the labels for this image to an array (in case they aren't already).\n",
        "                    batch_y[i] = np.array(batch_y[i])\n",
        "                    # If this image has no ground truth boxes, maybe we don't want to keep it in the batch.\n",
        "                    if (batch_y[i].size == 0) and not keep_images_without_gt:\n",
        "                        batch_items_to_remove.append(i)\n",
        "                        batch_inverse_transforms.append([])\n",
        "                        continue\n",
        "\n",
        "                # Apply any image transformations we may have received.\n",
        "                if transformations:\n",
        "\n",
        "                    inverse_transforms = []\n",
        "\n",
        "                    for transform in transformations:\n",
        "\n",
        "                        if not (self.labels is None):\n",
        "\n",
        "                            if ('inverse_transform' in returns) and ('return_inverter' in inspect.signature(transform).parameters):\n",
        "                                batch_X[i], batch_y[i], inverse_transform = transform(batch_X[i], batch_y[i], return_inverter=True)\n",
        "                                inverse_transforms.append(inverse_transform)\n",
        "                            else:\n",
        "                                batch_X[i], batch_y[i] = transform(batch_X[i], batch_y[i])\n",
        "\n",
        "                            if batch_X[i] is None: # In case the transform failed to produce an output image, which is possible for some random transforms.\n",
        "                                batch_items_to_remove.append(i)\n",
        "                                batch_inverse_transforms.append([])\n",
        "                                continue\n",
        "\n",
        "                        else:\n",
        "\n",
        "                            if ('inverse_transform' in returns) and ('return_inverter' in inspect.signature(transform).parameters):\n",
        "                                batch_X[i], inverse_transform = transform(batch_X[i], return_inverter=True)\n",
        "                                inverse_transforms.append(inverse_transform)\n",
        "                            else:\n",
        "                                batch_X[i] = transform(batch_X[i])\n",
        "\n",
        "                    batch_inverse_transforms.append(inverse_transforms[::-1])\n",
        "\n",
        "                #########################################################################################\n",
        "                # Check for degenerate boxes in this batch item.\n",
        "                #########################################################################################\n",
        "\n",
        "                if not (self.labels is None):\n",
        "\n",
        "                    xmin = self.labels_format['xmin']\n",
        "                    ymin = self.labels_format['ymin']\n",
        "                    xmax = self.labels_format['xmax']\n",
        "                    ymax = self.labels_format['ymax']\n",
        "\n",
        "                    if np.any(batch_y[i][:,xmax] - batch_y[i][:,xmin] <= 0) or np.any(batch_y[i][:,ymax] - batch_y[i][:,ymin] <= 0):\n",
        "                        if degenerate_box_handling == 'warn':\n",
        "                            warnings.warn(\"Detected degenerate ground truth bounding boxes for batch item {} with bounding boxes {}, \".format(i, batch_y[i]) +\n",
        "                                          \"i.e. bounding boxes where xmax <= xmin and/or ymax <= ymin. \" +\n",
        "                                          \"This could mean that your dataset contains degenerate ground truth boxes, or that any image transformations you may apply might \" +\n",
        "                                          \"result in degenerate ground truth boxes, or that you are parsing the ground truth in the wrong coordinate format.\" +\n",
        "                                          \"Degenerate ground truth bounding boxes may lead to NaN errors during the training.\")\n",
        "                        elif degenerate_box_handling == 'remove':\n",
        "                            batch_y[i] = box_filter(batch_y[i])\n",
        "                            if (batch_y[i].size == 0) and not keep_images_without_gt:\n",
        "                                batch_items_to_remove.append(i)\n",
        "\n",
        "            #########################################################################################\n",
        "            # Remove any items we might not want to keep from the batch.\n",
        "            #########################################################################################\n",
        "\n",
        "            if batch_items_to_remove:\n",
        "                for j in sorted(batch_items_to_remove, reverse=True):\n",
        "                    # This isn't efficient, but it hopefully shouldn't need to be done often anyway.\n",
        "                    batch_X.pop(j)\n",
        "                    batch_filenames.pop(j)\n",
        "                    if batch_inverse_transforms: batch_inverse_transforms.pop(j)\n",
        "                    if not (self.labels is None): batch_y.pop(j)\n",
        "                    if not (self.image_ids is None): batch_image_ids.pop(j)\n",
        "                    if not (self.eval_neutral is None): batch_eval_neutral.pop(j)\n",
        "                    if 'original_images' in returns: batch_original_images.pop(j)\n",
        "                    if 'original_labels' in returns and not (self.labels is None): batch_original_labels.pop(j)\n",
        "\n",
        "            #########################################################################################\n",
        "\n",
        "            # CAUTION: Converting `batch_X` into an array will result in an empty batch if the images have varying sizes\n",
        "            #          or varying numbers of channels. At this point, all images must have the same size and the same\n",
        "            #          number of channels.\n",
        "            batch_X = np.array(batch_X)\n",
        "            if (batch_X.size == 0):\n",
        "                raise DegenerateBatchError(\"You produced an empty batch. This might be because the images in the batch vary \" +\n",
        "                                           \"in their size and/or number of channels. Note that after all transformations \" +\n",
        "                                           \"(if any were given) have been applied to all images in the batch, all images \" +\n",
        "                                           \"must be homogenous in size along all axes.\")\n",
        "\n",
        "            #########################################################################################\n",
        "            # If we have a label encoder, encode our labels.\n",
        "            #########################################################################################\n",
        "\n",
        "            if not (label_encoder is None or self.labels is None):\n",
        "\n",
        "                if ('matched_anchors' in returns) and isinstance(label_encoder, SSDInputEncoder):\n",
        "                    batch_y_encoded, batch_matched_anchors = label_encoder(batch_y, diagnostics=True)\n",
        "                else:\n",
        "                    batch_y_encoded = label_encoder(batch_y, diagnostics=False)\n",
        "                    batch_matched_anchors = None\n",
        "\n",
        "            else:\n",
        "                batch_y_encoded = None\n",
        "                batch_matched_anchors = None\n",
        "\n",
        "            #########################################################################################\n",
        "            # Compose the output.\n",
        "            #########################################################################################\n",
        "\n",
        "            ret = []\n",
        "            if 'processed_images' in returns: ret.append(batch_X)\n",
        "            if 'encoded_labels' in returns: ret.append(batch_y_encoded)\n",
        "            if 'matched_anchors' in returns: ret.append(batch_matched_anchors)\n",
        "            if 'processed_labels' in returns: ret.append(batch_y)\n",
        "            if 'filenames' in returns: ret.append(batch_filenames)\n",
        "            if 'image_ids' in returns: ret.append(batch_image_ids)\n",
        "            if 'evaluation-neutral' in returns: ret.append(batch_eval_neutral)\n",
        "            if 'inverse_transform' in returns: ret.append(batch_inverse_transforms)\n",
        "            if 'original_images' in returns: ret.append(batch_original_images)\n",
        "            if 'original_labels' in returns: ret.append(batch_original_labels)\n",
        "\n",
        "            yield ret\n",
        "\n",
        "    def save_dataset(self,\n",
        "                     filenames_path='filenames.pkl',\n",
        "                     labels_path=None,\n",
        "                     image_ids_path=None,\n",
        "                     eval_neutral_path=None):\n",
        "        '''\n",
        "        Writes the current `filenames`, `labels`, and `image_ids` lists to the specified files.\n",
        "        This is particularly useful for large datasets with annotations that are\n",
        "        parsed from XML files, which can take quite long. If you'll be using the\n",
        "        same dataset repeatedly, you don't want to have to parse the XML label\n",
        "        files every time.\n",
        "        Arguments:\n",
        "            filenames_path (str): The path under which to save the filenames pickle.\n",
        "            labels_path (str): The path under which to save the labels pickle.\n",
        "            image_ids_path (str, optional): The path under which to save the image IDs pickle.\n",
        "            eval_neutral_path (str, optional): The path under which to save the pickle for\n",
        "                the evaluation-neutrality annotations.\n",
        "        '''\n",
        "        with open(filenames_path, 'wb') as f:\n",
        "            pickle.dump(self.filenames, f)\n",
        "        if not labels_path is None:\n",
        "            with open(labels_path, 'wb') as f:\n",
        "                pickle.dump(self.labels, f)\n",
        "        if not image_ids_path is None:\n",
        "            with open(image_ids_path, 'wb') as f:\n",
        "                pickle.dump(self.image_ids, f)\n",
        "        if not eval_neutral_path is None:\n",
        "            with open(eval_neutral_path, 'wb') as f:\n",
        "                pickle.dump(self.eval_neutral, f)\n",
        "\n",
        "    def get_dataset(self):\n",
        "        '''\n",
        "        Returns:\n",
        "            4-tuple containing lists and/or `None` for the filenames, labels, image IDs,\n",
        "            and evaluation-neutrality annotations.\n",
        "        '''\n",
        "        return self.filenames, self.labels, self.image_ids, self.eval_neutral\n",
        "\n",
        "    def get_dataset_size(self):\n",
        "        '''\n",
        "        Returns:\n",
        "            The number of images in the dataset.\n",
        "        '''\n",
        "        return self.dataset_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKurRHUJKEWd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "'''\n",
        "Various photometric image transformations, both deterministic and probabilistic.\n",
        "Copyright (C) 2018 Pierluigi Ferrari\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "   http://www.apache.org/licenses/LICENSE-2.0\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "'''\n",
        "\n",
        "from __future__ import division\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "class ConvertColor:\n",
        "    '''\n",
        "    Converts images between RGB, HSV and grayscale color spaces. This is just a wrapper\n",
        "    around `cv2.cvtColor()`.\n",
        "    '''\n",
        "    def __init__(self, current='RGB', to='HSV', keep_3ch=True):\n",
        "        '''\n",
        "        Arguments:\n",
        "            current (str, optional): The current color space of the images. Can be\n",
        "                one of 'RGB' and 'HSV'.\n",
        "            to (str, optional): The target color space of the images. Can be one of\n",
        "                'RGB', 'HSV', and 'GRAY'.\n",
        "            keep_3ch (bool, optional): Only relevant if `to == GRAY`.\n",
        "                If `True`, the resulting grayscale images will have three channels.\n",
        "        '''\n",
        "        if not ((current in {'RGB', 'HSV'}) and (to in {'RGB', 'HSV', 'GRAY'})):\n",
        "            raise NotImplementedError\n",
        "        self.current = current\n",
        "        self.to = to\n",
        "        self.keep_3ch = keep_3ch\n",
        "\n",
        "    def __call__(self, image, labels=None):\n",
        "        if self.current == 'RGB' and self.to == 'HSV':\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
        "        elif self.current == 'RGB' and self.to == 'GRAY':\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "            if self.keep_3ch:\n",
        "                image = np.stack([image] * 3, axis=-1)\n",
        "        elif self.current == 'HSV' and self.to == 'RGB':\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_HSV2RGB)\n",
        "        elif self.current == 'HSV' and self.to == 'GRAY':\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_HSV2GRAY)\n",
        "            if self.keep_3ch:\n",
        "                image = np.stack([image] * 3, axis=-1)\n",
        "        if labels is None:\n",
        "            return image\n",
        "        else:\n",
        "            return image, labels\n",
        "\n",
        "class ConvertDataType:\n",
        "    '''\n",
        "    Converts images represented as Numpy arrays between `uint8` and `float32`.\n",
        "    Serves as a helper for certain photometric distortions. This is just a wrapper\n",
        "    around `np.ndarray.astype()`.\n",
        "    '''\n",
        "    def __init__(self, to='uint8'):\n",
        "        '''\n",
        "        Arguments:\n",
        "            to (string, optional): To which datatype to convert the input images.\n",
        "                Can be either of 'uint8' and 'float32'.\n",
        "        '''\n",
        "        if not (to == 'uint8' or to == 'float32'):\n",
        "            raise ValueError(\"`to` can be either of 'uint8' or 'float32'.\")\n",
        "        self.to = to\n",
        "\n",
        "    def __call__(self, image, labels=None):\n",
        "        if self.to == 'uint8':\n",
        "            image = np.round(image, decimals=0).astype(np.uint8)\n",
        "        else:\n",
        "            image = image.astype(np.float32)\n",
        "        if labels is None:\n",
        "            return image\n",
        "        else:\n",
        "            return image, labels\n",
        "\n",
        "class ConvertTo3Channels:\n",
        "    '''\n",
        "    Converts 1-channel and 4-channel images to 3-channel images. Does nothing to images that\n",
        "    already have 3 channels. In the case of 4-channel images, the fourth channel will be\n",
        "    discarded.\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def __call__(self, image, labels=None):\n",
        "        if image.ndim == 2:\n",
        "            image = np.stack([image] * 3, axis=-1)\n",
        "        elif image.ndim == 3:\n",
        "            if image.shape[2] == 1:\n",
        "                image = np.concatenate([image] * 3, axis=-1)\n",
        "            elif image.shape[2] == 4:\n",
        "                image = image[:,:,:3]\n",
        "        if labels is None:\n",
        "            return image\n",
        "        else:\n",
        "            return image, labels\n",
        "\n",
        "class Hue:\n",
        "    '''\n",
        "    Changes the hue of HSV images.\n",
        "    Important:\n",
        "        - Expects HSV input.\n",
        "        - Expects input array to be of `dtype` `float`.\n",
        "    '''\n",
        "    def __init__(self, delta):\n",
        "        '''\n",
        "        Arguments:\n",
        "            delta (int): An integer in the closed interval `[-180, 180]` that determines the hue change, where\n",
        "                a change by integer `delta` means a change by `2 * delta` degrees. Read up on the HSV color format\n",
        "                if you need more information.\n",
        "        '''\n",
        "        if not (-180 <= delta <= 180): raise ValueError(\"`delta` must be in the closed interval `[-180, 180]`.\")\n",
        "        self.delta = delta\n",
        "\n",
        "    def __call__(self, image, labels=None):\n",
        "        image[:, :, 0] = (image[:, :, 0] + self.delta) % 180.0\n",
        "        if labels is None:\n",
        "            return image\n",
        "        else:\n",
        "            return image, labels\n",
        "\n",
        "class RandomHue:\n",
        "    '''\n",
        "    Randomly changes the hue of HSV images.\n",
        "    Important:\n",
        "        - Expects HSV input.\n",
        "        - Expects input array to be of `dtype` `float`.\n",
        "    '''\n",
        "    def __init__(self, max_delta=18, prob=0.5):\n",
        "        '''\n",
        "        Arguments:\n",
        "            max_delta (int): An integer in the closed interval `[0, 180]` that determines the maximal absolute\n",
        "                hue change.\n",
        "            prob (float, optional): `(1 - prob)` determines the probability with which the original,\n",
        "                unaltered image is returned.\n",
        "        '''\n",
        "        if not (0 <= max_delta <= 180): raise ValueError(\"`max_delta` must be in the closed interval `[0, 180]`.\")\n",
        "        self.max_delta = max_delta\n",
        "        self.prob = prob\n",
        "        self.change_hue = Hue(delta=0)\n",
        "\n",
        "    def __call__(self, image, labels=None):\n",
        "        p = np.random.uniform(0,1)\n",
        "        if p >= (1.0-self.prob):\n",
        "            self.change_hue.delta = np.random.uniform(-self.max_delta, self.max_delta)\n",
        "            return self.change_hue(image, labels)\n",
        "        elif labels is None:\n",
        "            return image\n",
        "        else:\n",
        "            return image, labels\n",
        "\n",
        "class Saturation:\n",
        "    '''\n",
        "    Changes the saturation of HSV images.\n",
        "    Important:\n",
        "        - Expects HSV input.\n",
        "        - Expects input array to be of `dtype` `float`.\n",
        "    '''\n",
        "    def __init__(self, factor):\n",
        "        '''\n",
        "        Arguments:\n",
        "            factor (float): A float greater than zero that determines saturation change, where\n",
        "                values less than one result in less saturation and values greater than one result\n",
        "                in more saturation.\n",
        "        '''\n",
        "        if factor <= 0.0: raise ValueError(\"It must be `factor > 0`.\")\n",
        "        self.factor = factor\n",
        "\n",
        "    def __call__(self, image, labels=None):\n",
        "        image[:,:,1] = np.clip(image[:,:,1] * self.factor, 0, 255)\n",
        "        if labels is None:\n",
        "            return image\n",
        "        else:\n",
        "            return image, labels\n",
        "\n",
        "class RandomSaturation:\n",
        "    '''\n",
        "    Randomly changes the saturation of HSV images.\n",
        "    Important:\n",
        "        - Expects HSV input.\n",
        "        - Expects input array to be of `dtype` `float`.\n",
        "    '''\n",
        "    def __init__(self, lower=0.3, upper=2.0, prob=0.5):\n",
        "        '''\n",
        "        Arguments:\n",
        "            lower (float, optional): A float greater than zero, the lower bound for the random\n",
        "                saturation change.\n",
        "            upper (float, optional): A float greater than zero, the upper bound for the random\n",
        "                saturation change. Must be greater than `lower`.\n",
        "            prob (float, optional): `(1 - prob)` determines the probability with which the original,\n",
        "                unaltered image is returned.\n",
        "        '''\n",
        "        if lower >= upper: raise ValueError(\"`upper` must be greater than `lower`.\")\n",
        "        self.lower = lower\n",
        "        self.upper = upper\n",
        "        self.prob = prob\n",
        "        self.change_saturation = Saturation(factor=1.0)\n",
        "\n",
        "    def __call__(self, image, labels=None):\n",
        "        p = np.random.uniform(0,1)\n",
        "        if p >= (1.0-self.prob):\n",
        "            self.change_saturation.factor = np.random.uniform(self.lower, self.upper)\n",
        "            return self.change_saturation(image, labels)\n",
        "        elif labels is None:\n",
        "            return image\n",
        "        else:\n",
        "            return image, labels\n",
        "\n",
        "class Brightness:\n",
        "    '''\n",
        "    Changes the brightness of RGB images.\n",
        "    Important:\n",
        "        - Expects RGB input.\n",
        "        - Expects input array to be of `dtype` `float`.\n",
        "    '''\n",
        "    def __init__(self, delta):\n",
        "        '''\n",
        "        Arguments:\n",
        "            delta (int): An integer, the amount to add to or subtract from the intensity\n",
        "                of every pixel.\n",
        "        '''\n",
        "        self.delta = delta\n",
        "\n",
        "    def __call__(self, image, labels=None):\n",
        "        image = np.clip(image + self.delta, 0, 255)\n",
        "        if labels is None:\n",
        "            return image\n",
        "        else:\n",
        "            return image, labels\n",
        "\n",
        "class RandomBrightness:\n",
        "    '''\n",
        "    Randomly changes the brightness of RGB images.\n",
        "    Important:\n",
        "        - Expects RGB input.\n",
        "        - Expects input array to be of `dtype` `float`.\n",
        "    '''\n",
        "    def __init__(self, lower=-84, upper=84, prob=0.5):\n",
        "        '''\n",
        "        Arguments:\n",
        "            lower (int, optional): An integer, the lower bound for the random brightness change.\n",
        "            upper (int, optional): An integer, the upper bound for the random brightness change.\n",
        "                Must be greater than `lower`.\n",
        "            prob (float, optional): `(1 - prob)` determines the probability with which the original,\n",
        "                unaltered image is returned.\n",
        "        '''\n",
        "        if lower >= upper: raise ValueError(\"`upper` must be greater than `lower`.\")\n",
        "        self.lower = float(lower)\n",
        "        self.upper = float(upper)\n",
        "        self.prob = prob\n",
        "        self.change_brightness = Brightness(delta=0)\n",
        "\n",
        "    def __call__(self, image, labels=None):\n",
        "        p = np.random.uniform(0,1)\n",
        "        if p >= (1.0-self.prob):\n",
        "            self.change_brightness.delta = np.random.uniform(self.lower, self.upper)\n",
        "            return self.change_brightness(image, labels)\n",
        "        elif labels is None:\n",
        "            return image\n",
        "        else:\n",
        "            return image, labels\n",
        "\n",
        "class Contrast:\n",
        "    '''\n",
        "    Changes the contrast of RGB images.\n",
        "    Important:\n",
        "        - Expects RGB input.\n",
        "        - Expects input array to be of `dtype` `float`.\n",
        "    '''\n",
        "    def __init__(self, factor):\n",
        "        '''\n",
        "        Arguments:\n",
        "            factor (float): A float greater than zero that determines contrast change, where\n",
        "                values less than one result in less contrast and values greater than one result\n",
        "                in more contrast.\n",
        "        '''\n",
        "        if factor <= 0.0: raise ValueError(\"It must be `factor > 0`.\")\n",
        "        self.factor = factor\n",
        "\n",
        "    def __call__(self, image, labels=None):\n",
        "        image = np.clip(127.5 + self.factor * (image - 127.5), 0, 255)\n",
        "        if labels is None:\n",
        "            return image\n",
        "        else:\n",
        "            return image, labels\n",
        "\n",
        "class RandomContrast:\n",
        "    '''\n",
        "    Randomly changes the contrast of RGB images.\n",
        "    Important:\n",
        "        - Expects RGB input.\n",
        "        - Expects input array to be of `dtype` `float`.\n",
        "    '''\n",
        "    def __init__(self, lower=0.5, upper=1.5, prob=0.5):\n",
        "        '''\n",
        "        Arguments:\n",
        "            lower (float, optional): A float greater than zero, the lower bound for the random\n",
        "                contrast change.\n",
        "            upper (float, optional): A float greater than zero, the upper bound for the random\n",
        "                contrast change. Must be greater than `lower`.\n",
        "            prob (float, optional): `(1 - prob)` determines the probability with which the original,\n",
        "                unaltered image is returned.\n",
        "        '''\n",
        "        if lower >= upper: raise ValueError(\"`upper` must be greater than `lower`.\")\n",
        "        self.lower = lower\n",
        "        self.upper = upper\n",
        "        self.prob = prob\n",
        "        self.change_contrast = Contrast(factor=1.0)\n",
        "\n",
        "    def __call__(self, image, labels=None):\n",
        "        p = np.random.uniform(0,1)\n",
        "        if p >= (1.0-self.prob):\n",
        "            self.change_contrast.factor = np.random.uniform(self.lower, self.upper)\n",
        "            return self.change_contrast(image, labels)\n",
        "        elif labels is None:\n",
        "            return image\n",
        "        else:\n",
        "            return image, labels\n",
        "\n",
        "class Gamma:\n",
        "    '''\n",
        "    Changes the gamma value of RGB images.\n",
        "    Important: Expects RGB input.\n",
        "    '''\n",
        "    def __init__(self, gamma):\n",
        "        '''\n",
        "        Arguments:\n",
        "            gamma (float): A float greater than zero that determines gamma change.\n",
        "        '''\n",
        "        if gamma <= 0.0: raise ValueError(\"It must be `gamma > 0`.\")\n",
        "        self.gamma = gamma\n",
        "        self.gamma_inv = 1.0 / gamma\n",
        "        # Build a lookup table mapping the pixel values [0, 255] to\n",
        "        # their adjusted gamma values.\n",
        "        self.table = np.array([((i / 255.0) ** self.gamma_inv) * 255 for i in np.arange(0, 256)]).astype(\"uint8\")\n",
        "\n",
        "    def __call__(self, image, labels=None):\n",
        "        image = cv2.LUT(image, table)\n",
        "        if labels is None:\n",
        "            return image\n",
        "        else:\n",
        "            return image, labels\n",
        "\n",
        "class RandomGamma:\n",
        "    '''\n",
        "    Randomly changes the gamma value of RGB images.\n",
        "    Important: Expects RGB input.\n",
        "    '''\n",
        "    def __init__(self, lower=0.25, upper=2.0, prob=0.5):\n",
        "        '''\n",
        "        Arguments:\n",
        "            lower (float, optional): A float greater than zero, the lower bound for the random\n",
        "                gamma change.\n",
        "            upper (float, optional): A float greater than zero, the upper bound for the random\n",
        "                gamma change. Must be greater than `lower`.\n",
        "            prob (float, optional): `(1 - prob)` determines the probability with which the original,\n",
        "                unaltered image is returned.\n",
        "        '''\n",
        "        if lower >= upper: raise ValueError(\"`upper` must be greater than `lower`.\")\n",
        "        self.lower = lower\n",
        "        self.upper = upper\n",
        "        self.prob = prob\n",
        "\n",
        "    def __call__(self, image, labels=None):\n",
        "        p = np.random.uniform(0,1)\n",
        "        if p >= (1.0-self.prob):\n",
        "            gamma = np.random.uniform(self.lower, self.upper)\n",
        "            change_gamma = Gamma(gamma=gamma)\n",
        "            return change_gamma(image, labels)\n",
        "        elif labels is None:\n",
        "            return image\n",
        "        else:\n",
        "            return image, labels\n",
        "\n",
        "class HistogramEqualization:\n",
        "    '''\n",
        "    Performs histogram equalization on HSV images.\n",
        "    Importat: Expects HSV input.\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def __call__(self, image, labels=None):\n",
        "        image[:,:,2] = cv2.equalizeHist(image[:,:,2])\n",
        "        if labels is None:\n",
        "            return image\n",
        "        else:\n",
        "            return image, labels\n",
        "\n",
        "class RandomHistogramEqualization:\n",
        "    '''\n",
        "    Randomly performs histogram equalization on HSV images. The randomness only refers\n",
        "    to whether or not the equalization is performed.\n",
        "    Importat: Expects HSV input.\n",
        "    '''\n",
        "    def __init__(self, prob=0.5):\n",
        "        '''\n",
        "        Arguments:\n",
        "            prob (float, optional): `(1 - prob)` determines the probability with which the original,\n",
        "                unaltered image is returned.\n",
        "        '''\n",
        "        self.prob = prob\n",
        "        self.equalize = HistogramEqualization()\n",
        "\n",
        "    def __call__(self, image, labels=None):\n",
        "        p = np.random.uniform(0,1)\n",
        "        if p >= (1.0-self.prob):\n",
        "            return self.equalize(image, labels)\n",
        "        elif labels is None:\n",
        "            return image\n",
        "        else:\n",
        "            return image, labels\n",
        "\n",
        "class ChannelSwap:\n",
        "    '''\n",
        "    Swaps the channels of images.\n",
        "    '''\n",
        "    def __init__(self, order):\n",
        "        '''\n",
        "        Arguments:\n",
        "            order (tuple): A tuple of integers that defines the desired channel order\n",
        "                of the input images after the channel swap.\n",
        "        '''\n",
        "        self.order = order\n",
        "\n",
        "    def __call__(self, image, labels=None):\n",
        "        image = image[:,:,self.order]\n",
        "        if labels is None:\n",
        "            return image\n",
        "        else:\n",
        "            return image, labels\n",
        "\n",
        "class RandomChannelSwap:\n",
        "    '''\n",
        "    Randomly swaps the channels of RGB images.\n",
        "    Important: Expects RGB input.\n",
        "    '''\n",
        "    def __init__(self, prob=0.5):\n",
        "        '''\n",
        "        Arguments:\n",
        "            prob (float, optional): `(1 - prob)` determines the probability with which the original,\n",
        "                unaltered image is returned.\n",
        "        '''\n",
        "        self.prob = prob\n",
        "        # All possible permutations of the three image channels except the original order.\n",
        "        self.permutations = ((0, 2, 1),\n",
        "                             (1, 0, 2), (1, 2, 0),\n",
        "                             (2, 0, 1), (2, 1, 0))\n",
        "        self.swap_channels = ChannelSwap(order=(0, 1, 2))\n",
        "\n",
        "    def __call__(self, image, labels=None):\n",
        "        p = np.random.uniform(0,1)\n",
        "        if p >= (1.0-self.prob):\n",
        "            i = np.random.randint(5) # There are 6 possible permutations.\n",
        "            self.swap_channels.order = self.permutations[i]\n",
        "            return self.swap_channels(image, labels)\n",
        "        elif labels is None:\n",
        "            return image\n",
        "        else:\n",
        "            return image, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxAH9IHFJmar",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Utilities for 2D object detection related to answering the following questions:\n",
        "1. Given an image size and bounding boxes, which bounding boxes meet certain\n",
        "   requirements with respect to the image size?\n",
        "2. Given an image size and bounding boxes, is an image of that size valid with\n",
        "   respect to the bounding boxes according to certain requirements?\n",
        "Copyright (C) 2018 Pierluigi Ferrari\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "   http://www.apache.org/licenses/LICENSE-2.0\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "'''\n",
        "\n",
        "from __future__ import division\n",
        "import numpy as np\n",
        "\n",
        "#from bounding_box_utils.bounding_box_utils import iou\n",
        "\n",
        "class BoundGenerator:\n",
        "    '''\n",
        "    Generates pairs of floating point values that represent lower and upper bounds\n",
        "    from a given sample space.\n",
        "    '''\n",
        "    def __init__(self,\n",
        "                 sample_space=((0.1, None),\n",
        "                               (0.3, None),\n",
        "                               (0.5, None),\n",
        "                               (0.7, None),\n",
        "                               (0.9, None),\n",
        "                               (None, None)),\n",
        "                 weights=None):\n",
        "        '''\n",
        "        Arguments:\n",
        "            sample_space (list or tuple): A list, tuple, or array-like object of shape\n",
        "                `(n, 2)` that contains `n` samples to choose from, where each sample\n",
        "                is a 2-tuple of scalars and/or `None` values.\n",
        "            weights (list or tuple, optional): A list or tuple representing the distribution\n",
        "                over the sample space. If `None`, a uniform distribution will be assumed.\n",
        "        '''\n",
        "\n",
        "        if (not (weights is None)) and len(weights) != len(sample_space):\n",
        "            raise ValueError(\"`weights` must either be `None` for uniform distribution or have the same length as `sample_space`.\")\n",
        "\n",
        "        self.sample_space = []\n",
        "        for bound_pair in sample_space:\n",
        "            if len(bound_pair) != 2:\n",
        "                raise ValueError(\"All elements of the sample space must be 2-tuples.\")\n",
        "            bound_pair = list(bound_pair)\n",
        "            if bound_pair[0] is None: bound_pair[0] = 0.0\n",
        "            if bound_pair[1] is None: bound_pair[1] = 1.0\n",
        "            if bound_pair[0] > bound_pair[1]:\n",
        "                raise ValueError(\"For all sample space elements, the lower bound cannot be greater than the upper bound.\")\n",
        "            self.sample_space.append(bound_pair)\n",
        "\n",
        "        self.sample_space_size = len(self.sample_space)\n",
        "\n",
        "        if weights is None:\n",
        "            self.weights = [1.0/self.sample_space_size] * self.sample_space_size\n",
        "        else:\n",
        "            self.weights = weights\n",
        "\n",
        "    def __call__(self):\n",
        "        '''\n",
        "        Returns:\n",
        "            An item of the sample space, i.e. a 2-tuple of scalars.\n",
        "        '''\n",
        "        i = np.random.choice(self.sample_space_size, p=self.weights)\n",
        "        return self.sample_space[i]\n",
        "\n",
        "class BoxFilter:\n",
        "    '''\n",
        "    Returns all bounding boxes that are valid with respect to a the defined criteria.\n",
        "    '''\n",
        "\n",
        "    def __init__(self,\n",
        "                 check_overlap=True,\n",
        "                 check_min_area=True,\n",
        "                 check_degenerate=True,\n",
        "                 overlap_criterion='center_point',\n",
        "                 overlap_bounds=(0.3, 1.0),\n",
        "                 min_area=16,\n",
        "                 labels_format={'class_id': 0, 'xmin': 1, 'ymin': 2, 'xmax': 3, 'ymax': 4},\n",
        "                 border_pixels='half'):\n",
        "        '''\n",
        "        Arguments:\n",
        "            check_overlap (bool, optional): Whether or not to enforce the overlap requirements defined by\n",
        "                `overlap_criterion` and `overlap_bounds`. Sometimes you might want to use the box filter only\n",
        "                to enforce a certain minimum area for all boxes (see next argument), in such cases you can\n",
        "                turn the overlap requirements off.\n",
        "            check_min_area (bool, optional): Whether or not to enforce the minimum area requirement defined\n",
        "                by `min_area`. If `True`, any boxes that have an area (in pixels) that is smaller than `min_area`\n",
        "                will be removed from the labels of an image. Bounding boxes below a certain area aren't useful\n",
        "                training examples. An object that takes up only, say, 5 pixels in an image is probably not\n",
        "                recognizable anymore, neither for a human, nor for an object detection model. It makes sense\n",
        "                to remove such boxes.\n",
        "            check_degenerate (bool, optional): Whether or not to check for and remove degenerate bounding boxes.\n",
        "                Degenerate bounding boxes are boxes that have `xmax <= xmin` and/or `ymax <= ymin`. In particular,\n",
        "                boxes with a width and/or height of zero are degenerate. It is obviously important to filter out\n",
        "                such boxes, so you should only set this option to `False` if you are certain that degenerate\n",
        "                boxes are not possible in your data and processing chain.\n",
        "            overlap_criterion (str, optional): Can be either of 'center_point', 'iou', or 'area'. Determines\n",
        "                which boxes are considered valid with respect to a given image. If set to 'center_point',\n",
        "                a given bounding box is considered valid if its center point lies within the image.\n",
        "                If set to 'area', a given bounding box is considered valid if the quotient of its intersection\n",
        "                area with the image and its own area is within the given `overlap_bounds`. If set to 'iou', a given\n",
        "                bounding box is considered valid if its IoU with the image is within the given `overlap_bounds`.\n",
        "            overlap_bounds (list or BoundGenerator, optional): Only relevant if `overlap_criterion` is 'area' or 'iou'.\n",
        "                Determines the lower and upper bounds for `overlap_criterion`. Can be either a 2-tuple of scalars\n",
        "                representing a lower bound and an upper bound, or a `BoundGenerator` object, which provides\n",
        "                the possibility to generate bounds randomly.\n",
        "            min_area (int, optional): Only relevant if `check_min_area` is `True`. Defines the minimum area in\n",
        "                pixels that a bounding box must have in order to be valid. Boxes with an area smaller than this\n",
        "                will be removed.\n",
        "            labels_format (dict, optional): A dictionary that defines which index in the last axis of the labels\n",
        "                of an image contains which bounding box coordinate. The dictionary maps at least the keywords\n",
        "                'xmin', 'ymin', 'xmax', and 'ymax' to their respective indices within last axis of the labels array.\n",
        "            border_pixels (str, optional): How to treat the border pixels of the bounding boxes.\n",
        "                Can be 'include', 'exclude', or 'half'. If 'include', the border pixels belong\n",
        "                to the boxes. If 'exclude', the border pixels do not belong to the boxes.\n",
        "                If 'half', then one of each of the two horizontal and vertical borders belong\n",
        "                to the boxex, but not the other.\n",
        "        '''\n",
        "        if not isinstance(overlap_bounds, (list, tuple, BoundGenerator)):\n",
        "            raise ValueError(\"`overlap_bounds` must be either a 2-tuple of scalars or a `BoundGenerator` object.\")\n",
        "        if isinstance(overlap_bounds, (list, tuple)) and (overlap_bounds[0] > overlap_bounds[1]):\n",
        "            raise ValueError(\"The lower bound must not be greater than the upper bound.\")\n",
        "        if not (overlap_criterion in {'iou', 'area', 'center_point'}):\n",
        "            raise ValueError(\"`overlap_criterion` must be one of 'iou', 'area', or 'center_point'.\")\n",
        "        self.overlap_criterion = overlap_criterion\n",
        "        self.overlap_bounds = overlap_bounds\n",
        "        self.min_area = min_area\n",
        "        self.check_overlap = check_overlap\n",
        "        self.check_min_area = check_min_area\n",
        "        self.check_degenerate = check_degenerate\n",
        "        self.labels_format = labels_format\n",
        "        self.border_pixels = border_pixels\n",
        "\n",
        "    def __call__(self,\n",
        "                 labels,\n",
        "                 image_height=None,\n",
        "                 image_width=None):\n",
        "        '''\n",
        "        Arguments:\n",
        "            labels (array): The labels to be filtered. This is an array with shape `(m,n)`, where\n",
        "                `m` is the number of bounding boxes and `n` is the number of elements that defines\n",
        "                each bounding box (box coordinates, class ID, etc.). The box coordinates are expected\n",
        "                to be in the image's coordinate system.\n",
        "            image_height (int): Only relevant if `check_overlap == True`. The height of the image\n",
        "                (in pixels) to compare the box coordinates to.\n",
        "            image_width (int): `check_overlap == True`. The width of the image (in pixels) to compare\n",
        "                the box coordinates to.\n",
        "        Returns:\n",
        "            An array containing the labels of all boxes that are valid.\n",
        "        '''\n",
        "\n",
        "        labels = np.copy(labels)\n",
        "\n",
        "        xmin = self.labels_format['xmin']\n",
        "        ymin = self.labels_format['ymin']\n",
        "        xmax = self.labels_format['xmax']\n",
        "        ymax = self.labels_format['ymax']\n",
        "\n",
        "        # Record the boxes that pass all checks here.\n",
        "        requirements_met = np.ones(shape=labels.shape[0], dtype=np.bool)\n",
        "\n",
        "        if self.check_degenerate:\n",
        "\n",
        "            non_degenerate = (labels[:,xmax] > labels[:,xmin]) * (labels[:,ymax] > labels[:,ymin])\n",
        "            requirements_met *= non_degenerate\n",
        "\n",
        "        if self.check_min_area:\n",
        "\n",
        "            min_area_met = (labels[:,xmax] - labels[:,xmin]) * (labels[:,ymax] - labels[:,ymin]) >= self.min_area\n",
        "            requirements_met *= min_area_met\n",
        "\n",
        "        if self.check_overlap:\n",
        "\n",
        "            # Get the lower and upper bounds.\n",
        "            if isinstance(self.overlap_bounds, BoundGenerator):\n",
        "                lower, upper = self.overlap_bounds()\n",
        "            else:\n",
        "                lower, upper = self.overlap_bounds\n",
        "\n",
        "            # Compute which boxes are valid.\n",
        "\n",
        "            if self.overlap_criterion == 'iou':\n",
        "                # Compute the patch coordinates.\n",
        "                image_coords = np.array([0, 0, image_width, image_height])\n",
        "                # Compute the IoU between the patch and all of the ground truth boxes.\n",
        "                image_boxes_iou = iou(image_coords, labels[:, [xmin, ymin, xmax, ymax]], coords='corners', mode='element-wise', border_pixels=self.border_pixels)\n",
        "                requirements_met *= (image_boxes_iou > lower) * (image_boxes_iou <= upper)\n",
        "\n",
        "            elif self.overlap_criterion == 'area':\n",
        "                if self.border_pixels == 'half':\n",
        "                    d = 0\n",
        "                elif self.border_pixels == 'include':\n",
        "                    d = 1 # If border pixels are supposed to belong to the bounding boxes, we have to add one pixel to any difference `xmax - xmin` or `ymax - ymin`.\n",
        "                elif self.border_pixels == 'exclude':\n",
        "                    d = -1 # If border pixels are not supposed to belong to the bounding boxes, we have to subtract one pixel from any difference `xmax - xmin` or `ymax - ymin`.\n",
        "                # Compute the areas of the boxes.\n",
        "                box_areas = (labels[:,xmax] - labels[:,xmin] + d) * (labels[:,ymax] - labels[:,ymin] + d)\n",
        "                # Compute the intersection area between the patch and all of the ground truth boxes.\n",
        "                clipped_boxes = np.copy(labels)\n",
        "                clipped_boxes[:,[ymin,ymax]] = np.clip(labels[:,[ymin,ymax]], a_min=0, a_max=image_height-1)\n",
        "                clipped_boxes[:,[xmin,xmax]] = np.clip(labels[:,[xmin,xmax]], a_min=0, a_max=image_width-1)\n",
        "                intersection_areas = (clipped_boxes[:,xmax] - clipped_boxes[:,xmin] + d) * (clipped_boxes[:,ymax] - clipped_boxes[:,ymin] + d) # +1 because the border pixels belong to the box areas.\n",
        "                # Check which boxes meet the overlap requirements.\n",
        "                if lower == 0.0:\n",
        "                    mask_lower = intersection_areas > lower * box_areas # If `self.lower == 0`, we want to make sure that boxes with area 0 don't count, hence the \">\" sign instead of the \">=\" sign.\n",
        "                else:\n",
        "                    mask_lower = intersection_areas >= lower * box_areas # Especially for the case `self.lower == 1` we want the \">=\" sign, otherwise no boxes would count at all.\n",
        "                mask_upper = intersection_areas <= upper * box_areas\n",
        "                requirements_met *= mask_lower * mask_upper\n",
        "\n",
        "            elif self.overlap_criterion == 'center_point':\n",
        "                # Compute the center points of the boxes.\n",
        "                cy = (labels[:,ymin] + labels[:,ymax]) / 2\n",
        "                cx = (labels[:,xmin] + labels[:,xmax]) / 2\n",
        "                # Check which of the boxes have center points within the cropped patch remove those that don't.\n",
        "                requirements_met *= (cy >= 0.0) * (cy <= image_height-1) * (cx >= 0.0) * (cx <= image_width-1)\n",
        "\n",
        "        return labels[requirements_met]\n",
        "\n",
        "class ImageValidator:\n",
        "    '''\n",
        "    Returns `True` if a given minimum number of bounding boxes meets given overlap\n",
        "    requirements with an image of a given height and width.\n",
        "    '''\n",
        "\n",
        "    def __init__(self,\n",
        "                 overlap_criterion='center_point',\n",
        "                 bounds=(0.3, 1.0),\n",
        "                 n_boxes_min=1,\n",
        "                 labels_format={'class_id': 0, 'xmin': 1, 'ymin': 2, 'xmax': 3, 'ymax': 4},\n",
        "                 border_pixels='half'):\n",
        "        '''\n",
        "        Arguments:\n",
        "            overlap_criterion (str, optional): Can be either of 'center_point', 'iou', or 'area'. Determines\n",
        "                which boxes are considered valid with respect to a given image. If set to 'center_point',\n",
        "                a given bounding box is considered valid if its center point lies within the image.\n",
        "                If set to 'area', a given bounding box is considered valid if the quotient of its intersection\n",
        "                area with the image and its own area is within `lower` and `upper`. If set to 'iou', a given\n",
        "                bounding box is considered valid if its IoU with the image is within `lower` and `upper`.\n",
        "            bounds (list or BoundGenerator, optional): Only relevant if `overlap_criterion` is 'area' or 'iou'.\n",
        "                Determines the lower and upper bounds for `overlap_criterion`. Can be either a 2-tuple of scalars\n",
        "                representing a lower bound and an upper bound, or a `BoundGenerator` object, which provides\n",
        "                the possibility to generate bounds randomly.\n",
        "            n_boxes_min (int or str, optional): Either a non-negative integer or the string 'all'.\n",
        "                Determines the minimum number of boxes that must meet the `overlap_criterion` with respect to\n",
        "                an image of the given height and width in order for the image to be a valid image.\n",
        "                If set to 'all', an image is considered valid if all given boxes meet the `overlap_criterion`.\n",
        "            labels_format (dict, optional): A dictionary that defines which index in the last axis of the labels\n",
        "                of an image contains which bounding box coordinate. The dictionary maps at least the keywords\n",
        "                'xmin', 'ymin', 'xmax', and 'ymax' to their respective indices within last axis of the labels array.\n",
        "            border_pixels (str, optional): How to treat the border pixels of the bounding boxes.\n",
        "                Can be 'include', 'exclude', or 'half'. If 'include', the border pixels belong\n",
        "                to the boxes. If 'exclude', the border pixels do not belong to the boxes.\n",
        "                If 'half', then one of each of the two horizontal and vertical borders belong\n",
        "                to the boxex, but not the other.\n",
        "        '''\n",
        "        if not ((isinstance(n_boxes_min, int) and n_boxes_min > 0) or n_boxes_min == 'all'):\n",
        "            raise ValueError(\"`n_boxes_min` must be a positive integer or 'all'.\")\n",
        "        self.overlap_criterion = overlap_criterion\n",
        "        self.bounds = bounds\n",
        "        self.n_boxes_min = n_boxes_min\n",
        "        self.labels_format = labels_format\n",
        "        self.border_pixels = border_pixels\n",
        "        self.box_filter = BoxFilter(check_overlap=True,\n",
        "                                    check_min_area=False,\n",
        "                                    check_degenerate=False,\n",
        "                                    overlap_criterion=self.overlap_criterion,\n",
        "                                    overlap_bounds=self.bounds,\n",
        "                                    labels_format=self.labels_format,\n",
        "                                    border_pixels=self.border_pixels)\n",
        "\n",
        "    def __call__(self,\n",
        "                 labels,\n",
        "                 image_height,\n",
        "                 image_width):\n",
        "        '''\n",
        "        Arguments:\n",
        "            labels (array): The labels to be tested. The box coordinates are expected\n",
        "                to be in the image's coordinate system.\n",
        "            image_height (int): The height of the image to compare the box coordinates to.\n",
        "            image_width (int): The width of the image to compare the box coordinates to.\n",
        "        Returns:\n",
        "            A boolean indicating whether an imgae of the given height and width is\n",
        "            valid with respect to the given bounding boxes.\n",
        "        '''\n",
        "\n",
        "        self.box_filter.overlap_bounds = self.bounds\n",
        "        self.box_filter.labels_format = self.labels_format\n",
        "\n",
        "        # Get all boxes that meet the overlap requirements.\n",
        "        valid_labels = self.box_filter(labels=labels,\n",
        "                                       image_height=image_height,\n",
        "                                       image_width=image_width)\n",
        "\n",
        "        # Check whether enough boxes meet the requirements.\n",
        "        if isinstance(self.n_boxes_min, int):\n",
        "            # The image is valid if at least `self.n_boxes_min` ground truth boxes meet the requirements.\n",
        "            if len(valid_labels) >= self.n_boxes_min:\n",
        "                return True\n",
        "            else:\n",
        "                return False\n",
        "        elif self.n_boxes_min == 'all':\n",
        "            # The image is valid if all ground truth boxes meet the requirements.\n",
        "            if len(valid_labels) == len(labels):\n",
        "                return True\n",
        "            else:\n",
        "                return False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jhg5DWTZEexy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Various geometric image transformations for 2D object detection, both deterministic\n",
        "and probabilistic.\n",
        "Copyright (C) 2018 Pierluigi Ferrari\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "   http://www.apache.org/licenses/LICENSE-2.0\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "'''\n",
        "\n",
        "from __future__ import division\n",
        "import numpy as np\n",
        "import cv2\n",
        "import random\n",
        "\n",
        "#from data_generator.object_detection_2d_image_boxes_validation_utils import BoxFilter, ImageValidator\n",
        "\n",
        "class Resize:\n",
        "    '''\n",
        "    Resizes images to a specified height and width in pixels.\n",
        "    '''\n",
        "\n",
        "    def __init__(self,\n",
        "                 height,\n",
        "                 width,\n",
        "                 interpolation_mode=cv2.INTER_LINEAR,\n",
        "                 box_filter=None,\n",
        "                 labels_format={'class_id': 0, 'xmin': 1, 'ymin': 2, 'xmax': 3, 'ymax': 4}):\n",
        "        '''\n",
        "        Arguments:\n",
        "            height (int): The desired height of the output images in pixels.\n",
        "            width (int): The desired width of the output images in pixels.\n",
        "            interpolation_mode (int, optional): An integer that denotes a valid\n",
        "                OpenCV interpolation mode. For example, integers 0 through 5 are\n",
        "                valid interpolation modes.\n",
        "            box_filter (BoxFilter, optional): Only relevant if ground truth bounding boxes are given.\n",
        "                A `BoxFilter` object to filter out bounding boxes that don't meet the given criteria\n",
        "                after the transformation. Refer to the `BoxFilter` documentation for details. If `None`,\n",
        "                the validity of the bounding boxes is not checked.\n",
        "            labels_format (dict, optional): A dictionary that defines which index in the last axis of the labels\n",
        "                of an image contains which bounding box coordinate. The dictionary maps at least the keywords\n",
        "                'xmin', 'ymin', 'xmax', and 'ymax' to their respective indices within last axis of the labels array.\n",
        "        '''\n",
        "        if not (isinstance(box_filter, BoxFilter) or box_filter is None):\n",
        "            raise ValueError(\"`box_filter` must be either `None` or a `BoxFilter` object.\")\n",
        "        self.out_height = height\n",
        "        self.out_width = width\n",
        "        self.interpolation_mode = interpolation_mode\n",
        "        self.box_filter = box_filter\n",
        "        self.labels_format = labels_format\n",
        "\n",
        "    def __call__(self, image, labels=None, return_inverter=False):\n",
        "\n",
        "        img_height, img_width = image.shape[:2]\n",
        "\n",
        "        xmin = self.labels_format['xmin']\n",
        "        ymin = self.labels_format['ymin']\n",
        "        xmax = self.labels_format['xmax']\n",
        "        ymax = self.labels_format['ymax']\n",
        "\n",
        "        image = cv2.resize(image,\n",
        "                           dsize=(self.out_width, self.out_height),\n",
        "                           interpolation=self.interpolation_mode)\n",
        "\n",
        "        if return_inverter:\n",
        "            def inverter(labels):\n",
        "                labels = np.copy(labels)\n",
        "                labels[:, [ymin+1, ymax+1]] = np.round(labels[:, [ymin+1, ymax+1]] * (img_height / self.out_height), decimals=0)\n",
        "                labels[:, [xmin+1, xmax+1]] = np.round(labels[:, [xmin+1, xmax+1]] * (img_width / self.out_width), decimals=0)\n",
        "                return labels\n",
        "\n",
        "        if labels is None:\n",
        "            if return_inverter:\n",
        "                return image, inverter\n",
        "            else:\n",
        "                return image\n",
        "        else:\n",
        "            labels = np.copy(labels)\n",
        "            labels[:, [ymin, ymax]] = np.round(labels[:, [ymin, ymax]] * (self.out_height / img_height), decimals=0)\n",
        "            labels[:, [xmin, xmax]] = np.round(labels[:, [xmin, xmax]] * (self.out_width / img_width), decimals=0)\n",
        "\n",
        "            if not (self.box_filter is None):\n",
        "                self.box_filter.labels_format = self.labels_format\n",
        "                labels = self.box_filter(labels=labels,\n",
        "                                         image_height=self.out_height,\n",
        "                                         image_width=self.out_width)\n",
        "\n",
        "            if return_inverter:\n",
        "                return image, labels, inverter\n",
        "            else:\n",
        "                return image, labels\n",
        "\n",
        "class ResizeRandomInterp:\n",
        "    '''\n",
        "    Resizes images to a specified height and width in pixels using a radnomly\n",
        "    selected interpolation mode.\n",
        "    '''\n",
        "\n",
        "    def __init__(self,\n",
        "                 height,\n",
        "                 width,\n",
        "                 interpolation_modes=[cv2.INTER_NEAREST,\n",
        "                                      cv2.INTER_LINEAR,\n",
        "                                      cv2.INTER_CUBIC,\n",
        "                                      cv2.INTER_AREA,\n",
        "                                      cv2.INTER_LANCZOS4],\n",
        "                 box_filter=None,\n",
        "                 labels_format={'class_id': 0, 'xmin': 1, 'ymin': 2, 'xmax': 3, 'ymax': 4}):\n",
        "        '''\n",
        "        Arguments:\n",
        "            height (int): The desired height of the output image in pixels.\n",
        "            width (int): The desired width of the output image in pixels.\n",
        "            interpolation_modes (list/tuple, optional): A list/tuple of integers\n",
        "                that represent valid OpenCV interpolation modes. For example,\n",
        "                integers 0 through 5 are valid interpolation modes.\n",
        "            box_filter (BoxFilter, optional): Only relevant if ground truth bounding boxes are given.\n",
        "                A `BoxFilter` object to filter out bounding boxes that don't meet the given criteria\n",
        "                after the transformation. Refer to the `BoxFilter` documentation for details. If `None`,\n",
        "                the validity of the bounding boxes is not checked.\n",
        "            labels_format (dict, optional): A dictionary that defines which index in the last axis of the labels\n",
        "                of an image contains which bounding box coordinate. The dictionary maps at least the keywords\n",
        "                'xmin', 'ymin', 'xmax', and 'ymax' to their respective indices within last axis of the labels array.\n",
        "        '''\n",
        "        if not (isinstance(interpolation_modes, (list, tuple))):\n",
        "            raise ValueError(\"`interpolation_mode` must be a list or tuple.\")\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.interpolation_modes = interpolation_modes\n",
        "        self.box_filter = box_filter\n",
        "        self.labels_format = labels_format\n",
        "        self.resize = Resize(height=self.height,\n",
        "                             width=self.width,\n",
        "                             box_filter=self.box_filter,\n",
        "                             labels_format=self.labels_format)\n",
        "\n",
        "    def __call__(self, image, labels=None, return_inverter=False):\n",
        "        self.resize.interpolation_mode = np.random.choice(self.interpolation_modes)\n",
        "        self.resize.labels_format = self.labels_format\n",
        "        return self.resize(image, labels, return_inverter)\n",
        "\n",
        "class Flip:\n",
        "    '''\n",
        "    Flips images horizontally or vertically.\n",
        "    '''\n",
        "    def __init__(self,\n",
        "                 dim='horizontal',\n",
        "                 labels_format={'class_id': 0, 'xmin': 1, 'ymin': 2, 'xmax': 3, 'ymax': 4}):\n",
        "        '''\n",
        "        Arguments:\n",
        "            dim (str, optional): Can be either of 'horizontal' and 'vertical'.\n",
        "                If 'horizontal', images will be flipped horizontally, i.e. along\n",
        "                the vertical axis. If 'horizontal', images will be flipped vertically,\n",
        "                i.e. along the horizontal axis.\n",
        "            labels_format (dict, optional): A dictionary that defines which index in the last axis of the labels\n",
        "                of an image contains which bounding box coordinate. The dictionary maps at least the keywords\n",
        "                'xmin', 'ymin', 'xmax', and 'ymax' to their respective indices within last axis of the labels array.\n",
        "        '''\n",
        "        if not (dim in {'horizontal', 'vertical'}): raise ValueError(\"`dim` can be one of 'horizontal' and 'vertical'.\")\n",
        "        self.dim = dim\n",
        "        self.labels_format = labels_format\n",
        "\n",
        "    def __call__(self, image, labels=None, return_inverter=False):\n",
        "\n",
        "        img_height, img_width = image.shape[:2]\n",
        "\n",
        "        xmin = self.labels_format['xmin']\n",
        "        ymin = self.labels_format['ymin']\n",
        "        xmax = self.labels_format['xmax']\n",
        "        ymax = self.labels_format['ymax']\n",
        "\n",
        "        if self.dim == 'horizontal':\n",
        "            image = image[:,::-1]\n",
        "            if labels is None:\n",
        "                return image\n",
        "            else:\n",
        "                labels = np.copy(labels)\n",
        "                labels[:, [xmin, xmax]] = img_width - labels[:, [xmax, xmin]]\n",
        "                return image, labels\n",
        "        else:\n",
        "            image = image[::-1]\n",
        "            if labels is None:\n",
        "                return image\n",
        "            else:\n",
        "                labels = np.copy(labels)\n",
        "                labels[:, [ymin, ymax]] = img_height - labels[:, [ymax, ymin]]\n",
        "                return image, labels\n",
        "\n",
        "class RandomFlip:\n",
        "    '''\n",
        "    Randomly flips images horizontally or vertically. The randomness only refers\n",
        "    to whether or not the image will be flipped.\n",
        "    '''\n",
        "    def __init__(self,\n",
        "                 dim='horizontal',\n",
        "                 prob=0.5,\n",
        "                 labels_format={'class_id': 0, 'xmin': 1, 'ymin': 2, 'xmax': 3, 'ymax': 4}):\n",
        "        '''\n",
        "        Arguments:\n",
        "            dim (str, optional): Can be either of 'horizontal' and 'vertical'.\n",
        "                If 'horizontal', images will be flipped horizontally, i.e. along\n",
        "                the vertical axis. If 'horizontal', images will be flipped vertically,\n",
        "                i.e. along the horizontal axis.\n",
        "            prob (float, optional): `(1 - prob)` determines the probability with which the original,\n",
        "                unaltered image is returned.\n",
        "            labels_format (dict, optional): A dictionary that defines which index in the last axis of the labels\n",
        "                of an image contains which bounding box coordinate. The dictionary maps at least the keywords\n",
        "                'xmin', 'ymin', 'xmax', and 'ymax' to their respective indices within last axis of the labels array.\n",
        "        '''\n",
        "        self.dim = dim\n",
        "        self.prob = prob\n",
        "        self.labels_format = labels_format\n",
        "        self.flip = Flip(dim=self.dim, labels_format=self.labels_format)\n",
        "\n",
        "    def __call__(self, image, labels=None):\n",
        "        p = np.random.uniform(0,1)\n",
        "        if p >= (1.0-self.prob):\n",
        "            self.flip.labels_format = self.labels_format\n",
        "            return self.flip(image, labels)\n",
        "        elif labels is None:\n",
        "            return image\n",
        "        else:\n",
        "            return image, labels\n",
        "\n",
        "class Translate:\n",
        "    '''\n",
        "    Translates images horizontally and/or vertically.\n",
        "    '''\n",
        "\n",
        "    def __init__(self,\n",
        "                 dy,\n",
        "                 dx,\n",
        "                 clip_boxes=True,\n",
        "                 box_filter=None,\n",
        "                 background=(0,0,0),\n",
        "                 labels_format={'class_id': 0, 'xmin': 1, 'ymin': 2, 'xmax': 3, 'ymax': 4}):\n",
        "        '''\n",
        "        Arguments:\n",
        "            dy (float): The fraction of the image height by which to translate images along the\n",
        "                vertical axis. Positive values translate images downwards, negative values\n",
        "                translate images upwards.\n",
        "            dx (float): The fraction of the image width by which to translate images along the\n",
        "                horizontal axis. Positive values translate images to the right, negative values\n",
        "                translate images to the left.\n",
        "            clip_boxes (bool, optional): Only relevant if ground truth bounding boxes are given.\n",
        "                If `True`, any ground truth bounding boxes will be clipped to lie entirely within the\n",
        "                image after the translation.\n",
        "            box_filter (BoxFilter, optional): Only relevant if ground truth bounding boxes are given.\n",
        "                A `BoxFilter` object to filter out bounding boxes that don't meet the given criteria\n",
        "                after the transformation. Refer to the `BoxFilter` documentation for details. If `None`,\n",
        "                the validity of the bounding boxes is not checked.\n",
        "            background (list/tuple, optional): A 3-tuple specifying the RGB color value of the\n",
        "                background pixels of the translated images.\n",
        "            labels_format (dict, optional): A dictionary that defines which index in the last axis of the labels\n",
        "                of an image contains which bounding box coordinate. The dictionary maps at least the keywords\n",
        "                'xmin', 'ymin', 'xmax', and 'ymax' to their respective indices within last axis of the labels array.\n",
        "        '''\n",
        "\n",
        "        if not (isinstance(box_filter, BoxFilter) or box_filter is None):\n",
        "            raise ValueError(\"`box_filter` must be either `None` or a `BoxFilter` object.\")\n",
        "        self.dy_rel = dy\n",
        "        self.dx_rel = dx\n",
        "        self.clip_boxes = clip_boxes\n",
        "        self.box_filter = box_filter\n",
        "        self.background = background\n",
        "        self.labels_format = labels_format\n",
        "\n",
        "    def __call__(self, image, labels=None):\n",
        "\n",
        "        img_height, img_width = image.shape[:2]\n",
        "\n",
        "        # Compute the translation matrix.\n",
        "        dy_abs = int(round(img_height * self.dy_rel))\n",
        "        dx_abs = int(round(img_width * self.dx_rel))\n",
        "        M = np.float32([[1, 0, dx_abs],\n",
        "                        [0, 1, dy_abs]])\n",
        "\n",
        "        # Translate the image.\n",
        "        image = cv2.warpAffine(image,\n",
        "                               M=M,\n",
        "                               dsize=(img_width, img_height),\n",
        "                               borderMode=cv2.BORDER_CONSTANT,\n",
        "                               borderValue=self.background)\n",
        "\n",
        "        if labels is None:\n",
        "            return image\n",
        "        else:\n",
        "            xmin = self.labels_format['xmin']\n",
        "            ymin = self.labels_format['ymin']\n",
        "            xmax = self.labels_format['xmax']\n",
        "            ymax = self.labels_format['ymax']\n",
        "\n",
        "            labels = np.copy(labels)\n",
        "            # Translate the box coordinates to the translated image's coordinate system.\n",
        "            labels[:,[xmin,xmax]] += dx_abs\n",
        "            labels[:,[ymin,ymax]] += dy_abs\n",
        "\n",
        "            # Compute all valid boxes for this patch.\n",
        "            if not (self.box_filter is None):\n",
        "                self.box_filter.labels_format = self.labels_format\n",
        "                labels = self.box_filter(labels=labels,\n",
        "                                         image_height=img_height,\n",
        "                                         image_width=img_width)\n",
        "\n",
        "            if self.clip_boxes:\n",
        "                labels[:,[ymin,ymax]] = np.clip(labels[:,[ymin,ymax]], a_min=0, a_max=img_height-1)\n",
        "                labels[:,[xmin,xmax]] = np.clip(labels[:,[xmin,xmax]], a_min=0, a_max=img_width-1)\n",
        "\n",
        "            return image, labels\n",
        "\n",
        "class RandomTranslate:\n",
        "    '''\n",
        "    Randomly translates images horizontally and/or vertically.\n",
        "    '''\n",
        "\n",
        "    def __init__(self,\n",
        "                 dy_minmax=(0.03,0.3),\n",
        "                 dx_minmax=(0.03,0.3),\n",
        "                 prob=0.5,\n",
        "                 clip_boxes=True,\n",
        "                 box_filter=None,\n",
        "                 image_validator=None,\n",
        "                 n_trials_max=3,\n",
        "                 background=(0,0,0),\n",
        "                 labels_format={'class_id': 0, 'xmin': 1, 'ymin': 2, 'xmax': 3, 'ymax': 4}):\n",
        "        '''\n",
        "        Arguments:\n",
        "            dy_minmax (list/tuple, optional): A 2-tuple `(min, max)` of non-negative floats that\n",
        "                determines the minimum and maximum relative translation of images along the vertical\n",
        "                axis both upward and downward. That is, images will be randomly translated by at least\n",
        "                `min` and at most `max` either upward or downward. For example, if `dy_minmax == (0.05,0.3)`,\n",
        "                an image of size `(100,100)` will be translated by at least 5 and at most 30 pixels\n",
        "                either upward or downward. The translation direction is chosen randomly.\n",
        "            dx_minmax (list/tuple, optional): A 2-tuple `(min, max)` of non-negative floats that\n",
        "                determines the minimum and maximum relative translation of images along the horizontal\n",
        "                axis both to the left and right. That is, images will be randomly translated by at least\n",
        "                `min` and at most `max` either left or right. For example, if `dx_minmax == (0.05,0.3)`,\n",
        "                an image of size `(100,100)` will be translated by at least 5 and at most 30 pixels\n",
        "                either left or right. The translation direction is chosen randomly.\n",
        "            prob (float, optional): `(1 - prob)` determines the probability with which the original,\n",
        "                unaltered image is returned.\n",
        "            clip_boxes (bool, optional): Only relevant if ground truth bounding boxes are given.\n",
        "                If `True`, any ground truth bounding boxes will be clipped to lie entirely within the\n",
        "                image after the translation.\n",
        "            box_filter (BoxFilter, optional): Only relevant if ground truth bounding boxes are given.\n",
        "                A `BoxFilter` object to filter out bounding boxes that don't meet the given criteria\n",
        "                after the transformation. Refer to the `BoxFilter` documentation for details. If `None`,\n",
        "                the validity of the bounding boxes is not checked.\n",
        "            image_validator (ImageValidator, optional): Only relevant if ground truth bounding boxes are given.\n",
        "                An `ImageValidator` object to determine whether a translated image is valid. If `None`,\n",
        "                any outcome is valid.\n",
        "            n_trials_max (int, optional): Only relevant if ground truth bounding boxes are given.\n",
        "                Determines the maxmial number of trials to produce a valid image. If no valid image could\n",
        "                be produced in `n_trials_max` trials, returns the unaltered input image.\n",
        "            background (list/tuple, optional): A 3-tuple specifying the RGB color value of the\n",
        "                background pixels of the translated images.\n",
        "            labels_format (dict, optional): A dictionary that defines which index in the last axis of the labels\n",
        "                of an image contains which bounding box coordinate. The dictionary maps at least the keywords\n",
        "                'xmin', 'ymin', 'xmax', and 'ymax' to their respective indices within last axis of the labels array.\n",
        "        '''\n",
        "        if dy_minmax[0] > dy_minmax[1]:\n",
        "            raise ValueError(\"It must be `dy_minmax[0] <= dy_minmax[1]`.\")\n",
        "        if dx_minmax[0] > dx_minmax[1]:\n",
        "            raise ValueError(\"It must be `dx_minmax[0] <= dx_minmax[1]`.\")\n",
        "        if dy_minmax[0] < 0 or dx_minmax[0] < 0:\n",
        "            raise ValueError(\"It must be `dy_minmax[0] >= 0` and `dx_minmax[0] >= 0`.\")\n",
        "        if not (isinstance(image_validator, ImageValidator) or image_validator is None):\n",
        "            raise ValueError(\"`image_validator` must be either `None` or an `ImageValidator` object.\")\n",
        "        self.dy_minmax = dy_minmax\n",
        "        self.dx_minmax = dx_minmax\n",
        "        self.prob = prob\n",
        "        self.clip_boxes = clip_boxes\n",
        "        self.box_filter = box_filter\n",
        "        self.image_validator = image_validator\n",
        "        self.n_trials_max = n_trials_max\n",
        "        self.background = background\n",
        "        self.labels_format = labels_format\n",
        "        self.translate = Translate(dy=0,\n",
        "                                   dx=0,\n",
        "                                   clip_boxes=self.clip_boxes,\n",
        "                                   box_filter=self.box_filter,\n",
        "                                   background=self.background,\n",
        "                                   labels_format=self.labels_format)\n",
        "\n",
        "    def __call__(self, image, labels=None):\n",
        "\n",
        "        p = np.random.uniform(0,1)\n",
        "        if p >= (1.0-self.prob):\n",
        "\n",
        "            img_height, img_width = image.shape[:2]\n",
        "\n",
        "            xmin = self.labels_format['xmin']\n",
        "            ymin = self.labels_format['ymin']\n",
        "            xmax = self.labels_format['xmax']\n",
        "            ymax = self.labels_format['ymax']\n",
        "\n",
        "            # Override the preset labels format.\n",
        "            if not self.image_validator is None:\n",
        "                self.image_validator.labels_format = self.labels_format\n",
        "            self.translate.labels_format = self.labels_format\n",
        "\n",
        "            for _ in range(max(1, self.n_trials_max)):\n",
        "\n",
        "                # Pick the relative amount by which to translate.\n",
        "                dy_abs = np.random.uniform(self.dy_minmax[0], self.dy_minmax[1])\n",
        "                dx_abs = np.random.uniform(self.dx_minmax[0], self.dx_minmax[1])\n",
        "                # Pick the direction in which to translate.\n",
        "                dy = np.random.choice([-dy_abs, dy_abs])\n",
        "                dx = np.random.choice([-dx_abs, dx_abs])\n",
        "                self.translate.dy_rel = dy\n",
        "                self.translate.dx_rel = dx\n",
        "\n",
        "                if (labels is None) or (self.image_validator is None):\n",
        "                    # We either don't have any boxes or if we do, we will accept any outcome as valid.\n",
        "                    return self.translate(image, labels)\n",
        "                else:\n",
        "                    # Translate the box coordinates to the translated image's coordinate system.\n",
        "                    new_labels = np.copy(labels)\n",
        "                    new_labels[:, [ymin, ymax]] += int(round(img_height * dy))\n",
        "                    new_labels[:, [xmin, xmax]] += int(round(img_width * dx))\n",
        "\n",
        "                    # Check if the patch is valid.\n",
        "                    if self.image_validator(labels=new_labels,\n",
        "                                            image_height=img_height,\n",
        "                                            image_width=img_width):\n",
        "                        return self.translate(image, labels)\n",
        "\n",
        "            # If all attempts failed, return the unaltered input image.\n",
        "            if labels is None:\n",
        "                return image\n",
        "\n",
        "            else:\n",
        "                return image, labels\n",
        "\n",
        "        elif labels is None:\n",
        "            return image\n",
        "\n",
        "        else:\n",
        "            return image, labels\n",
        "\n",
        "class Scale:\n",
        "    '''\n",
        "    Scales images, i.e. zooms in or out.\n",
        "    '''\n",
        "\n",
        "    def __init__(self,\n",
        "                 factor,\n",
        "                 clip_boxes=True,\n",
        "                 box_filter=None,\n",
        "                 background=(0,0,0),\n",
        "                 labels_format={'class_id': 0, 'xmin': 1, 'ymin': 2, 'xmax': 3, 'ymax': 4}):\n",
        "        '''\n",
        "        Arguments:\n",
        "            factor (float): The fraction of the image size by which to scale images. Must be positive.\n",
        "            clip_boxes (bool, optional): Only relevant if ground truth bounding boxes are given.\n",
        "                If `True`, any ground truth bounding boxes will be clipped to lie entirely within the\n",
        "                image after the translation.\n",
        "            box_filter (BoxFilter, optional): Only relevant if ground truth bounding boxes are given.\n",
        "                A `BoxFilter` object to filter out bounding boxes that don't meet the given criteria\n",
        "                after the transformation. Refer to the `BoxFilter` documentation for details. If `None`,\n",
        "                the validity of the bounding boxes is not checked.\n",
        "            background (list/tuple, optional): A 3-tuple specifying the RGB color value of the potential\n",
        "                background pixels of the scaled images.\n",
        "            labels_format (dict, optional): A dictionary that defines which index in the last axis of the labels\n",
        "                of an image contains which bounding box coordinate. The dictionary maps at least the keywords\n",
        "                'xmin', 'ymin', 'xmax', and 'ymax' to their respective indices within last axis of the labels array.\n",
        "        '''\n",
        "\n",
        "        if factor <= 0:\n",
        "            raise ValueError(\"It must be `factor > 0`.\")\n",
        "        if not (isinstance(box_filter, BoxFilter) or box_filter is None):\n",
        "            raise ValueError(\"`box_filter` must be either `None` or a `BoxFilter` object.\")\n",
        "        self.factor = factor\n",
        "        self.clip_boxes = clip_boxes\n",
        "        self.box_filter = box_filter\n",
        "        self.background = background\n",
        "        self.labels_format = labels_format\n",
        "\n",
        "    def __call__(self, image, labels=None):\n",
        "\n",
        "        img_height, img_width = image.shape[:2]\n",
        "\n",
        "        # Compute the rotation matrix.\n",
        "        M = cv2.getRotationMatrix2D(center=(img_width / 2, img_height / 2),\n",
        "                                    angle=0,\n",
        "                                    scale=self.factor)\n",
        "\n",
        "        # Scale the image.\n",
        "        image = cv2.warpAffine(image,\n",
        "                               M=M,\n",
        "                               dsize=(img_width, img_height),\n",
        "                               borderMode=cv2.BORDER_CONSTANT,\n",
        "                               borderValue=self.background)\n",
        "\n",
        "        if labels is None:\n",
        "            return image\n",
        "        else:\n",
        "            xmin = self.labels_format['xmin']\n",
        "            ymin = self.labels_format['ymin']\n",
        "            xmax = self.labels_format['xmax']\n",
        "            ymax = self.labels_format['ymax']\n",
        "\n",
        "            labels = np.copy(labels)\n",
        "            # Scale the bounding boxes accordingly.\n",
        "            # Transform two opposite corner points of the rectangular boxes using the rotation matrix `M`.\n",
        "            toplefts = np.array([labels[:,xmin], labels[:,ymin], np.ones(labels.shape[0])])\n",
        "            bottomrights = np.array([labels[:,xmax], labels[:,ymax], np.ones(labels.shape[0])])\n",
        "            new_toplefts = (np.dot(M, toplefts)).T\n",
        "            new_bottomrights = (np.dot(M, bottomrights)).T\n",
        "            labels[:,[xmin,ymin]] = np.round(new_toplefts, decimals=0).astype(np.int)\n",
        "            labels[:,[xmax,ymax]] = np.round(new_bottomrights, decimals=0).astype(np.int)\n",
        "\n",
        "            # Compute all valid boxes for this patch.\n",
        "            if not (self.box_filter is None):\n",
        "                self.box_filter.labels_format = self.labels_format\n",
        "                labels = self.box_filter(labels=labels,\n",
        "                                         image_height=img_height,\n",
        "                                         image_width=img_width)\n",
        "\n",
        "            if self.clip_boxes:\n",
        "                labels[:,[ymin,ymax]] = np.clip(labels[:,[ymin,ymax]], a_min=0, a_max=img_height-1)\n",
        "                labels[:,[xmin,xmax]] = np.clip(labels[:,[xmin,xmax]], a_min=0, a_max=img_width-1)\n",
        "\n",
        "            return image, labels\n",
        "\n",
        "class RandomScale:\n",
        "    '''\n",
        "    Randomly scales images.\n",
        "    '''\n",
        "\n",
        "    def __init__(self,\n",
        "                 min_factor=0.5,\n",
        "                 max_factor=1.5,\n",
        "                 prob=0.5,\n",
        "                 clip_boxes=True,\n",
        "                 box_filter=None,\n",
        "                 image_validator=None,\n",
        "                 n_trials_max=3,\n",
        "                 background=(0,0,0),\n",
        "                 labels_format={'class_id': 0, 'xmin': 1, 'ymin': 2, 'xmax': 3, 'ymax': 4}):\n",
        "        '''\n",
        "        Arguments:\n",
        "            min_factor (float, optional): The minimum fraction of the image size by which to scale images.\n",
        "                Must be positive.\n",
        "            max_factor (float, optional): The maximum fraction of the image size by which to scale images.\n",
        "                Must be positive.\n",
        "            prob (float, optional): `(1 - prob)` determines the probability with which the original,\n",
        "                unaltered image is returned.\n",
        "            clip_boxes (bool, optional): Only relevant if ground truth bounding boxes are given.\n",
        "                If `True`, any ground truth bounding boxes will be clipped to lie entirely within the\n",
        "                image after the translation.\n",
        "            box_filter (BoxFilter, optional): Only relevant if ground truth bounding boxes are given.\n",
        "                A `BoxFilter` object to filter out bounding boxes that don't meet the given criteria\n",
        "                after the transformation. Refer to the `BoxFilter` documentation for details. If `None`,\n",
        "                the validity of the bounding boxes is not checked.\n",
        "            image_validator (ImageValidator, optional): Only relevant if ground truth bounding boxes are given.\n",
        "                An `ImageValidator` object to determine whether a scaled image is valid. If `None`,\n",
        "                any outcome is valid.\n",
        "            n_trials_max (int, optional): Only relevant if ground truth bounding boxes are given.\n",
        "                Determines the maxmial number of trials to produce a valid image. If no valid image could\n",
        "                be produced in `n_trials_max` trials, returns the unaltered input image.\n",
        "            background (list/tuple, optional): A 3-tuple specifying the RGB color value of the potential\n",
        "                background pixels of the scaled images.\n",
        "            labels_format (dict, optional): A dictionary that defines which index in the last axis of the labels\n",
        "                of an image contains which bounding box coordinate. The dictionary maps at least the keywords\n",
        "                'xmin', 'ymin', 'xmax', and 'ymax' to their respective indices within last axis of the labels array.\n",
        "        '''\n",
        "\n",
        "        if not (0 < min_factor <= max_factor):\n",
        "            raise ValueError(\"It must be `0 < min_factor <= max_factor`.\")\n",
        "        if not (isinstance(image_validator, ImageValidator) or image_validator is None):\n",
        "            raise ValueError(\"`image_validator` must be either `None` or an `ImageValidator` object.\")\n",
        "        self.min_factor = min_factor\n",
        "        self.max_factor = max_factor\n",
        "        self.prob = prob\n",
        "        self.clip_boxes = clip_boxes\n",
        "        self.box_filter = box_filter\n",
        "        self.image_validator = image_validator\n",
        "        self.n_trials_max = n_trials_max\n",
        "        self.background = background\n",
        "        self.labels_format = labels_format\n",
        "        self.scale = Scale(factor=1.0,\n",
        "                           clip_boxes=self.clip_boxes,\n",
        "                           box_filter=self.box_filter,\n",
        "                           background=self.background,\n",
        "                           labels_format=self.labels_format)\n",
        "\n",
        "    def __call__(self, image, labels=None):\n",
        "\n",
        "        p = np.random.uniform(0,1)\n",
        "        if p >= (1.0-self.prob):\n",
        "\n",
        "            img_height, img_width = image.shape[:2]\n",
        "\n",
        "            xmin = self.labels_format['xmin']\n",
        "            ymin = self.labels_format['ymin']\n",
        "            xmax = self.labels_format['xmax']\n",
        "            ymax = self.labels_format['ymax']\n",
        "\n",
        "            # Override the preset labels format.\n",
        "            if not self.image_validator is None:\n",
        "                self.image_validator.labels_format = self.labels_format\n",
        "            self.scale.labels_format = self.labels_format\n",
        "\n",
        "            for _ in range(max(1, self.n_trials_max)):\n",
        "\n",
        "                # Pick a scaling factor.\n",
        "                factor = np.random.uniform(self.min_factor, self.max_factor)\n",
        "                self.scale.factor = factor\n",
        "\n",
        "                if (labels is None) or (self.image_validator is None):\n",
        "                    # We either don't have any boxes or if we do, we will accept any outcome as valid.\n",
        "                    return self.scale(image, labels)\n",
        "                else:\n",
        "                    # Scale the bounding boxes accordingly.\n",
        "                    # Transform two opposite corner points of the rectangular boxes using the rotation matrix `M`.\n",
        "                    toplefts = np.array([labels[:,xmin], labels[:,ymin], np.ones(labels.shape[0])])\n",
        "                    bottomrights = np.array([labels[:,xmax], labels[:,ymax], np.ones(labels.shape[0])])\n",
        "\n",
        "                    # Compute the rotation matrix.\n",
        "                    M = cv2.getRotationMatrix2D(center=(img_width / 2, img_height / 2),\n",
        "                                                angle=0,\n",
        "                                                scale=factor)\n",
        "\n",
        "                    new_toplefts = (np.dot(M, toplefts)).T\n",
        "                    new_bottomrights = (np.dot(M, bottomrights)).T\n",
        "\n",
        "                    new_labels = np.copy(labels)\n",
        "                    new_labels[:,[xmin,ymin]] = np.around(new_toplefts, decimals=0).astype(np.int)\n",
        "                    new_labels[:,[xmax,ymax]] = np.around(new_bottomrights, decimals=0).astype(np.int)\n",
        "\n",
        "                    # Check if the patch is valid.\n",
        "                    if self.image_validator(labels=new_labels,\n",
        "                                            image_height=img_height,\n",
        "                                            image_width=img_width):\n",
        "                        return self.scale(image, labels)\n",
        "\n",
        "            # If all attempts failed, return the unaltered input image.\n",
        "            if labels is None:\n",
        "                return image\n",
        "\n",
        "            else:\n",
        "                return image, labels\n",
        "\n",
        "        elif labels is None:\n",
        "            return image\n",
        "\n",
        "        else:\n",
        "            return image, labels\n",
        "\n",
        "class Rotate:\n",
        "    '''\n",
        "    Rotates images counter-clockwise by 90, 180, or 270 degrees.\n",
        "    '''\n",
        "\n",
        "    def __init__(self,\n",
        "                 angle,\n",
        "                 labels_format={'class_id': 0, 'xmin': 1, 'ymin': 2, 'xmax': 3, 'ymax': 4}):\n",
        "        '''\n",
        "        Arguments:\n",
        "            angle (int): The angle in degrees by which to rotate the images counter-clockwise.\n",
        "                Only 90, 180, and 270 are valid values.\n",
        "            labels_format (dict, optional): A dictionary that defines which index in the last axis of the labels\n",
        "                of an image contains which bounding box coordinate. The dictionary maps at least the keywords\n",
        "                'xmin', 'ymin', 'xmax', and 'ymax' to their respective indices within last axis of the labels array.\n",
        "        '''\n",
        "\n",
        "        if not angle in {90, 180, 270}:\n",
        "            raise ValueError(\"`angle` must be in the set {90, 180, 270}.\")\n",
        "        self.angle = angle\n",
        "        self.labels_format = labels_format\n",
        "\n",
        "    def __call__(self, image, labels=None):\n",
        "\n",
        "        img_height, img_width = image.shape[:2]\n",
        "\n",
        "        # Compute the rotation matrix.\n",
        "        M = cv2.getRotationMatrix2D(center=(img_width / 2, img_height / 2),\n",
        "                                    angle=self.angle,\n",
        "                                    scale=1)\n",
        "\n",
        "        # Get the sine and cosine from the rotation matrix.\n",
        "        cos_angle = np.abs(M[0, 0])\n",
        "        sin_angle = np.abs(M[0, 1])\n",
        "\n",
        "        # Compute the new bounding dimensions of the image.\n",
        "        img_width_new = int(img_height * sin_angle + img_width * cos_angle)\n",
        "        img_height_new = int(img_height * cos_angle + img_width * sin_angle)\n",
        "\n",
        "        # Adjust the rotation matrix to take into account the translation.\n",
        "        M[1, 2] += (img_height_new - img_height) / 2\n",
        "        M[0, 2] += (img_width_new - img_width) / 2\n",
        "\n",
        "        # Rotate the image.\n",
        "        image = cv2.warpAffine(image,\n",
        "                               M=M,\n",
        "                               dsize=(img_width_new, img_height_new))\n",
        "\n",
        "        if labels is None:\n",
        "            return image\n",
        "        else:\n",
        "            xmin = self.labels_format['xmin']\n",
        "            ymin = self.labels_format['ymin']\n",
        "            xmax = self.labels_format['xmax']\n",
        "            ymax = self.labels_format['ymax']\n",
        "\n",
        "            labels = np.copy(labels)\n",
        "            # Rotate the bounding boxes accordingly.\n",
        "            # Transform two opposite corner points of the rectangular boxes using the rotation matrix `M`.\n",
        "            toplefts = np.array([labels[:,xmin], labels[:,ymin], np.ones(labels.shape[0])])\n",
        "            bottomrights = np.array([labels[:,xmax], labels[:,ymax], np.ones(labels.shape[0])])\n",
        "            new_toplefts = (np.dot(M, toplefts)).T\n",
        "            new_bottomrights = (np.dot(M, bottomrights)).T\n",
        "            labels[:,[xmin,ymin]] = np.round(new_toplefts, decimals=0).astype(np.int)\n",
        "            labels[:,[xmax,ymax]] = np.round(new_bottomrights, decimals=0).astype(np.int)\n",
        "\n",
        "            if self.angle == 90:\n",
        "                # ymin and ymax were switched by the rotation.\n",
        "                labels[:,[ymax,ymin]] = labels[:,[ymin,ymax]]\n",
        "            elif self.angle == 180:\n",
        "                # ymin and ymax were switched by the rotation,\n",
        "                # and also xmin and xmax were switched.\n",
        "                labels[:,[ymax,ymin]] = labels[:,[ymin,ymax]]\n",
        "                labels[:,[xmax,xmin]] = labels[:,[xmin,xmax]]\n",
        "            elif self.angle == 270:\n",
        "                # xmin and xmax were switched by the rotation.\n",
        "                labels[:,[xmax,xmin]] = labels[:,[xmin,xmax]]\n",
        "\n",
        "            return image, labels\n",
        "\n",
        "class RandomRotate:\n",
        "    '''\n",
        "    Randomly rotates images counter-clockwise.\n",
        "    '''\n",
        "\n",
        "    def __init__(self,\n",
        "                 angles=[90, 180, 270],\n",
        "                 prob=0.5,\n",
        "                 labels_format={'class_id': 0, 'xmin': 1, 'ymin': 2, 'xmax': 3, 'ymax': 4}):\n",
        "        '''\n",
        "        Arguments:\n",
        "            angle (list): The list of angles in degrees from which one is randomly selected to rotate\n",
        "                the images counter-clockwise. Only 90, 180, and 270 are valid values.\n",
        "            prob (float, optional): `(1 - prob)` determines the probability with which the original,\n",
        "                unaltered image is returned.\n",
        "            labels_format (dict, optional): A dictionary that defines which index in the last axis of the labels\n",
        "                of an image contains which bounding box coordinate. The dictionary maps at least the keywords\n",
        "                'xmin', 'ymin', 'xmax', and 'ymax' to their respective indices within last axis of the labels array.\n",
        "        '''\n",
        "        for angle in angles:\n",
        "            if not angle in {90, 180, 270}:\n",
        "                raise ValueError(\"`angles` can only contain the values 90, 180, and 270.\")\n",
        "        self.angles = angles\n",
        "        self.prob = prob\n",
        "        self.labels_format = labels_format\n",
        "        self.rotate = Rotate(angle=90, labels_format=self.labels_format)\n",
        "\n",
        "    def __call__(self, image, labels=None):\n",
        "\n",
        "        p = np.random.uniform(0,1)\n",
        "        if p >= (1.0-self.prob):\n",
        "            # Pick a rotation angle.\n",
        "            self.rotate.angle = random.choice(self.angles)\n",
        "            self.rotate.labels_format = self.labels_format\n",
        "            return self.rotate(image, labels)\n",
        "\n",
        "        elif labels is None:\n",
        "            return image\n",
        "\n",
        "        else:\n",
        "            return image, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uxdC97Vstxi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Various patch sampling operations for data augmentation in 2D object detection.\n",
        "Copyright (C) 2018 Pierluigi Ferrari\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "   http://www.apache.org/licenses/LICENSE-2.0\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "'''\n",
        "\n",
        "from __future__ import division\n",
        "import numpy as np\n",
        "\n",
        "#from data_generator.object_detection_2d_image_boxes_validation_utils import BoundGenerator, BoxFilter, ImageValidator\n",
        "\n",
        "class PatchCoordinateGenerator:\n",
        "    '''\n",
        "    Generates random patch coordinates that meet specified requirements.\n",
        "    '''\n",
        "\n",
        "    def __init__(self,\n",
        "                 img_height=None,\n",
        "                 img_width=None,\n",
        "                 must_match='h_w',\n",
        "                 min_scale=0.3,\n",
        "                 max_scale=1.0,\n",
        "                 scale_uniformly=False,\n",
        "                 min_aspect_ratio = 0.5,\n",
        "                 max_aspect_ratio = 2.0,\n",
        "                 patch_ymin=None,\n",
        "                 patch_xmin=None,\n",
        "                 patch_height=None,\n",
        "                 patch_width=None,\n",
        "                 patch_aspect_ratio=None):\n",
        "        '''\n",
        "        Arguments:\n",
        "            img_height (int): The height of the image for which the patch coordinates\n",
        "                shall be generated. Doesn't have to be known upon construction.\n",
        "            img_width (int): The width of the image for which the patch coordinates\n",
        "                shall be generated. Doesn't have to be known upon construction.\n",
        "            must_match (str, optional): Can be either of 'h_w', 'h_ar', and 'w_ar'.\n",
        "                Specifies which two of the three quantities height, width, and aspect\n",
        "                ratio determine the shape of the generated patch. The respective third\n",
        "                quantity will be computed from the other two. For example,\n",
        "                if `must_match == 'h_w'`, then the patch's height and width will be\n",
        "                set to lie within [min_scale, max_scale] of the image size or to\n",
        "                `patch_height` and/or `patch_width`, if given. The patch's aspect ratio\n",
        "                is the dependent variable in this case, it will be computed from the\n",
        "                height and width. Any given values for `patch_aspect_ratio`,\n",
        "                `min_aspect_ratio`, or `max_aspect_ratio` will be ignored.\n",
        "            min_scale (float, optional): The minimum size of a dimension of the patch\n",
        "                as a fraction of the respective dimension of the image. Can be greater\n",
        "                than 1. For example, if the image width is 200 and `min_scale == 0.5`,\n",
        "                then the width of the generated patch will be at least 100. If `min_scale == 1.5`,\n",
        "                the width of the generated patch will be at least 300.\n",
        "            max_scale (float, optional): The maximum size of a dimension of the patch\n",
        "                as a fraction of the respective dimension of the image. Can be greater\n",
        "                than 1. For example, if the image width is 200 and `max_scale == 1.0`,\n",
        "                then the width of the generated patch will be at most 200. If `max_scale == 1.5`,\n",
        "                the width of the generated patch will be at most 300. Must be greater than\n",
        "                `min_scale`.\n",
        "            scale_uniformly (bool, optional): If `True` and if `must_match == 'h_w'`,\n",
        "                the patch height and width will be scaled uniformly, otherwise they will\n",
        "                be scaled independently.\n",
        "            min_aspect_ratio (float, optional): Determines the minimum aspect ratio\n",
        "                for the generated patches.\n",
        "            max_aspect_ratio (float, optional): Determines the maximum aspect ratio\n",
        "                for the generated patches.\n",
        "            patch_ymin (int, optional): `None` or the vertical coordinate of the top left\n",
        "                corner of the generated patches. If this is not `None`, the position of the\n",
        "                patches along the vertical axis is fixed. If this is `None`, then the\n",
        "                vertical position of generated patches will be chosen randomly such that\n",
        "                the overlap of a patch and the image along the vertical dimension is\n",
        "                always maximal.\n",
        "            patch_xmin (int, optional): `None` or the horizontal coordinate of the top left\n",
        "                corner of the generated patches. If this is not `None`, the position of the\n",
        "                patches along the horizontal axis is fixed. If this is `None`, then the\n",
        "                horizontal position of generated patches will be chosen randomly such that\n",
        "                the overlap of a patch and the image along the horizontal dimension is\n",
        "                always maximal.\n",
        "            patch_height (int, optional): `None` or the fixed height of the generated patches.\n",
        "            patch_width (int, optional): `None` or the fixed width of the generated patches.\n",
        "            patch_aspect_ratio (float, optional): `None` or the fixed aspect ratio of the\n",
        "                generated patches.\n",
        "        '''\n",
        "\n",
        "        if not (must_match in {'h_w', 'h_ar', 'w_ar'}):\n",
        "            raise ValueError(\"`must_match` must be either of 'h_w', 'h_ar' and 'w_ar'.\")\n",
        "        if min_scale >= max_scale:\n",
        "            raise ValueError(\"It must be `min_scale < max_scale`.\")\n",
        "        if min_aspect_ratio >= max_aspect_ratio:\n",
        "            raise ValueError(\"It must be `min_aspect_ratio < max_aspect_ratio`.\")\n",
        "        if scale_uniformly and not ((patch_height is None) and (patch_width is None)):\n",
        "            raise ValueError(\"If `scale_uniformly == True`, `patch_height` and `patch_width` must both be `None`.\")\n",
        "        self.img_height = img_height\n",
        "        self.img_width = img_width\n",
        "        self.must_match = must_match\n",
        "        self.min_scale = min_scale\n",
        "        self.max_scale = max_scale\n",
        "        self.scale_uniformly = scale_uniformly\n",
        "        self.min_aspect_ratio = min_aspect_ratio\n",
        "        self.max_aspect_ratio = max_aspect_ratio\n",
        "        self.patch_ymin = patch_ymin\n",
        "        self.patch_xmin = patch_xmin\n",
        "        self.patch_height = patch_height\n",
        "        self.patch_width = patch_width\n",
        "        self.patch_aspect_ratio = patch_aspect_ratio\n",
        "\n",
        "    def __call__(self):\n",
        "        '''\n",
        "        Returns:\n",
        "            A 4-tuple `(ymin, xmin, height, width)` that represents the coordinates\n",
        "            of the generated patch.\n",
        "        '''\n",
        "\n",
        "        # Get the patch height and width.\n",
        "\n",
        "        if self.must_match == 'h_w': # Aspect is the dependent variable.\n",
        "            if not self.scale_uniformly:\n",
        "                # Get the height.\n",
        "                if self.patch_height is None:\n",
        "                    patch_height = int(np.random.uniform(self.min_scale, self.max_scale) * self.img_height)\n",
        "                else:\n",
        "                    patch_height = self.patch_height\n",
        "                # Get the width.\n",
        "                if self.patch_width is None:\n",
        "                    patch_width = int(np.random.uniform(self.min_scale, self.max_scale) * self.img_width)\n",
        "                else:\n",
        "                    patch_width = self.patch_width\n",
        "            else:\n",
        "                scaling_factor = np.random.uniform(self.min_scale, self.max_scale)\n",
        "                patch_height = int(scaling_factor * self.img_height)\n",
        "                patch_width = int(scaling_factor * self.img_width)\n",
        "\n",
        "        elif self.must_match == 'h_ar': # Width is the dependent variable.\n",
        "            # Get the height.\n",
        "            if self.patch_height is None:\n",
        "                patch_height = int(np.random.uniform(self.min_scale, self.max_scale) * self.img_height)\n",
        "            else:\n",
        "                patch_height = self.patch_height\n",
        "            # Get the aspect ratio.\n",
        "            if self.patch_aspect_ratio is None:\n",
        "                patch_aspect_ratio = np.random.uniform(self.min_aspect_ratio, self.max_aspect_ratio)\n",
        "            else:\n",
        "                patch_aspect_ratio = self.patch_aspect_ratio\n",
        "            # Get the width.\n",
        "            patch_width = int(patch_height * patch_aspect_ratio)\n",
        "\n",
        "        elif self.must_match == 'w_ar': # Height is the dependent variable.\n",
        "            # Get the width.\n",
        "            if self.patch_width is None:\n",
        "                patch_width = int(np.random.uniform(self.min_scale, self.max_scale) * self.img_width)\n",
        "            else:\n",
        "                patch_width = self.patch_width\n",
        "            # Get the aspect ratio.\n",
        "            if self.patch_aspect_ratio is None:\n",
        "                patch_aspect_ratio = np.random.uniform(self.min_aspect_ratio, self.max_aspect_ratio)\n",
        "            else:\n",
        "                patch_aspect_ratio = self.patch_aspect_ratio\n",
        "            # Get the height.\n",
        "            patch_height = int(patch_width / patch_aspect_ratio)\n",
        "\n",
        "        # Get the top left corner coordinates of the patch.\n",
        "\n",
        "        if self.patch_ymin is None:\n",
        "            # Compute how much room we have along the vertical axis to place the patch.\n",
        "            # A negative number here means that we want to sample a patch that is larger than the original image\n",
        "            # in the vertical dimension, in which case the patch will be placed such that it fully contains the\n",
        "            # image in the vertical dimension.\n",
        "            y_range = self.img_height - patch_height\n",
        "            # Select a random top left corner for the sample position from the possible positions.\n",
        "            if y_range >= 0: patch_ymin = np.random.randint(0, y_range + 1) # There are y_range + 1 possible positions for the crop in the vertical dimension.\n",
        "            else: patch_ymin = np.random.randint(y_range, 1) # The possible positions for the image on the background canvas in the vertical dimension.\n",
        "        else:\n",
        "            patch_ymin = self.patch_ymin\n",
        "\n",
        "        if self.patch_xmin is None:\n",
        "            # Compute how much room we have along the horizontal axis to place the patch.\n",
        "            # A negative number here means that we want to sample a patch that is larger than the original image\n",
        "            # in the horizontal dimension, in which case the patch will be placed such that it fully contains the\n",
        "            # image in the horizontal dimension.\n",
        "            x_range = self.img_width - patch_width\n",
        "            # Select a random top left corner for the sample position from the possible positions.\n",
        "            if x_range >= 0: patch_xmin = np.random.randint(0, x_range + 1) # There are x_range + 1 possible positions for the crop in the horizontal dimension.\n",
        "            else: patch_xmin = np.random.randint(x_range, 1) # The possible positions for the image on the background canvas in the horizontal dimension.\n",
        "        else:\n",
        "            patch_xmin = self.patch_xmin\n",
        "\n",
        "        return (patch_ymin, patch_xmin, patch_height, patch_width)\n",
        "\n",
        "class CropPad:\n",
        "    '''\n",
        "    Crops and/or pads an image deterministically.\n",
        "    Depending on the given output patch size and the position (top left corner) relative\n",
        "    to the input image, the image will be cropped and/or padded along one or both spatial\n",
        "    dimensions.\n",
        "    For example, if the output patch lies entirely within the input image, this will result\n",
        "    in a regular crop. If the input image lies entirely within the output patch, this will\n",
        "    result in the image being padded in every direction. All other cases are mixed cases\n",
        "    where the image might be cropped in some directions and padded in others.\n",
        "    The output patch can be arbitrary in both size and position as long as it overlaps\n",
        "    with the input image.\n",
        "    '''\n",
        "\n",
        "    def __init__(self,\n",
        "                 patch_ymin,\n",
        "                 patch_xmin,\n",
        "                 patch_height,\n",
        "                 patch_width,\n",
        "                 clip_boxes=True,\n",
        "                 box_filter=None,\n",
        "                 background=(0,0,0),\n",
        "                 labels_format={'class_id': 0, 'xmin': 1, 'ymin': 2, 'xmax': 3, 'ymax': 4}):\n",
        "        '''\n",
        "        Arguments:\n",
        "            patch_ymin (int, optional): The vertical coordinate of the top left corner of the output\n",
        "                patch relative to the image coordinate system. Can be negative (i.e. lie outside the image)\n",
        "                as long as the resulting patch still overlaps with the image.\n",
        "            patch_ymin (int, optional): The horizontal coordinate of the top left corner of the output\n",
        "                patch relative to the image coordinate system. Can be negative (i.e. lie outside the image)\n",
        "                as long as the resulting patch still overlaps with the image.\n",
        "            patch_height (int): The height of the patch to be sampled from the image. Can be greater\n",
        "                than the height of the input image.\n",
        "            patch_width (int): The width of the patch to be sampled from the image. Can be greater\n",
        "                than the width of the input image.\n",
        "            clip_boxes (bool, optional): Only relevant if ground truth bounding boxes are given.\n",
        "                If `True`, any ground truth bounding boxes will be clipped to lie entirely within the\n",
        "                sampled patch.\n",
        "            box_filter (BoxFilter, optional): Only relevant if ground truth bounding boxes are given.\n",
        "                A `BoxFilter` object to filter out bounding boxes that don't meet the given criteria\n",
        "                after the transformation. Refer to the `BoxFilter` documentation for details. If `None`,\n",
        "                the validity of the bounding boxes is not checked.\n",
        "            background (list/tuple, optional): A 3-tuple specifying the RGB color value of the potential\n",
        "                background pixels of the scaled images. In the case of single-channel images,\n",
        "                the first element of `background` will be used as the background pixel value.\n",
        "            labels_format (dict, optional): A dictionary that defines which index in the last axis of the labels\n",
        "                of an image contains which bounding box coordinate. The dictionary maps at least the keywords\n",
        "                'xmin', 'ymin', 'xmax', and 'ymax' to their respective indices within last axis of the labels array.\n",
        "        '''\n",
        "        #if (patch_height <= 0) or (patch_width <= 0):\n",
        "        #    raise ValueError(\"Patch height and width must both be positive.\")\n",
        "        #if (patch_ymin + patch_height < 0) or (patch_xmin + patch_width < 0):\n",
        "        #    raise ValueError(\"A patch with the given coordinates cannot overlap with an input image.\")\n",
        "        if not (isinstance(box_filter, BoxFilter) or box_filter is None):\n",
        "            raise ValueError(\"`box_filter` must be either `None` or a `BoxFilter` object.\")\n",
        "        self.patch_height = patch_height\n",
        "        self.patch_width = patch_width\n",
        "        self.patch_ymin = patch_ymin\n",
        "        self.patch_xmin = patch_xmin\n",
        "        self.clip_boxes = clip_boxes\n",
        "        self.box_filter = box_filter\n",
        "        self.background = background\n",
        "        self.labels_format = labels_format\n",
        "\n",
        "    def __call__(self, image, labels=None, return_inverter=False):\n",
        "\n",
        "        img_height, img_width = image.shape[:2]\n",
        "\n",
        "        if (self.patch_ymin > img_height) or (self.patch_xmin > img_width):\n",
        "            raise ValueError(\"The given patch doesn't overlap with the input image.\")\n",
        "\n",
        "        labels = np.copy(labels)\n",
        "\n",
        "        xmin = self.labels_format['xmin']\n",
        "        ymin = self.labels_format['ymin']\n",
        "        xmax = self.labels_format['xmax']\n",
        "        ymax = self.labels_format['ymax']\n",
        "\n",
        "        # Top left corner of the patch relative to the image coordinate system:\n",
        "        patch_ymin = self.patch_ymin\n",
        "        patch_xmin = self.patch_xmin\n",
        "\n",
        "        # Create a canvas of the size of the patch we want to end up with.\n",
        "        if image.ndim == 3:\n",
        "            canvas = np.zeros(shape=(self.patch_height, self.patch_width, 3), dtype=np.uint8)\n",
        "            canvas[:, :] = self.background\n",
        "        elif image.ndim == 2:\n",
        "            canvas = np.zeros(shape=(self.patch_height, self.patch_width), dtype=np.uint8)\n",
        "            canvas[:, :] = self.background[0]\n",
        "\n",
        "        # Perform the crop.\n",
        "        if patch_ymin < 0 and patch_xmin < 0: # Pad the image at the top and on the left.\n",
        "            image_crop_height = min(img_height, self.patch_height + patch_ymin)  # The number of pixels of the image that will end up on the canvas in the vertical direction.\n",
        "            image_crop_width = min(img_width, self.patch_width + patch_xmin) # The number of pixels of the image that will end up on the canvas in the horizontal direction.\n",
        "            canvas[-patch_ymin:-patch_ymin + image_crop_height, -patch_xmin:-patch_xmin + image_crop_width] = image[:image_crop_height, :image_crop_width]\n",
        "\n",
        "        elif patch_ymin < 0 and patch_xmin >= 0: # Pad the image at the top and crop it on the left.\n",
        "            image_crop_height = min(img_height, self.patch_height + patch_ymin)  # The number of pixels of the image that will end up on the canvas in the vertical direction.\n",
        "            image_crop_width = min(self.patch_width, img_width - patch_xmin) # The number of pixels of the image that will end up on the canvas in the horizontal direction.\n",
        "            canvas[-patch_ymin:-patch_ymin + image_crop_height, :image_crop_width] = image[:image_crop_height, patch_xmin:patch_xmin + image_crop_width]\n",
        "\n",
        "        elif patch_ymin >= 0 and patch_xmin < 0: # Crop the image at the top and pad it on the left.\n",
        "            image_crop_height = min(self.patch_height, img_height - patch_ymin) # The number of pixels of the image that will end up on the canvas in the vertical direction.\n",
        "            image_crop_width = min(img_width, self.patch_width + patch_xmin) # The number of pixels of the image that will end up on the canvas in the horizontal direction.\n",
        "            canvas[:image_crop_height, -patch_xmin:-patch_xmin + image_crop_width] = image[patch_ymin:patch_ymin + image_crop_height, :image_crop_width]\n",
        "\n",
        "        elif patch_ymin >= 0 and patch_xmin >= 0: # Crop the image at the top and on the left.\n",
        "            image_crop_height = min(self.patch_height, img_height - patch_ymin) # The number of pixels of the image that will end up on the canvas in the vertical direction.\n",
        "            image_crop_width = min(self.patch_width, img_width - patch_xmin) # The number of pixels of the image that will end up on the canvas in the horizontal direction.\n",
        "            canvas[:image_crop_height, :image_crop_width] = image[patch_ymin:patch_ymin + image_crop_height, patch_xmin:patch_xmin + image_crop_width]\n",
        "\n",
        "        image = canvas\n",
        "\n",
        "        if return_inverter:\n",
        "            def inverter(labels):\n",
        "                labels = np.copy(labels)\n",
        "                labels[:, [ymin+1, ymax+1]] += patch_ymin\n",
        "                labels[:, [xmin+1, xmax+1]] += patch_xmin\n",
        "                return labels\n",
        "\n",
        "        if not (labels is None):\n",
        "\n",
        "            # Translate the box coordinates to the patch's coordinate system.\n",
        "            labels[:, [ymin, ymax]] -= patch_ymin\n",
        "            labels[:, [xmin, xmax]] -= patch_xmin\n",
        "\n",
        "            # Compute all valid boxes for this patch.\n",
        "            if not (self.box_filter is None):\n",
        "                self.box_filter.labels_format = self.labels_format\n",
        "                labels = self.box_filter(labels=labels,\n",
        "                                         image_height=self.patch_height,\n",
        "                                         image_width=self.patch_width)\n",
        "\n",
        "            if self.clip_boxes:\n",
        "                labels[:,[ymin,ymax]] = np.clip(labels[:,[ymin,ymax]], a_min=0, a_max=self.patch_height-1)\n",
        "                labels[:,[xmin,xmax]] = np.clip(labels[:,[xmin,xmax]], a_min=0, a_max=self.patch_width-1)\n",
        "\n",
        "            if return_inverter:\n",
        "                return image, labels, inverter\n",
        "            else:\n",
        "                return image, labels\n",
        "\n",
        "        else:\n",
        "            if return_inverter:\n",
        "                return image, inverter\n",
        "            else:\n",
        "                return image\n",
        "\n",
        "class Crop:\n",
        "    '''\n",
        "    Crops off the specified numbers of pixels from the borders of images.\n",
        "    This is just a convenience interface for `CropPad`.\n",
        "    '''\n",
        "\n",
        "    def __init__(self,\n",
        "                 crop_top,\n",
        "                 crop_bottom,\n",
        "                 crop_left,\n",
        "                 crop_right,\n",
        "                 clip_boxes=True,\n",
        "                 box_filter=None,\n",
        "                 labels_format={'class_id': 0, 'xmin': 1, 'ymin': 2, 'xmax': 3, 'ymax': 4}):\n",
        "        self.crop_top = crop_top\n",
        "        self.crop_bottom = crop_bottom\n",
        "        self.crop_left = crop_left\n",
        "        self.crop_right = crop_right\n",
        "        self.clip_boxes = clip_boxes\n",
        "        self.box_filter = box_filter\n",
        "        self.labels_format = labels_format\n",
        "        self.crop = CropPad(patch_ymin=self.crop_top,\n",
        "                            patch_xmin=self.crop_left,\n",
        "                            patch_height=None,\n",
        "                            patch_width=None,\n",
        "                            clip_boxes=self.clip_boxes,\n",
        "                            box_filter=self.box_filter,\n",
        "                            labels_format=self.labels_format)\n",
        "\n",
        "    def __call__(self, image, labels=None, return_inverter=False):\n",
        "\n",
        "        img_height, img_width = image.shape[:2]\n",
        "\n",
        "        self.crop.patch_height = img_height - self.crop_top - self.crop_bottom\n",
        "        self.crop.patch_width = img_width - self.crop_left - self.crop_right\n",
        "        self.crop.labels_format = self.labels_format\n",
        "\n",
        "        return self.crop(image, labels, return_inverter)\n",
        "\n",
        "class Pad:\n",
        "    '''\n",
        "    Pads images by the specified numbers of pixels on each side.\n",
        "    This is just a convenience interface for `CropPad`.\n",
        "    '''\n",
        "\n",
        "    def __init__(self,\n",
        "                 pad_top,\n",
        "                 pad_bottom,\n",
        "                 pad_left,\n",
        "                 pad_right,\n",
        "                 background=(0,0,0),\n",
        "                 labels_format={'class_id': 0, 'xmin': 1, 'ymin': 2, 'xmax': 3, 'ymax': 4}):\n",
        "        self.pad_top = pad_top\n",
        "        self.pad_bottom = pad_bottom\n",
        "        self.pad_left = pad_left\n",
        "        self.pad_right = pad_right\n",
        "        self.background = background\n",
        "        self.labels_format = labels_format\n",
        "        self.pad = CropPad(patch_ymin=-self.pad_top,\n",
        "                           patch_xmin=-self.pad_left,\n",
        "                           patch_height=None,\n",
        "                           patch_width=None,\n",
        "                           clip_boxes=False,\n",
        "                           box_filter=None,\n",
        "                           background=self.background,\n",
        "                           labels_format=self.labels_format)\n",
        "\n",
        "    def __call__(self, image, labels=None, return_inverter=False):\n",
        "\n",
        "        img_height, img_width = image.shape[:2]\n",
        "\n",
        "        self.pad.patch_height = img_height + self.pad_top + self.pad_bottom\n",
        "        self.pad.patch_width = img_width + self.pad_left + self.pad_right\n",
        "        self.pad.labels_format = self.labels_format\n",
        "\n",
        "        return self.pad(image, labels, return_inverter)\n",
        "\n",
        "class RandomPatch:\n",
        "    '''\n",
        "    Randomly samples a patch from an image. The randomness refers to whatever\n",
        "    randomness may be introduced by the patch coordinate generator, the box filter,\n",
        "    and the patch validator.\n",
        "    Input images may be cropped and/or padded along either or both of the two\n",
        "    spatial dimensions as necessary in order to obtain the required patch.\n",
        "    As opposed to `RandomPatchInf`, it is possible for this transform to fail to produce\n",
        "    an output image at all, in which case it will return `None`. This is useful, because\n",
        "    if this transform is used to generate patches of a fixed size or aspect ratio, then\n",
        "    the caller needs to be able to rely on the output image satisfying the set size or\n",
        "    aspect ratio. It might therefore not be an option to return the unaltered input image\n",
        "    as other random transforms do when they fail to produce a valid transformed image.\n",
        "    '''\n",
        "\n",
        "    def __init__(self,\n",
        "                 patch_coord_generator,\n",
        "                 box_filter=None,\n",
        "                 image_validator=None,\n",
        "                 n_trials_max=3,\n",
        "                 clip_boxes=True,\n",
        "                 prob=1.0,\n",
        "                 background=(0,0,0),\n",
        "                 can_fail=False,\n",
        "                 labels_format={'class_id': 0, 'xmin': 1, 'ymin': 2, 'xmax': 3, 'ymax': 4}):\n",
        "        '''\n",
        "        Arguments:\n",
        "            patch_coord_generator (PatchCoordinateGenerator): A `PatchCoordinateGenerator` object\n",
        "                to generate the positions and sizes of the patches to be sampled from the input images.\n",
        "            box_filter (BoxFilter, optional): Only relevant if ground truth bounding boxes are given.\n",
        "                A `BoxFilter` object to filter out bounding boxes that don't meet the given criteria\n",
        "                after the transformation. Refer to the `BoxFilter` documentation for details. If `None`,\n",
        "                the validity of the bounding boxes is not checked.\n",
        "            image_validator (ImageValidator, optional): Only relevant if ground truth bounding boxes are given.\n",
        "                An `ImageValidator` object to determine whether a sampled patch is valid. If `None`,\n",
        "                any outcome is valid.\n",
        "            n_trials_max (int, optional): Only relevant if ground truth bounding boxes are given.\n",
        "                Determines the maxmial number of trials to sample a valid patch. If no valid patch could\n",
        "                be sampled in `n_trials_max` trials, returns one `None` in place of each regular output.\n",
        "            clip_boxes (bool, optional): Only relevant if ground truth bounding boxes are given.\n",
        "                If `True`, any ground truth bounding boxes will be clipped to lie entirely within the\n",
        "                sampled patch.\n",
        "            prob (float, optional): `(1 - prob)` determines the probability with which the original,\n",
        "                unaltered image is returned.\n",
        "            background (list/tuple, optional): A 3-tuple specifying the RGB color value of the potential\n",
        "                background pixels of the scaled images. In the case of single-channel images,\n",
        "                the first element of `background` will be used as the background pixel value.\n",
        "            can_fail (bool, optional): If `True`, will return `None` if no valid patch could be found after\n",
        "                `n_trials_max` trials. If `False`, will return the unaltered input image in such a case.\n",
        "            labels_format (dict, optional): A dictionary that defines which index in the last axis of the labels\n",
        "                of an image contains which bounding box coordinate. The dictionary maps at least the keywords\n",
        "                'xmin', 'ymin', 'xmax', and 'ymax' to their respective indices within last axis of the labels array.\n",
        "        '''\n",
        "        if not isinstance(patch_coord_generator, PatchCoordinateGenerator):\n",
        "            raise ValueError(\"`patch_coord_generator` must be an instance of `PatchCoordinateGenerator`.\")\n",
        "        if not (isinstance(image_validator, ImageValidator) or image_validator is None):\n",
        "            raise ValueError(\"`image_validator` must be either `None` or an `ImageValidator` object.\")\n",
        "        self.patch_coord_generator = patch_coord_generator\n",
        "        self.box_filter = box_filter\n",
        "        self.image_validator = image_validator\n",
        "        self.n_trials_max = n_trials_max\n",
        "        self.clip_boxes = clip_boxes\n",
        "        self.prob = prob\n",
        "        self.background = background\n",
        "        self.can_fail = can_fail\n",
        "        self.labels_format = labels_format\n",
        "        self.sample_patch = CropPad(patch_ymin=None,\n",
        "                                    patch_xmin=None,\n",
        "                                    patch_height=None,\n",
        "                                    patch_width=None,\n",
        "                                    clip_boxes=self.clip_boxes,\n",
        "                                    box_filter=self.box_filter,\n",
        "                                    background=self.background,\n",
        "                                    labels_format=self.labels_format)\n",
        "\n",
        "    def __call__(self, image, labels=None, return_inverter=False):\n",
        "\n",
        "        p = np.random.uniform(0,1)\n",
        "        if p >= (1.0-self.prob):\n",
        "\n",
        "            img_height, img_width = image.shape[:2]\n",
        "            self.patch_coord_generator.img_height = img_height\n",
        "            self.patch_coord_generator.img_width = img_width\n",
        "\n",
        "            xmin = self.labels_format['xmin']\n",
        "            ymin = self.labels_format['ymin']\n",
        "            xmax = self.labels_format['xmax']\n",
        "            ymax = self.labels_format['ymax']\n",
        "\n",
        "            # Override the preset labels format.\n",
        "            if not self.image_validator is None:\n",
        "                self.image_validator.labels_format = self.labels_format\n",
        "            self.sample_patch.labels_format = self.labels_format\n",
        "\n",
        "            for _ in range(max(1, self.n_trials_max)):\n",
        "\n",
        "                # Generate patch coordinates.\n",
        "                patch_ymin, patch_xmin, patch_height, patch_width = self.patch_coord_generator()\n",
        "\n",
        "                self.sample_patch.patch_ymin = patch_ymin\n",
        "                self.sample_patch.patch_xmin = patch_xmin\n",
        "                self.sample_patch.patch_height = patch_height\n",
        "                self.sample_patch.patch_width = patch_width\n",
        "\n",
        "                if (labels is None) or (self.image_validator is None):\n",
        "                    # We either don't have any boxes or if we do, we will accept any outcome as valid.\n",
        "                    return self.sample_patch(image, labels, return_inverter)\n",
        "                else:\n",
        "                    # Translate the box coordinates to the patch's coordinate system.\n",
        "                    new_labels = np.copy(labels)\n",
        "                    new_labels[:, [ymin, ymax]] -= patch_ymin\n",
        "                    new_labels[:, [xmin, xmax]] -= patch_xmin\n",
        "                    # Check if the patch is valid.\n",
        "                    if self.image_validator(labels=new_labels,\n",
        "                                            image_height=patch_height,\n",
        "                                            image_width=patch_width):\n",
        "                        return self.sample_patch(image, labels, return_inverter)\n",
        "\n",
        "            # If we weren't able to sample a valid patch...\n",
        "            if self.can_fail:\n",
        "                # ...return `None`.\n",
        "                if labels is None:\n",
        "                    if return_inverter:\n",
        "                        return None, None\n",
        "                    else:\n",
        "                        return None\n",
        "                else:\n",
        "                    if return_inverter:\n",
        "                        return None, None, None\n",
        "                    else:\n",
        "                        return None, None\n",
        "            else:\n",
        "                # ...return the unaltered input image.\n",
        "                if labels is None:\n",
        "                    if return_inverter:\n",
        "                        return image, None\n",
        "                    else:\n",
        "                        return image\n",
        "                else:\n",
        "                    if return_inverter:\n",
        "                        return image, labels, None\n",
        "                    else:\n",
        "                        return image, labels\n",
        "\n",
        "        else:\n",
        "            if return_inverter:\n",
        "                def inverter(labels):\n",
        "                    return labels\n",
        "\n",
        "            if labels is None:\n",
        "                if return_inverter:\n",
        "                    return image, inverter\n",
        "                else:\n",
        "                    return image\n",
        "            else:\n",
        "                if return_inverter:\n",
        "                    return image, labels, inverter\n",
        "                else:\n",
        "                    return image, labels\n",
        "\n",
        "class RandomPatchInf:\n",
        "    '''\n",
        "    Randomly samples a patch from an image. The randomness refers to whatever\n",
        "    randomness may be introduced by the patch coordinate generator, the box filter,\n",
        "    and the patch validator.\n",
        "    Input images may be cropped and/or padded along either or both of the two\n",
        "    spatial dimensions as necessary in order to obtain the required patch.\n",
        "    This operation is very similar to `RandomPatch`, except that:\n",
        "    1. This operation runs indefinitely until either a valid patch is found or\n",
        "       the input image is returned unaltered, i.e. it cannot fail.\n",
        "    2. If a bound generator is given, a new pair of bounds will be generated\n",
        "       every `n_trials_max` iterations.\n",
        "    '''\n",
        "\n",
        "    def __init__(self,\n",
        "                 patch_coord_generator,\n",
        "                 box_filter=None,\n",
        "                 image_validator=None,\n",
        "                 bound_generator=None,\n",
        "                 n_trials_max=50,\n",
        "                 clip_boxes=True,\n",
        "                 prob=0.857,\n",
        "                 background=(0,0,0),\n",
        "                 labels_format={'class_id': 0, 'xmin': 1, 'ymin': 2, 'xmax': 3, 'ymax': 4}):\n",
        "        '''\n",
        "        Arguments:\n",
        "            patch_coord_generator (PatchCoordinateGenerator): A `PatchCoordinateGenerator` object\n",
        "                to generate the positions and sizes of the patches to be sampled from the input images.\n",
        "            box_filter (BoxFilter, optional): Only relevant if ground truth bounding boxes are given.\n",
        "                A `BoxFilter` object to filter out bounding boxes that don't meet the given criteria\n",
        "                after the transformation. Refer to the `BoxFilter` documentation for details. If `None`,\n",
        "                the validity of the bounding boxes is not checked.\n",
        "            image_validator (ImageValidator, optional): Only relevant if ground truth bounding boxes are given.\n",
        "                An `ImageValidator` object to determine whether a sampled patch is valid. If `None`,\n",
        "                any outcome is valid.\n",
        "            bound_generator (BoundGenerator, optional): A `BoundGenerator` object to generate upper and\n",
        "                lower bound values for the patch validator. Every `n_trials_max` trials, a new pair of\n",
        "                upper and lower bounds will be generated until a valid patch is found or the original image\n",
        "                is returned. This bound generator overrides the bound generator of the patch validator.\n",
        "            n_trials_max (int, optional): Only relevant if ground truth bounding boxes are given.\n",
        "                The sampler will run indefinitely until either a valid patch is found or the original image\n",
        "                is returned, but this determines the maxmial number of trials to sample a valid patch for each\n",
        "                selected pair of lower and upper bounds before a new pair is picked.\n",
        "            clip_boxes (bool, optional): Only relevant if ground truth bounding boxes are given.\n",
        "                If `True`, any ground truth bounding boxes will be clipped to lie entirely within the\n",
        "                sampled patch.\n",
        "            prob (float, optional): `(1 - prob)` determines the probability with which the original,\n",
        "                unaltered image is returned.\n",
        "            background (list/tuple, optional): A 3-tuple specifying the RGB color value of the potential\n",
        "                background pixels of the scaled images. In the case of single-channel images,\n",
        "                the first element of `background` will be used as the background pixel value.\n",
        "            labels_format (dict, optional): A dictionary that defines which index in the last axis of the labels\n",
        "                of an image contains which bounding box coordinate. The dictionary maps at least the keywords\n",
        "                'xmin', 'ymin', 'xmax', and 'ymax' to their respective indices within last axis of the labels array.\n",
        "        '''\n",
        "\n",
        "        if not isinstance(patch_coord_generator, PatchCoordinateGenerator):\n",
        "            raise ValueError(\"`patch_coord_generator` must be an instance of `PatchCoordinateGenerator`.\")\n",
        "        if not (isinstance(image_validator, ImageValidator) or image_validator is None):\n",
        "            raise ValueError(\"`image_validator` must be either `None` or an `ImageValidator` object.\")\n",
        "        if not (isinstance(bound_generator, BoundGenerator) or bound_generator is None):\n",
        "            raise ValueError(\"`bound_generator` must be either `None` or a `BoundGenerator` object.\")\n",
        "        self.patch_coord_generator = patch_coord_generator\n",
        "        self.box_filter = box_filter\n",
        "        self.image_validator = image_validator\n",
        "        self.bound_generator = bound_generator\n",
        "        self.n_trials_max = n_trials_max\n",
        "        self.clip_boxes = clip_boxes\n",
        "        self.prob = prob\n",
        "        self.background = background\n",
        "        self.labels_format = labels_format\n",
        "        self.sample_patch = CropPad(patch_ymin=None,\n",
        "                                    patch_xmin=None,\n",
        "                                    patch_height=None,\n",
        "                                    patch_width=None,\n",
        "                                    clip_boxes=self.clip_boxes,\n",
        "                                    box_filter=self.box_filter,\n",
        "                                    background=self.background,\n",
        "                                    labels_format=self.labels_format)\n",
        "\n",
        "    def __call__(self, image, labels=None, return_inverter=False):\n",
        "\n",
        "        img_height, img_width = image.shape[:2]\n",
        "        self.patch_coord_generator.img_height = img_height\n",
        "        self.patch_coord_generator.img_width = img_width\n",
        "\n",
        "        xmin = self.labels_format['xmin']\n",
        "        ymin = self.labels_format['ymin']\n",
        "        xmax = self.labels_format['xmax']\n",
        "        ymax = self.labels_format['ymax']\n",
        "\n",
        "        # Override the preset labels format.\n",
        "        if not self.image_validator is None:\n",
        "            self.image_validator.labels_format = self.labels_format\n",
        "        self.sample_patch.labels_format = self.labels_format\n",
        "\n",
        "        while True: # Keep going until we either find a valid patch or return the original image.\n",
        "\n",
        "            p = np.random.uniform(0,1)\n",
        "            if p >= (1.0-self.prob):\n",
        "\n",
        "                # In case we have a bound generator, pick a lower and upper bound for the patch validator.\n",
        "                if not ((self.image_validator is None) or (self.bound_generator is None)):\n",
        "                    self.image_validator.bounds = self.bound_generator()\n",
        "\n",
        "                # Use at most `self.n_trials_max` attempts to find a crop\n",
        "                # that meets our requirements.\n",
        "                for _ in range(max(1, self.n_trials_max)):\n",
        "\n",
        "                    # Generate patch coordinates.\n",
        "                    patch_ymin, patch_xmin, patch_height, patch_width = self.patch_coord_generator()\n",
        "\n",
        "                    self.sample_patch.patch_ymin = patch_ymin\n",
        "                    self.sample_patch.patch_xmin = patch_xmin\n",
        "                    self.sample_patch.patch_height = patch_height\n",
        "                    self.sample_patch.patch_width = patch_width\n",
        "\n",
        "                    # Check if the resulting patch meets the aspect ratio requirements.\n",
        "                    aspect_ratio = patch_width / patch_height\n",
        "                    if not (self.patch_coord_generator.min_aspect_ratio <= aspect_ratio <= self.patch_coord_generator.max_aspect_ratio):\n",
        "                        continue\n",
        "\n",
        "                    if (labels is None) or (self.image_validator is None):\n",
        "                        # We either don't have any boxes or if we do, we will accept any outcome as valid.\n",
        "                        return self.sample_patch(image, labels, return_inverter)\n",
        "                    else:\n",
        "                        # Translate the box coordinates to the patch's coordinate system.\n",
        "                        new_labels = np.copy(labels)\n",
        "                        new_labels[:, [ymin, ymax]] -= patch_ymin\n",
        "                        new_labels[:, [xmin, xmax]] -= patch_xmin\n",
        "                        # Check if the patch contains the minimum number of boxes we require.\n",
        "                        if self.image_validator(labels=new_labels,\n",
        "                                                image_height=patch_height,\n",
        "                                                image_width=patch_width):\n",
        "                            return self.sample_patch(image, labels, return_inverter)\n",
        "            else:\n",
        "                if return_inverter:\n",
        "                    def inverter(labels):\n",
        "                        return labels\n",
        "\n",
        "                if labels is None:\n",
        "                    if return_inverter:\n",
        "                        return image, inverter\n",
        "                    else:\n",
        "                        return image\n",
        "                else:\n",
        "                    if return_inverter:\n",
        "                        return image, labels, inverter\n",
        "                    else:\n",
        "                        return image, labels\n",
        "\n",
        "class RandomMaxCropFixedAR:\n",
        "    '''\n",
        "    Crops the largest possible patch of a given fixed aspect ratio\n",
        "    from an image.\n",
        "    Since the aspect ratio of the sampled patches is constant, they\n",
        "    can subsequently be resized to the same size without distortion.\n",
        "    '''\n",
        "\n",
        "    def __init__(self,\n",
        "                 patch_aspect_ratio,\n",
        "                 box_filter=None,\n",
        "                 image_validator=None,\n",
        "                 n_trials_max=3,\n",
        "                 clip_boxes=True,\n",
        "                 labels_format={'class_id': 0, 'xmin': 1, 'ymin': 2, 'xmax': 3, 'ymax': 4}):\n",
        "        '''\n",
        "        Arguments:\n",
        "            patch_aspect_ratio (float): The fixed aspect ratio that all sampled patches will have.\n",
        "            box_filter (BoxFilter, optional): Only relevant if ground truth bounding boxes are given.\n",
        "                A `BoxFilter` object to filter out bounding boxes that don't meet the given criteria\n",
        "                after the transformation. Refer to the `BoxFilter` documentation for details. If `None`,\n",
        "                the validity of the bounding boxes is not checked.\n",
        "            image_validator (ImageValidator, optional): Only relevant if ground truth bounding boxes are given.\n",
        "                An `ImageValidator` object to determine whether a sampled patch is valid. If `None`,\n",
        "                any outcome is valid.\n",
        "            n_trials_max (int, optional): Only relevant if ground truth bounding boxes are given.\n",
        "                Determines the maxmial number of trials to sample a valid patch. If no valid patch could\n",
        "                be sampled in `n_trials_max` trials, returns `None`.\n",
        "            clip_boxes (bool, optional): Only relevant if ground truth bounding boxes are given.\n",
        "                If `True`, any ground truth bounding boxes will be clipped to lie entirely within the\n",
        "                sampled patch.\n",
        "            labels_format (dict, optional): A dictionary that defines which index in the last axis of the labels\n",
        "                of an image contains which bounding box coordinate. The dictionary maps at least the keywords\n",
        "                'xmin', 'ymin', 'xmax', and 'ymax' to their respective indices within last axis of the labels array.\n",
        "        '''\n",
        "\n",
        "        self.patch_aspect_ratio = patch_aspect_ratio\n",
        "        self.box_filter = box_filter\n",
        "        self.image_validator = image_validator\n",
        "        self.n_trials_max = n_trials_max\n",
        "        self.clip_boxes = clip_boxes\n",
        "        self.labels_format = labels_format\n",
        "        self.random_patch = RandomPatch(patch_coord_generator=PatchCoordinateGenerator(), # Just a dummy object\n",
        "                                        box_filter=self.box_filter,\n",
        "                                        image_validator=self.image_validator,\n",
        "                                        n_trials_max=self.n_trials_max,\n",
        "                                        clip_boxes=self.clip_boxes,\n",
        "                                        prob=1.0,\n",
        "                                        can_fail=False,\n",
        "                                        labels_format=self.labels_format)\n",
        "\n",
        "    def __call__(self, image, labels=None, return_inverter=False):\n",
        "\n",
        "        img_height, img_width = image.shape[:2]\n",
        "\n",
        "        # The ratio of the input image aspect ratio and patch aspect ratio determines the maximal possible crop.\n",
        "        image_aspect_ratio = img_width / img_height\n",
        "\n",
        "        if image_aspect_ratio < self.patch_aspect_ratio:\n",
        "            patch_width = img_width\n",
        "            patch_height = int(round(patch_width / self.patch_aspect_ratio))\n",
        "        else:\n",
        "            patch_height = img_height\n",
        "            patch_width = int(round(patch_height * self.patch_aspect_ratio))\n",
        "\n",
        "        # Now that we know the desired height and width for the patch,\n",
        "        # instantiate an appropriate patch coordinate generator.\n",
        "        patch_coord_generator = PatchCoordinateGenerator(img_height=img_height,\n",
        "                                                         img_width=img_width,\n",
        "                                                         must_match='h_w',\n",
        "                                                         patch_height=patch_height,\n",
        "                                                         patch_width=patch_width)\n",
        "\n",
        "        # The rest of the work is done by `RandomPatch`.\n",
        "        self.random_patch.patch_coord_generator = patch_coord_generator\n",
        "        self.random_patch.labels_format = self.labels_format\n",
        "        return self.random_patch(image, labels, return_inverter)\n",
        "\n",
        "class RandomPadFixedAR:\n",
        "    '''\n",
        "    Adds the minimal possible padding to an image that results in a patch\n",
        "    of the given fixed aspect ratio that contains the entire image.\n",
        "    Since the aspect ratio of the resulting images is constant, they\n",
        "    can subsequently be resized to the same size without distortion.\n",
        "    '''\n",
        "\n",
        "    def __init__(self,\n",
        "                 patch_aspect_ratio,\n",
        "                 background=(0,0,0),\n",
        "                 labels_format={'class_id': 0, 'xmin': 1, 'ymin': 2, 'xmax': 3, 'ymax': 4}):\n",
        "        '''\n",
        "        Arguments:\n",
        "            patch_aspect_ratio (float): The fixed aspect ratio that all sampled patches will have.\n",
        "            background (list/tuple, optional): A 3-tuple specifying the RGB color value of the potential\n",
        "                background pixels of the scaled images. In the case of single-channel images,\n",
        "                the first element of `background` will be used as the background pixel value.\n",
        "            labels_format (dict, optional): A dictionary that defines which index in the last axis of the labels\n",
        "                of an image contains which bounding box coordinate. The dictionary maps at least the keywords\n",
        "                'xmin', 'ymin', 'xmax', and 'ymax' to their respective indices within last axis of the labels array.\n",
        "        '''\n",
        "\n",
        "        self.patch_aspect_ratio = patch_aspect_ratio\n",
        "        self.background = background\n",
        "        self.labels_format = labels_format\n",
        "        self.random_patch = RandomPatch(patch_coord_generator=PatchCoordinateGenerator(), # Just a dummy object\n",
        "                                        box_filter=None,\n",
        "                                        image_validator=None,\n",
        "                                        n_trials_max=1,\n",
        "                                        clip_boxes=False,\n",
        "                                        background=self.background,\n",
        "                                        prob=1.0,\n",
        "                                        labels_format=self.labels_format)\n",
        "\n",
        "    def __call__(self, image, labels=None, return_inverter=False):\n",
        "\n",
        "        img_height, img_width = image.shape[:2]\n",
        "\n",
        "        if img_width < img_height:\n",
        "            patch_height = img_height\n",
        "            patch_width = int(round(patch_height * self.patch_aspect_ratio))\n",
        "        else:\n",
        "            patch_width = img_width\n",
        "            patch_height = int(round(patch_width / self.patch_aspect_ratio))\n",
        "\n",
        "        # Now that we know the desired height and width for the patch,\n",
        "        # instantiate an appropriate patch coordinate generator.\n",
        "        patch_coord_generator = PatchCoordinateGenerator(img_height=img_height,\n",
        "                                                         img_width=img_width,\n",
        "                                                         must_match='h_w',\n",
        "                                                         patch_height=patch_height,\n",
        "                                                         patch_width=patch_width)\n",
        "\n",
        "        # The rest of the work is done by `RandomPatch`.\n",
        "        self.random_patch.patch_coord_generator = patch_coord_generator\n",
        "        self.random_patch.labels_format = self.labels_format\n",
        "        return self.random_patch(image, labels, return_inverter)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vCZK7Y-EevE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# original chain ssd implementation\n",
        "\n",
        "'''\n",
        "The data augmentation operations of the original SSD implementation.\n",
        "Copyright (C) 2018 Pierluigi Ferrari\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "   http://www.apache.org/licenses/LICENSE-2.0\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "'''\n",
        "\n",
        "from __future__ import division\n",
        "import numpy as np\n",
        "import cv2\n",
        "import inspect\n",
        "\n",
        "#from data_generator.object_detection_2d_photometric_ops import ConvertColor, ConvertDataType, ConvertTo3Channels, RandomBrightness, RandomContrast, RandomHue, RandomSaturation, RandomChannelSwap\n",
        "#from data_generator.object_detection_2d_patch_sampling_ops import PatchCoordinateGenerator, RandomPatch, RandomPatchInf\n",
        "#from data_generator.object_detection_2d_geometric_ops import ResizeRandomInterp, RandomFlip\n",
        "#from data_generator.object_detection_2d_image_boxes_validation_utils import BoundGenerator, BoxFilter, ImageValidator\n",
        "\n",
        "class SSDRandomCrop:\n",
        "    '''\n",
        "    Performs the same random crops as defined by the `batch_sampler` instructions\n",
        "    of the original Caffe implementation of SSD. A description of this random cropping\n",
        "    strategy can also be found in the data augmentation section of the paper:\n",
        "    https://arxiv.org/abs/1512.02325\n",
        "    '''\n",
        "\n",
        "    def __init__(self, labels_format={'class_id': 0, 'xmin': 1, 'ymin': 2, 'xmax': 3, 'ymax': 4}):\n",
        "        '''\n",
        "        Arguments:\n",
        "            labels_format (dict, optional): A dictionary that defines which index in the last axis of the labels\n",
        "                of an image contains which bounding box coordinate. The dictionary maps at least the keywords\n",
        "                'xmin', 'ymin', 'xmax', and 'ymax' to their respective indices within last axis of the labels array.\n",
        "        '''\n",
        "\n",
        "        self.labels_format = labels_format\n",
        "\n",
        "        # This randomly samples one of the lower IoU bounds defined\n",
        "        # by the `sample_space` every time it is called.\n",
        "        self.bound_generator = BoundGenerator(sample_space=((None, None),\n",
        "                                                            (0.1, None),\n",
        "                                                            (0.3, None),\n",
        "                                                            (0.5, None),\n",
        "                                                            (0.7, None),\n",
        "                                                            (0.9, None)),\n",
        "                                              weights=None)\n",
        "\n",
        "        # Produces coordinates for candidate patches such that the height\n",
        "        # and width of the patches are between 0.3 and 1.0 of the height\n",
        "        # and width of the respective image and the aspect ratio of the\n",
        "        # patches is between 0.5 and 2.0.\n",
        "        self.patch_coord_generator = PatchCoordinateGenerator(must_match='h_w',\n",
        "                                                              min_scale=0.3,\n",
        "                                                              max_scale=1.0,\n",
        "                                                              scale_uniformly=False,\n",
        "                                                              min_aspect_ratio = 0.5,\n",
        "                                                              max_aspect_ratio = 2.0)\n",
        "\n",
        "        # Filters out boxes whose center point does not lie within the\n",
        "        # chosen patches.\n",
        "        self.box_filter = BoxFilter(check_overlap=True,\n",
        "                                    check_min_area=False,\n",
        "                                    check_degenerate=False,\n",
        "                                    overlap_criterion='center_point',\n",
        "                                    labels_format=self.labels_format)\n",
        "\n",
        "        # Determines whether a given patch is considered a valid patch.\n",
        "        # Defines a patch to be valid if at least one ground truth bounding box\n",
        "        # (n_boxes_min == 1) has an IoU overlap with the patch that\n",
        "        # meets the requirements defined by `bound_generator`.\n",
        "        self.image_validator = ImageValidator(overlap_criterion='iou',\n",
        "                                              n_boxes_min=1,\n",
        "                                              labels_format=self.labels_format,\n",
        "                                              border_pixels='half')\n",
        "\n",
        "        # Performs crops according to the parameters set in the objects above.\n",
        "        # Runs until either a valid patch is found or the original input image\n",
        "        # is returned unaltered. Runs a maximum of 50 trials to find a valid\n",
        "        # patch for each new sampled IoU threshold. Every 50 trials, the original\n",
        "        # image is returned as is with probability (1 - prob) = 0.143.\n",
        "        self.random_crop = RandomPatchInf(patch_coord_generator=self.patch_coord_generator,\n",
        "                                          box_filter=self.box_filter,\n",
        "                                          image_validator=self.image_validator,\n",
        "                                          bound_generator=self.bound_generator,\n",
        "                                          n_trials_max=50,\n",
        "                                          clip_boxes=True,\n",
        "                                          prob=0.857,\n",
        "                                          labels_format=self.labels_format)\n",
        "\n",
        "    def __call__(self, image, labels=None, return_inverter=False):\n",
        "        self.random_crop.labels_format = self.labels_format\n",
        "        return self.random_crop(image, labels, return_inverter)\n",
        "\n",
        "class SSDExpand:\n",
        "    '''\n",
        "    Performs the random image expansion as defined by the `train_transform_param` instructions\n",
        "    of the original Caffe implementation of SSD. A description of this expansion strategy\n",
        "    can also be found in section 3.6 (\"Data Augmentation for Small Object Accuracy\") of the paper:\n",
        "    https://arxiv.org/abs/1512.02325\n",
        "    '''\n",
        "\n",
        "    def __init__(self, background=(123, 117, 104), labels_format={'class_id': 0, 'xmin': 1, 'ymin': 2, 'xmax': 3, 'ymax': 4}):\n",
        "        '''\n",
        "        Arguments:\n",
        "            background (list/tuple, optional): A 3-tuple specifying the RGB color value of the\n",
        "                background pixels of the translated images.\n",
        "            labels_format (dict, optional): A dictionary that defines which index in the last axis of the labels\n",
        "                of an image contains which bounding box coordinate. The dictionary maps at least the keywords\n",
        "                'xmin', 'ymin', 'xmax', and 'ymax' to their respective indices within last axis of the labels array.\n",
        "        '''\n",
        "\n",
        "        self.labels_format = labels_format\n",
        "\n",
        "        # Generate coordinates for patches that are between 1.0 and 4.0 times\n",
        "        # the size of the input image in both spatial dimensions.\n",
        "        self.patch_coord_generator = PatchCoordinateGenerator(must_match='h_w',\n",
        "                                                              min_scale=1.0,\n",
        "                                                              max_scale=4.0,\n",
        "                                                              scale_uniformly=True)\n",
        "\n",
        "        # With probability 0.5, place the input image randomly on a canvas filled with\n",
        "        # mean color values according to the parameters set above. With probability 0.5,\n",
        "        # return the input image unaltered.\n",
        "        self.expand = RandomPatch(patch_coord_generator=self.patch_coord_generator,\n",
        "                                  box_filter=None,\n",
        "                                  image_validator=None,\n",
        "                                  n_trials_max=1,\n",
        "                                  clip_boxes=False,\n",
        "                                  prob=0.5,\n",
        "                                  background=background,\n",
        "                                  labels_format=self.labels_format)\n",
        "\n",
        "    def __call__(self, image, labels=None, return_inverter=False):\n",
        "        self.expand.labels_format = self.labels_format\n",
        "        return self.expand(image, labels, return_inverter)\n",
        "\n",
        "class SSDPhotometricDistortions:\n",
        "    '''\n",
        "    Performs the photometric distortions defined by the `train_transform_param` instructions\n",
        "    of the original Caffe implementation of SSD.\n",
        "    '''\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        self.convert_RGB_to_HSV = ConvertColor(current='RGB', to='HSV')\n",
        "        self.convert_HSV_to_RGB = ConvertColor(current='HSV', to='RGB')\n",
        "        self.convert_to_float32 = ConvertDataType(to='float32')\n",
        "        self.convert_to_uint8 = ConvertDataType(to='uint8')\n",
        "        self.convert_to_3_channels = ConvertTo3Channels()\n",
        "        self.random_brightness = RandomBrightness(lower=-32, upper=32, prob=0.5)\n",
        "        self.random_contrast = RandomContrast(lower=0.5, upper=1.5, prob=0.5)\n",
        "        self.random_saturation = RandomSaturation(lower=0.5, upper=1.5, prob=0.5)\n",
        "        self.random_hue = RandomHue(max_delta=18, prob=0.5)\n",
        "        self.random_channel_swap = RandomChannelSwap(prob=0.0)\n",
        "\n",
        "        self.sequence1 = [self.convert_to_3_channels,\n",
        "                          self.convert_to_float32,\n",
        "                          self.random_brightness,\n",
        "                          self.random_contrast,\n",
        "                          self.convert_to_uint8,\n",
        "                          self.convert_RGB_to_HSV,\n",
        "                          self.convert_to_float32,\n",
        "                          self.random_saturation,\n",
        "                          self.random_hue,\n",
        "                          self.convert_to_uint8,\n",
        "                          self.convert_HSV_to_RGB,\n",
        "                          self.random_channel_swap]\n",
        "\n",
        "        self.sequence2 = [self.convert_to_3_channels,\n",
        "                          self.convert_to_float32,\n",
        "                          self.random_brightness,\n",
        "                          self.convert_to_uint8,\n",
        "                          self.convert_RGB_to_HSV,\n",
        "                          self.convert_to_float32,\n",
        "                          self.random_saturation,\n",
        "                          self.random_hue,\n",
        "                          self.convert_to_uint8,\n",
        "                          self.convert_HSV_to_RGB,\n",
        "                          self.convert_to_float32,\n",
        "                          self.random_contrast,\n",
        "                          self.convert_to_uint8,\n",
        "                          self.random_channel_swap]\n",
        "\n",
        "    def __call__(self, image, labels):\n",
        "\n",
        "        # Choose sequence 1 with probability 0.5.\n",
        "        if np.random.choice(2):\n",
        "\n",
        "            for transform in self.sequence1:\n",
        "                image, labels = transform(image, labels)\n",
        "            return image, labels\n",
        "        # Choose sequence 2 with probability 0.5.\n",
        "        else:\n",
        "\n",
        "            for transform in self.sequence2:\n",
        "                image, labels = transform(image, labels)\n",
        "            return image, labels\n",
        "\n",
        "class SSDDataAugmentation:\n",
        "    '''\n",
        "    Reproduces the data augmentation pipeline used in the training of the original\n",
        "    Caffe implementation of SSD.\n",
        "    '''\n",
        "\n",
        "    def __init__(self,\n",
        "                 img_height=300,\n",
        "                 img_width=300,\n",
        "                 background=(123, 117, 104),\n",
        "                 labels_format={'class_id': 0, 'xmin': 1, 'ymin': 2, 'xmax': 3, 'ymax': 4}):\n",
        "        '''\n",
        "        Arguments:\n",
        "            height (int): The desired height of the output images in pixels.\n",
        "            width (int): The desired width of the output images in pixels.\n",
        "            background (list/tuple, optional): A 3-tuple specifying the RGB color value of the\n",
        "                background pixels of the translated images.\n",
        "            labels_format (dict, optional): A dictionary that defines which index in the last axis of the labels\n",
        "                of an image contains which bounding box coordinate. The dictionary maps at least the keywords\n",
        "                'xmin', 'ymin', 'xmax', and 'ymax' to their respective indices within last axis of the labels array.\n",
        "        '''\n",
        "\n",
        "        self.labels_format = labels_format\n",
        "\n",
        "        self.photometric_distortions = SSDPhotometricDistortions()\n",
        "        self.expand = SSDExpand(background=background, labels_format=self.labels_format)\n",
        "        self.random_crop = SSDRandomCrop(labels_format=self.labels_format)\n",
        "        self.random_flip = RandomFlip(dim='horizontal', prob=0.5, labels_format=self.labels_format)\n",
        "\n",
        "        # This box filter makes sure that the resized images don't contain any degenerate boxes.\n",
        "        # Resizing the images could lead the boxes to becomes smaller. For boxes that are already\n",
        "        # pretty small, that might result in boxes with height and/or width zero, which we obviously\n",
        "        # cannot allow.\n",
        "        self.box_filter = BoxFilter(check_overlap=False,\n",
        "                                    check_min_area=False,\n",
        "                                    check_degenerate=True,\n",
        "                                    labels_format=self.labels_format)\n",
        "\n",
        "        self.resize = ResizeRandomInterp(height=img_height,\n",
        "                                         width=img_width,\n",
        "                                         interpolation_modes=[cv2.INTER_NEAREST,\n",
        "                                                              cv2.INTER_LINEAR,\n",
        "                                                              cv2.INTER_CUBIC,\n",
        "                                                              cv2.INTER_AREA,\n",
        "                                                              cv2.INTER_LANCZOS4],\n",
        "                                         box_filter=self.box_filter,\n",
        "                                         labels_format=self.labels_format)\n",
        "\n",
        "        self.sequence = [self.photometric_distortions,\n",
        "                         self.expand,\n",
        "                         self.random_crop,\n",
        "                         self.random_flip,\n",
        "                         self.resize]\n",
        "\n",
        "    def __call__(self, image, labels, return_inverter=False):\n",
        "        self.expand.labels_format = self.labels_format\n",
        "        self.random_crop.labels_format = self.labels_format\n",
        "        self.random_flip.labels_format = self.labels_format\n",
        "        self.resize.labels_format = self.labels_format\n",
        "\n",
        "        inverters = []\n",
        "\n",
        "        for transform in self.sequence:\n",
        "            if return_inverter and ('return_inverter' in inspect.signature(transform).parameters):\n",
        "                image, labels, inverter = transform(image, labels, return_inverter=True)\n",
        "                inverters.append(inverter)\n",
        "                print(\"SSDDataAugmentation...............\")\n",
        "            else:\n",
        "                image, labels = transform(image, labels)\n",
        "\n",
        "        if return_inverter:\n",
        "            return image, labels, inverters[::-1]\n",
        "        else:\n",
        "            return image, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdBXGZocEesH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Miscellaneous data generator utilities.\n",
        "Copyright (C) 2018 Pierluigi Ferrari\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "   http://www.apache.org/licenses/LICENSE-2.0\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "'''\n",
        "\n",
        "from __future__ import division\n",
        "import numpy as np\n",
        "\n",
        "def apply_inverse_transforms(y_pred_decoded, inverse_transforms):\n",
        "    '''\n",
        "    Takes a list or Numpy array of decoded predictions and applies a given list of\n",
        "    transforms to them. The list of inverse transforms would usually contain the\n",
        "    inverter functions that some of the image transformations that come with this\n",
        "    data generator return. This function would normally be used to transform predictions\n",
        "    that were made on a transformed image back to the original image.\n",
        "    Arguments:\n",
        "        y_pred_decoded (list or array): Either a list of length `batch_size` that\n",
        "            contains Numpy arrays that contain the predictions for each batch item\n",
        "            or a Numpy array. If this is a list of Numpy arrays, the arrays would\n",
        "            usually have the shape `(num_predictions, 6)`, where `num_predictions`\n",
        "            is different for each batch item. If this is a Numpy array, it would\n",
        "            usually have the shape `(batch_size, num_predictions, 6)`. The last axis\n",
        "            would usually contain the class ID, confidence score, and four bounding\n",
        "            box coordinates for each prediction.\n",
        "        inverse_predictions (list): A nested list of length `batch_size` that contains\n",
        "            for each batch item a list of functions that take one argument (one element\n",
        "            of `y_pred_decoded` if it is a list or one slice along the first axis of\n",
        "            `y_pred_decoded` if it is an array) and return an output of the same shape\n",
        "            and data type.\n",
        "    Returns:\n",
        "        The transformed predictions, which have the same structure as `y_pred_decoded`.\n",
        "    '''\n",
        "\n",
        "    if isinstance(y_pred_decoded, list):\n",
        "\n",
        "        y_pred_decoded_inv = []\n",
        "\n",
        "        for i in range(len(y_pred_decoded)):\n",
        "            y_pred_decoded_inv.append(np.copy(y_pred_decoded[i]))\n",
        "            if y_pred_decoded_inv[i].size > 0: # If there are any predictions for this batch item.\n",
        "                for inverter in inverse_transforms[i]:\n",
        "                    if not (inverter is None):\n",
        "                        y_pred_decoded_inv[i] = inverter(y_pred_decoded_inv[i])\n",
        "\n",
        "    elif isinstance(y_pred_decoded, np.ndarray):\n",
        "\n",
        "        y_pred_decoded_inv = np.copy(y_pred_decoded)\n",
        "\n",
        "        for i in range(len(y_pred_decoded)):\n",
        "            if y_pred_decoded_inv[i].size > 0: # If there are any predictions for this batch item.\n",
        "                for inverter in inverse_transforms[i]:\n",
        "                    if not (inverter is None):\n",
        "                        y_pred_decoded_inv[i] = inverter(y_pred_decoded_inv[i])\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"`y_pred_decoded` must be either a list or a Numpy array.\")\n",
        "\n",
        "    return y_pred_decoded_inv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66oNACGcXB_h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "The Keras-compatible loss function for the SSD model. Currently supports TensorFlow only.\n",
        "Copyright (C) 2018 Pierluigi Ferrari\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "   http://www.apache.org/licenses/LICENSE-2.0\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "'''\n",
        "\n",
        "from __future__ import division\n",
        "import tensorflow as tf\n",
        "\n",
        "class SSDLoss:\n",
        "    '''\n",
        "    The SSD loss, see https://arxiv.org/abs/1512.02325.\n",
        "    '''\n",
        "\n",
        "    def __init__(self,\n",
        "                 neg_pos_ratio=3,\n",
        "                 n_neg_min=0,\n",
        "                 alpha=1.0):\n",
        "        '''\n",
        "        Arguments:\n",
        "            neg_pos_ratio (int, optional): The maximum ratio of negative (i.e. background)\n",
        "                to positive ground truth boxes to include in the loss computation.\n",
        "                There are no actual background ground truth boxes of course, but `y_true`\n",
        "                contains anchor boxes labeled with the background class. Since\n",
        "                the number of background boxes in `y_true` will usually exceed\n",
        "                the number of positive boxes by far, it is necessary to balance\n",
        "                their influence on the loss. Defaults to 3 following the paper.\n",
        "            n_neg_min (int, optional): The minimum number of negative ground truth boxes to\n",
        "                enter the loss computation *per batch*. This argument can be used to make\n",
        "                sure that the model learns from a minimum number of negatives in batches\n",
        "                in which there are very few, or even none at all, positive ground truth\n",
        "                boxes. It defaults to 0 and if used, it should be set to a value that\n",
        "                stands in reasonable proportion to the batch size used for training.\n",
        "            alpha (float, optional): A factor to weight the localization loss in the\n",
        "                computation of the total loss. Defaults to 1.0 following the paper.\n",
        "        '''\n",
        "        self.neg_pos_ratio = neg_pos_ratio\n",
        "        self.n_neg_min = n_neg_min\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def smooth_L1_loss(self, y_true, y_pred):\n",
        "        '''\n",
        "        Compute smooth L1 loss, see references.\n",
        "        Arguments:\n",
        "            y_true (nD tensor): A TensorFlow tensor of any shape containing the ground truth data.\n",
        "                In this context, the expected tensor has shape `(batch_size, #boxes, 4)` and\n",
        "                contains the ground truth bounding box coordinates, where the last dimension\n",
        "                contains `(xmin, xmax, ymin, ymax)`.\n",
        "            y_pred (nD tensor): A TensorFlow tensor of identical structure to `y_true` containing\n",
        "                the predicted data, in this context the predicted bounding box coordinates.\n",
        "        Returns:\n",
        "            The smooth L1 loss, a nD-1 Tensorflow tensor. In this context a 2D tensor\n",
        "            of shape (batch, n_boxes_total).\n",
        "        References:\n",
        "            https://arxiv.org/abs/1504.08083\n",
        "        '''\n",
        "        absolute_loss = tf.abs(y_true - y_pred)\n",
        "        square_loss = 0.5 * (y_true - y_pred)**2\n",
        "        l1_loss = tf.where(tf.less(absolute_loss, 1.0), square_loss, absolute_loss - 0.5)\n",
        "        return tf.reduce_sum(l1_loss, axis=-1)\n",
        "\n",
        "    def log_loss(self, y_true, y_pred):\n",
        "        '''\n",
        "        Compute the softmax log loss.\n",
        "        Arguments:\n",
        "            y_true (nD tensor): A TensorFlow tensor of any shape containing the ground truth data.\n",
        "                In this context, the expected tensor has shape (batch_size, #boxes, #classes)\n",
        "                and contains the ground truth bounding box categories.\n",
        "            y_pred (nD tensor): A TensorFlow tensor of identical structure to `y_true` containing\n",
        "                the predicted data, in this context the predicted bounding box categories.\n",
        "        Returns:\n",
        "            The softmax log loss, a nD-1 Tensorflow tensor. In this context a 2D tensor\n",
        "            of shape (batch, n_boxes_total).\n",
        "        '''\n",
        "        # Make sure that `y_pred` doesn't contain any zeros (which would break the log function)\n",
        "        y_pred = tf.maximum(y_pred, 1e-15)\n",
        "        # Compute the log loss\n",
        "        log_loss = -tf.reduce_sum(y_true * tf.log(y_pred), axis=-1)\n",
        "        return log_loss\n",
        "\n",
        "    def compute_loss(self, y_true, y_pred):\n",
        "        '''\n",
        "        Compute the loss of the SSD model prediction against the ground truth.\n",
        "        Arguments:\n",
        "            y_true (array): A Numpy array of shape `(batch_size, #boxes, #classes + 12)`,\n",
        "                where `#boxes` is the total number of boxes that the model predicts\n",
        "                per image. Be careful to make sure that the index of each given\n",
        "                box in `y_true` is the same as the index for the corresponding\n",
        "                box in `y_pred`. The last axis must have length `#classes + 12` and contain\n",
        "                `[classes one-hot encoded, 4 ground truth box coordinate offsets, 8 arbitrary entries]`\n",
        "                in this order, including the background class. The last eight entries of the\n",
        "                last axis are not used by this function and therefore their contents are\n",
        "                irrelevant, they only exist so that `y_true` has the same shape as `y_pred`,\n",
        "                where the last four entries of the last axis contain the anchor box\n",
        "                coordinates, which are needed during inference. Important: Boxes that\n",
        "                you want the cost function to ignore need to have a one-hot\n",
        "                class vector of all zeros.\n",
        "            y_pred (Keras tensor): The model prediction. The shape is identical\n",
        "                to that of `y_true`, i.e. `(batch_size, #boxes, #classes + 12)`.\n",
        "                The last axis must contain entries in the format\n",
        "                `[classes one-hot encoded, 4 predicted box coordinate offsets, 8 arbitrary entries]`.\n",
        "        Returns:\n",
        "            A scalar, the total multitask loss for classification and localization.\n",
        "        '''\n",
        "        self.neg_pos_ratio = tf.constant(self.neg_pos_ratio)\n",
        "        self.n_neg_min = tf.constant(self.n_neg_min)\n",
        "        self.alpha = tf.constant(self.alpha)\n",
        "\n",
        "        batch_size = tf.shape(y_pred)[0] # Output dtype: tf.int32\n",
        "        n_boxes = tf.shape(y_pred)[1] # Output dtype: tf.int32, note that `n_boxes` in this context denotes the total number of boxes per image, not the number of boxes per cell.\n",
        "\n",
        "        # 1: Compute the losses for class and box predictions for every box.\n",
        "\n",
        "        classification_loss = tf.to_float(self.log_loss(y_true[:,:,:-12], y_pred[:,:,:-12])) # Output shape: (batch_size, n_boxes)\n",
        "        localization_loss = tf.to_float(self.smooth_L1_loss(y_true[:,:,-12:-8], y_pred[:,:,-12:-8])) # Output shape: (batch_size, n_boxes)\n",
        "\n",
        "        # 2: Compute the classification losses for the positive and negative targets.\n",
        "\n",
        "        # Create masks for the positive and negative ground truth classes.\n",
        "        negatives = y_true[:,:,0] # Tensor of shape (batch_size, n_boxes)\n",
        "        positives = tf.to_float(tf.reduce_max(y_true[:,:,1:-12], axis=-1)) # Tensor of shape (batch_size, n_boxes)\n",
        "\n",
        "        # Count the number of positive boxes (classes 1 to n) in y_true across the whole batch.\n",
        "        n_positive = tf.reduce_sum(positives)\n",
        "\n",
        "        # Now mask all negative boxes and sum up the losses for the positive boxes PER batch item\n",
        "        # (Keras loss functions must output one scalar loss value PER batch item, rather than just\n",
        "        # one scalar for the entire batch, that's why we're not summing across all axes).\n",
        "        pos_class_loss = tf.reduce_sum(classification_loss * positives, axis=-1) # Tensor of shape (batch_size,)\n",
        "\n",
        "        # Compute the classification loss for the negative default boxes (if there are any).\n",
        "\n",
        "        # First, compute the classification loss for all negative boxes.\n",
        "        neg_class_loss_all = classification_loss * negatives # Tensor of shape (batch_size, n_boxes)\n",
        "        n_neg_losses = tf.count_nonzero(neg_class_loss_all, dtype=tf.int32) # The number of non-zero loss entries in `neg_class_loss_all`\n",
        "        # What's the point of `n_neg_losses`? For the next step, which will be to compute which negative boxes enter the classification\n",
        "        # loss, we don't just want to know how many negative ground truth boxes there are, but for how many of those there actually is\n",
        "        # a positive (i.e. non-zero) loss. This is necessary because `tf.nn.top-k()` in the function below will pick the top k boxes with\n",
        "        # the highest losses no matter what, even if it receives a vector where all losses are zero. In the unlikely event that all negative\n",
        "        # classification losses ARE actually zero though, this behavior might lead to `tf.nn.top-k()` returning the indices of positive\n",
        "        # boxes, leading to an incorrect negative classification loss computation, and hence an incorrect overall loss computation.\n",
        "        # We therefore need to make sure that `n_negative_keep`, which assumes the role of the `k` argument in `tf.nn.top-k()`,\n",
        "        # is at most the number of negative boxes for which there is a positive classification loss.\n",
        "\n",
        "        # Compute the number of negative examples we want to account for in the loss.\n",
        "        # We'll keep at most `self.neg_pos_ratio` times the number of positives in `y_true`, but at least `self.n_neg_min` (unless `n_neg_loses` is smaller).\n",
        "        n_negative_keep = tf.minimum(tf.maximum(self.neg_pos_ratio * tf.to_int32(n_positive), self.n_neg_min), n_neg_losses)\n",
        "\n",
        "        # In the unlikely case when either (1) there are no negative ground truth boxes at all\n",
        "        # or (2) the classification loss for all negative boxes is zero, return zero as the `neg_class_loss`.\n",
        "        def f1():\n",
        "            return tf.zeros([batch_size])\n",
        "        # Otherwise compute the negative loss.\n",
        "        def f2():\n",
        "            # Now we'll identify the top-k (where k == `n_negative_keep`) boxes with the highest confidence loss that\n",
        "            # belong to the background class in the ground truth data. Note that this doesn't necessarily mean that the model\n",
        "            # predicted the wrong class for those boxes, it just means that the loss for those boxes is the highest.\n",
        "\n",
        "            # To do this, we reshape `neg_class_loss_all` to 1D...\n",
        "            neg_class_loss_all_1D = tf.reshape(neg_class_loss_all, [-1]) # Tensor of shape (batch_size * n_boxes,)\n",
        "            # ...and then we get the indices for the `n_negative_keep` boxes with the highest loss out of those...\n",
        "            values, indices = tf.nn.top_k(neg_class_loss_all_1D,\n",
        "                                          k=n_negative_keep,\n",
        "                                          sorted=False) # We don't need them sorted.\n",
        "            # ...and with these indices we'll create a mask...\n",
        "            negatives_keep = tf.scatter_nd(indices=tf.expand_dims(indices, axis=1),\n",
        "                                           updates=tf.ones_like(indices, dtype=tf.int32),\n",
        "                                           shape=tf.shape(neg_class_loss_all_1D)) # Tensor of shape (batch_size * n_boxes,)\n",
        "            negatives_keep = tf.to_float(tf.reshape(negatives_keep, [batch_size, n_boxes])) # Tensor of shape (batch_size, n_boxes)\n",
        "            # ...and use it to keep only those boxes and mask all other classification losses\n",
        "            neg_class_loss = tf.reduce_sum(classification_loss * negatives_keep, axis=-1) # Tensor of shape (batch_size,)\n",
        "            return neg_class_loss\n",
        "\n",
        "        neg_class_loss = tf.cond(tf.equal(n_neg_losses, tf.constant(0)), f1, f2)\n",
        "\n",
        "        class_loss = pos_class_loss + neg_class_loss # Tensor of shape (batch_size,)\n",
        "\n",
        "        # 3: Compute the localization loss for the positive targets.\n",
        "        #    We don't compute a localization loss for negative predicted boxes (obviously: there are no ground truth boxes they would correspond to).\n",
        "\n",
        "        loc_loss = tf.reduce_sum(localization_loss * positives, axis=-1) # Tensor of shape (batch_size,)\n",
        "\n",
        "        # 4: Compute the total loss.\n",
        "\n",
        "        total_loss = (class_loss + self.alpha * loc_loss) / tf.maximum(1.0, n_positive) # In case `n_positive == 0`\n",
        "        # Keras has the annoying habit of dividing the loss by the batch size, which sucks in our case\n",
        "        # because the relevant criterion to average our loss over is the number of positive boxes in the batch\n",
        "        # (by which we're dividing in the line above), not the batch size. So in order to revert Keras' averaging\n",
        "        # over the batch size, we'll have to multiply by it.\n",
        "        total_loss = total_loss * tf.to_float(batch_size)\n",
        "\n",
        "        return total_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6OSY54KLAdv",
        "colab_type": "text"
      },
      "source": [
        "1. Set the model configuration parameters\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCK58x5jEeqK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_height = 300 # Height of the model input images\n",
        "img_width = 300 # Width of the model input images\n",
        "img_channels = 3 # Number of color channels of the model input images\n",
        "mean_color = [123, 117, 104] # The per-channel mean of the images in the dataset. Do not change this value if you're using any of the pre-trained weights.\n",
        "swap_channels = [2, 1, 0] # The color channel order in the original SSD is BGR, so we'll have the model reverse the color channel order of the input images.\n",
        "n_classes = 3 #12 #20 # Number of positive classes, e.g. 20 for Pascal VOC, 80 for MS COCO\n",
        "scales_pascal = [0.1, 0.2, 0.37, 0.54, 0.71, 0.88, 1.05] # The anchor box scaling factors used in the original SSD300 for the Pascal VOC datasets\n",
        "scales_coco = [0.07, 0.15, 0.33, 0.51, 0.69, 0.87, 1.05] # The anchor box scaling factors used in the original SSD300 for the MS COCO datasets\n",
        "scales = scales_pascal\n",
        "aspect_ratios = [[1.0, 2.0, 0.5],\n",
        "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
        "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
        "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
        "                 [1.0, 2.0, 0.5],\n",
        "                 [1.0, 2.0, 0.5]] # The anchor box aspect ratios used in the original SSD300; the order matters\n",
        "two_boxes_for_ar1 = True\n",
        "steps = [8, 16, 32, 64, 100, 300] # The space between two adjacent anchor box center points for each predictor layer.\n",
        "offsets = [0.5, 0.5, 0.5, 0.5, 0.5, 0.5] # The offsets of the first anchor box center points from the top and left borders of the image as a fraction of the step size for each predictor layer.\n",
        "clip_boxes = False # Whether or not to clip the anchor boxes to lie entirely within the image boundaries\n",
        "variances = [0.1, 0.1, 0.2, 0.2] # The variances by which the encoded target coordinates are divided as in the original implementation\n",
        "normalize_coords = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KLxekJtV-Ec",
        "colab_type": "code",
        "outputId": "e86d3cfc-1de5-4df7-df30-700cd4408199",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# 1: Build the Keras model.\n",
        "\n",
        "K.clear_session() # Clear previous models from memory.\n",
        "\n",
        "model = ssd_300(image_size=(img_height, img_width, img_channels),\n",
        "                n_classes=n_classes,\n",
        "                mode='training',\n",
        "                l2_regularization=0.0005,\n",
        "                scales=scales,\n",
        "                aspect_ratios_per_layer=aspect_ratios,\n",
        "                two_boxes_for_ar1=two_boxes_for_ar1,\n",
        "                steps=steps,\n",
        "                offsets=offsets,\n",
        "                clip_boxes=clip_boxes,\n",
        "                variances=variances,\n",
        "                normalize_coords=normalize_coords,\n",
        "                subtract_mean=mean_color,\n",
        "                swap_channels=swap_channels)\n",
        "\n",
        "model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.engine.training.Model at 0x7f5860d10da0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PCtRt7TWkg6",
        "colab_type": "code",
        "outputId": "914deb5f-1283-4e9c-8337-f72dc6b9b9a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "# 2: Load some weights into the model.\n",
        "\n",
        "# TODO: Set the path to the weights you want to load.\n",
        "weights_path = '/content/drive/My Drive/Capstone project/SSD/pierliguiferrari/VGG_ILSVRC_16_layers_fc_reduced.h5'\n",
        "\n",
        "model.load_weights(weights_path, by_name=True)\n",
        "model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.engine.training.Model at 0x7f5860d10da0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1vXh4EuEenK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 3: Instantiate an optimizer and the SSD loss function and compile the model.\n",
        "#    If you want to follow the original Caffe implementation, use the preset SGD\n",
        "#    optimizer, otherwise I'd recommend the commented-out Adam optimizer.\n",
        "from keras import metrics\n",
        "\n",
        "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
        "#sgd = SGD(lr=0.001, momentum=0.9, decay=0.0, nesterov=False)\n",
        "\n",
        "ssd_loss = SSDLoss(neg_pos_ratio=3, alpha=1.0)\n",
        "\n",
        "#model.compile(optimizer=sgd, loss=ssd_loss.compute_loss)\n",
        "#model.compile(optimizer=adam, loss=ssd_loss.compute_loss)\n",
        "model.compile(optimizer=adam, loss=ssd_loss.compute_loss,metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2z1CUrM8Eekx",
        "colab_type": "code",
        "outputId": "0c3ac334-e76a-4652-d1ef-01247c87e67f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 1: Instantiate two `DataGenerator` objects: One for training, one for validation.\n",
        "\n",
        "# Optional: If you have enough memory, consider loading the images into memory for the reasons explained above.\n",
        "\n",
        "train_dataset = DataGenerator(load_images_into_memory=False, hdf5_dataset_path=None)\n",
        "val_dataset = DataGenerator(load_images_into_memory=False, hdf5_dataset_path=None)\n",
        "test_dataset = DataGenerator(load_images_into_memory=False, hdf5_dataset_path=None)\n",
        "\n",
        "# 2: Parse the image and label lists for the training and validation datasets. This can take a while.\n",
        "\n",
        "# TODO: Set the paths to the datasets here.\n",
        "images_dir_train = '/content/drive/My Drive/Capstone project/SSD/Training'\n",
        "images_dir_val = '/content/drive/My Drive/Capstone project/SSD/Validation'\n",
        "images_dir_test = '/content/drive/My Drive/Capstone project/SSD/Testing'\n",
        "\n",
        "# the below is wrong one..gets into error\n",
        "#train_labels_filename = '/content/drive/My Drive/Capstone project/SSD/pierliguiferrari/df_training.csv'\n",
        "#val_labels_filename   = '/content/drive/My Drive/Capstone project/SSD/pierliguiferrari/df_validation.csv'\n",
        "\n",
        "# the below has 12 class ids\n",
        "#train_labels_filename = '/content/drive/My Drive/Capstone project/SSD/pierliguiferrari/df_training_EDITED.csv'\n",
        "#val_labels_filename   = '/content/drive/My Drive/Capstone project/SSD/pierliguiferrari/df_validation_EDITED.csv'\n",
        "\n",
        "# the below has three class ids only : 1,2,3 only\n",
        "train_labels_filename = '/content/drive/My Drive/Capstone project/SSD/pierliguiferrari/Trial/df_training_trial.csv'\n",
        "val_labels_filename   = '/content/drive/My Drive/Capstone project/SSD/pierliguiferrari/Trial/df_validation_trial.csv'\n",
        "test_labels_filename   = '/content/drive/My Drive/Capstone project/SSD/pierliguiferrari/Trial/df_testing_trial.csv'\n",
        "\n",
        "\n",
        "# The XML parser needs to now what object class names to look for and in which order to map them to integers.\n",
        "\"\"\"classes = ['Abstract_Dot_Off-the-Shoulder_Top', 'Abstract_Floral_Fringe_Crop_Top', 'Abstract_Floral_Print_Poncho', 'Abstract-Plaid_Ruffled_Bell_Sleeve_Top',\n",
        "             'Abstract-Quilted_Drawstring_Hoodie','Acid_Wash_Jeggings', 'Asymmetrical_Open-Front_Blazer', 'Button-Tab_Ankle_Chinos', 'Tile_Print_Drawstring_Caftan', 'Tonal_Plaid_Robe',\n",
        "               'Topstitched_Jodhpurs', 'Zippered_Sleeveless_Hoodie'] \"\"\"\n",
        "\n",
        "\n",
        "classes = ['TOP','BOTTOM','FULL DRESS'] # 1 is Top, 2 is Bottom, 3 is Full dress\n",
        "\n",
        "train_dataset.parse_csv(images_dir=images_dir_train,\n",
        "                        labels_filename=train_labels_filename,\n",
        "                        input_format=['image_name', 'xmin', 'xmax', 'ymin', 'ymax', 'class_id'], # This is the order of the first six columns in the CSV file that contains the labels for your dataset. If your labels are in XML format, maybe the XML parser will be helpful, check the documentation.\n",
        "                        include_classes='all')\n",
        "\n",
        "\n",
        "val_dataset.parse_csv(images_dir=images_dir_val,\n",
        "                      labels_filename=val_labels_filename,\n",
        "                      input_format=['image_name', 'xmin', 'xmax', 'ymin', 'ymax', 'class_id'],\n",
        "                      include_classes='all')\n",
        "\n",
        "\n",
        "test_dataset.parse_csv(images_dir=images_dir_test,\n",
        "                      labels_filename=test_labels_filename,\n",
        "                      input_format=['image_name', 'xmin', 'xmax', 'ymin', 'ymax', 'class_id'],\n",
        "                      include_classes='all')\n",
        "\n",
        "# Get the number of samples in the training and validations datasets.\n",
        "train_dataset_size = train_dataset.get_dataset_size()\n",
        "val_dataset_size   = val_dataset.get_dataset_size()\n",
        "test_dataset_size   = test_dataset.get_dataset_size()\n",
        "\n",
        "print(\"Number of images in the training dataset:\\t{:>6}\".format(train_dataset_size))\n",
        "print(\"Number of images in the validation dataset:\\t{:>6}\".format(val_dataset_size))\n",
        "print(\"Number of images in the testing dataset:\\t{:>6}\".format(test_dataset_size))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INSIDE PARSE_CSV..............\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000001.jpg', '19', '76', '193', '300', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000003.jpg', '28', '47', '164', '227', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000005.jpg', '76', '49', '240', '293', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000007.jpg', '53', '13', '269', '253', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000008.jpg', '20', '46', '164', '261', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000009.jpg', '9', '41', '156', '282', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000010.jpg', '1', '17', '240', '282', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000013.jpg', '80', '50', '225', '235', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000014.jpg', '68', '46', '254', '288', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000015.jpg', '42', '54', '188', '259', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000016.jpg', '34', '60', '222', '299', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000020.jpg', '30', '1', '281', '287', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000021.jpg', '45', '53', '184', '243', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000022.jpg', '34', '68', '178', '278', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000023.jpg', '15', '14', '189', '246', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000024.jpg', '89', '45', '216', '187', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000027.jpg', '170', '48', '300', '300', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000028.jpg', '26', '69', '213', '300', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000029.jpg', '46', '59', '219', '275', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000030.jpg', '56', '1', '157', '211', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000031.jpg', '55', '39', '234', '300', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000033.jpg', '1', '1', '225', '300', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000034.jpg', '35', '1', '273', '284', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000035.jpg', '32', '1', '277', '299', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000036.jpg', '16', '55', '196', '259', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000037.jpg', '38', '11', '211', '208', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000038.jpg', '24', '39', '175', '226', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000039.jpg', '26', '48', '139', '209', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000041.jpg', '57', '1', '211', '300', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000042.jpg', '1', '1', '200', '241', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000043.jpg', '17', '1', '228', '272', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000044.jpg', '26', '46', '219', '297', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000045.jpg', '16', '50', '180', '282', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000046.jpg', '23', '52', '173', '292', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000047.jpg', '11', '1', '196', '276', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000048.jpg', '72', '36', '161', '158', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000050.jpg', '47', '27', '190', '217', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000052.jpg', '15', '5', '178', '284', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000053.jpg', '10', '51', '155', '289', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000055.jpg', '1', '1', '179', '242', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000056.jpg', '53', '1', '256', '277', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000057.jpg', '39', '50', '176', '265', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000058.jpg', '33', '1', '285', '300', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000061.jpg', '31', '44', '194', '285', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000062.jpg', '28', '1', '270', '300', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000064.jpg', '28', '49', '173', '284', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000065.jpg', '77', '33', '161', '154', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000067.jpg', '53', '4', '169', '159', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000068.jpg', '2', '8', '192', '271', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000070.jpg', '7', '1', '218', '276', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000071.jpg', '1', '1', '200', '286', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000072.jpg', '17', '48', '175', '242', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000073.jpg', '34', '52', '172', '231', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000074.jpg', '24', '74', '158', '279', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000075.jpg', '69', '31', '177', '250', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000076.jpg', '1', '1', '230', '290', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000078.jpg', '7', '7', '171', '226', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000079.jpg', '1', '20', '170', '276', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000080.jpg', '33', '16', '293', '272', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000081.jpg', '45', '16', '259', '284', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000082.jpg', '54', '60', '197', '249', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000084.jpg', '26', '35', '180', '300', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000086.jpg', '43', '55', '215', '252', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000087.jpg', '1', '11', '225', '255', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000088.jpg', '47', '41', '246', '276', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000091.jpg', '9', '17', '173', '300', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000092.jpg', '15', '54', '217', '289', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000093.jpg', '20', '49', '172', '299', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000094.jpg', '40', '39', '268', '279', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000095.jpg', '46', '1', '156', '165', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000096.jpg', '98', '43', '212', '221', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000098.jpg', '41', '67', '192', '277', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000099.jpg', '1', '1', '204', '252', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000100.jpg', '70', '62', '240', '280', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000101.jpg', '32', '70', '214', '300', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000104.jpg', '81', '51', '271', '282', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000105.jpg', '28', '3', '231', '300', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000107.jpg', '1', '1', '200', '268', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000108.jpg', '25', '68', '185', '276', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000109.jpg', '46', '65', '213', '269', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000110.jpg', '22', '16', '238', '278', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000112.jpg', '88', '47', '213', '237', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000113.jpg', '75', '53', '218', '283', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000114.jpg', '2', '23', '199', '268', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000115.jpg', '99', '108', '213', '229', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000116.jpg', '18', '43', '192', '286', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000117.jpg', '80', '21', '225', '207', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000118.jpg', '30', '60', '152', '231', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000119.jpg', '28', '57', '191', '263', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000120.jpg', '1', '1', '200', '273', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000121.jpg', '69', '34', '149', '153', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000122.jpg', '81', '58', '230', '255', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000123.jpg', '19', '5', '209', '272', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000002.jpg', '19', '42', '201', '272', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000003.jpg', '12', '1', '290', '300', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000004.jpg', '138', '6', '285', '224', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000005.jpg', '25', '39', '219', '281', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000006.jpg', '1', '67', '157', '300', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000007.jpg', '84', '86', '234', '300', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000008.jpg', '24', '63', '140', '231', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000009.jpg', '1', '20', '199', '250', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000010.jpg', '1', '39', '185', '280', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000011.jpg', '47', '74', '196', '281', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000013.jpg', '1', '22', '286', '300', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000014.jpg', '49', '81', '177', '252', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000015.jpg', '166', '71', '287', '234', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000016.jpg', '28', '57', '225', '295', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000018.jpg', '70', '43', '155', '163', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000019.jpg', '1', '27', '155', '221', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000020.jpg', '24', '69', '131', '222', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000021.jpg', '136', '24', '280', '213', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000022.jpg', '67', '28', '198', '226', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000025.jpg', '39', '24', '196', '245', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000026.jpg', '21', '62', '286', '293', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000028.jpg', '124', '23', '213', '123', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000030.jpg', '54', '28', '251', '291', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000031.jpg', '1', '67', '147', '262', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000032.jpg', '70', '78', '221', '238', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000034.jpg', '33', '30', '135', '147', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000037.jpg', '3', '50', '231', '300', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000038.jpg', '27', '9', '231', '295', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000039.jpg', '1', '10', '300', '300', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000040.jpg', '37', '27', '273', '300', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000042.jpg', '26', '9', '248', '273', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000043.jpg', '1', '61', '114', '175', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000044.jpg', '64', '62', '242', '300', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000046.jpg', '40', '88', '194', '234', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000047.jpg', '151', '89', '287', '222', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000048.jpg', '1', '52', '126', '225', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000049.jpg', '22', '51', '216', '297', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000050.jpg', '27', '21', '190', '227', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000051.jpg', '67', '47', '163', '158', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000052.jpg', '59', '58', '198', '277', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000053.jpg', '146', '45', '244', '170', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000056.jpg', '19', '71', '176', '242', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000057.jpg', '1', '25', '180', '282', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000058.jpg', '70', '78', '156', '183', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000059.jpg', '1', '47', '95', '168', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000060.jpg', '69', '52', '182', '163', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000062.jpg', '65', '32', '238', '271', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000001.jpg', '31', '76', '104', '176', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000002.jpg', '119', '78', '210', '192', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000003.jpg', '80', '101', '230', '291', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000005.jpg', '25', '85', '175', '230', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000006.jpg', '19', '115', '173', '220', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000007.jpg', '15', '99', '204', '296', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000009.jpg', '17', '75', '181', '269', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000010.jpg', '29', '91', '183', '276', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000011.jpg', '64', '83', '239', '262', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000012.jpg', '15', '91', '176', '262', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000013.jpg', '59', '47', '151', '198', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000014.jpg', '29', '76', '187', '244', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000016.jpg', '54', '53', '164', '226', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000018.jpg', '8', '85', '180', '196', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000020.jpg', '61', '15', '233', '287', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000023.jpg', '36', '90', '202', '274', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000024.jpg', '22', '94', '207', '249', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000026.jpg', '66', '74', '148', '271', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000027.jpg', '52', '40', '182', '218', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000029.jpg', '69', '73', '225', '278', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000030.jpg', '1', '93', '178', '239', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000031.jpg', '19', '55', '150', '285', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000033.jpg', '116', '45', '169', '130', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000034.jpg', '39', '97', '173', '259', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000035.jpg', '74', '81', '230', '205', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000036.jpg', '36', '60', '144', '189', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000039.jpg', '8', '89', '172', '285', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000040.jpg', '2', '100', '192', '227', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000041.jpg', '16', '77', '154', '256', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000043.jpg', '65', '76', '251', '273', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000044.jpg', '1', '71', '142', '192', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Quilted_Drawstring_Hoodie/img_00000001.jpg', '25', '69', '189', '287', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Quilted_Drawstring_Hoodie/img_00000002.jpg', '46', '73', '189', '251', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Quilted_Drawstring_Hoodie/img_00000003.jpg', '26', '63', '178', '188', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Quilted_Drawstring_Hoodie/img_00000004.jpg', '47', '51', '133', '175', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Quilted_Drawstring_Hoodie/img_00000005.jpg', '59', '57', '186', '198', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Quilted_Drawstring_Hoodie/img_00000008.jpg', '41', '75', '205', '297', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Quilted_Drawstring_Hoodie/img_00000009.jpg', '88', '53', '186', '259', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Quilted_Drawstring_Hoodie/img_00000010.jpg', '10', '40', '132', '202', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Quilted_Drawstring_Hoodie/img_00000012.jpg', '8', '64', '202', '300', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Quilted_Drawstring_Hoodie/img_00000013.jpg', '19', '40', '133', '176', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Quilted_Drawstring_Hoodie/img_00000015.jpg', '9', '16', '190', '244', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Quilted_Drawstring_Hoodie/img_00000017.jpg', '35', '59', '160', '250', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Quilted_Drawstring_Hoodie/img_00000020.jpg', '26', '42', '160', '205', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Quilted_Drawstring_Hoodie/img_00000022.jpg', '35', '61', '169', '280', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Quilted_Drawstring_Hoodie/img_00000024.jpg', '61', '78', '213', '300', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Quilted_Drawstring_Hoodie/img_00000025.jpg', '34', '48', '162', '228', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Quilted_Drawstring_Hoodie/img_00000028.jpg', '19', '64', '183', '254', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Quilted_Drawstring_Hoodie/img_00000029.jpg', '6', '52', '152', '227', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Quilted_Drawstring_Hoodie/img_00000030.jpg', '85', '45', '248', '280', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Quilted_Drawstring_Hoodie/img_00000032.jpg', '10', '45', '138', '200', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Quilted_Drawstring_Hoodie/img_00000033.jpg', '17', '42', '132', '200', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Quilted_Drawstring_Hoodie/img_00000034.jpg', '26', '57', '176', '237', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Quilted_Drawstring_Hoodie/img_00000036.jpg', '17', '61', '185', '270', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Quilted_Drawstring_Hoodie/img_00000037.jpg', '19', '41', '128', '200', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Quilted_Drawstring_Hoodie/img_00000039.jpg', '8', '65', '174', '285', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Quilted_Drawstring_Hoodie/img_00000041.jpg', '14', '70', '201', '300', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Quilted_Drawstring_Hoodie/img_00000042.jpg', '36', '50', '142', '248', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Quilted_Drawstring_Hoodie/img_00000044.jpg', '39', '65', '196', '272', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Quilted_Drawstring_Hoodie/img_00000045.jpg', '14', '55', '120', '221', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Quilted_Drawstring_Hoodie/img_00000046.jpg', '12', '71', '193', '281', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Dot_Off-the-Shoulder_Top/img_00000003.jpg', '20', '86', '151', '281', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Dot_Off-the-Shoulder_Top/img_00000004.jpg', '30', '79', '188', '270', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Dot_Off-the-Shoulder_Top/img_00000007.jpg', '35', '76', '165', '172', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Dot_Off-the-Shoulder_Top/img_00000008.jpg', '29', '98', '180', '236', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Dot_Off-the-Shoulder_Top/img_00000009.jpg', '83', '96', '232', '216', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Dot_Off-the-Shoulder_Top/img_00000010.jpg', '35', '79', '160', '251', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Dot_Off-the-Shoulder_Top/img_00000011.jpg', '10', '73', '154', '220', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Dot_Off-the-Shoulder_Top/img_00000012.jpg', '25', '111', '141', '230', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Dot_Off-the-Shoulder_Top/img_00000015.jpg', '37', '93', '137', '157', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Dot_Off-the-Shoulder_Top/img_00000016.jpg', '21', '93', '201', '267', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Dot_Off-the-Shoulder_Top/img_00000017.jpg', '27', '79', '193', '248', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Dot_Off-the-Shoulder_Top/img_00000019.jpg', '16', '112', '189', '256', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Dot_Off-the-Shoulder_Top/img_00000020.jpg', '55', '88', '189', '261', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Dot_Off-the-Shoulder_Top/img_00000021.jpg', '1', '63', '167', '218', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Dot_Off-the-Shoulder_Top/img_00000022.jpg', '107', '84', '225', '271', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Dot_Off-the-Shoulder_Top/img_00000023.jpg', '20', '74', '162', '194', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Dot_Off-the-Shoulder_Top/img_00000024.jpg', '69', '68', '158', '165', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Dot_Off-the-Shoulder_Top/img_00000026.jpg', '111', '67', '161', '264', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Dot_Off-the-Shoulder_Top/img_00000027.jpg', '1', '88', '181', '250', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Dot_Off-the-Shoulder_Top/img_00000028.jpg', '36', '66', '163', '215', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Dot_Off-the-Shoulder_Top/img_00000029.jpg', '15', '80', '200', '271', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Dot_Off-the-Shoulder_Top/img_00000030.jpg', '90', '66', '218', '245', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Dot_Off-the-Shoulder_Top/img_00000032.jpg', '5', '34', '46', '71', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Dot_Off-the-Shoulder_Top/img_00000033.jpg', '16', '101', '177', '280', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Dot_Off-the-Shoulder_Top/img_00000034.jpg', '68', '104', '183', '225', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Dot_Off-the-Shoulder_Top/img_00000035.jpg', '39', '94', '183', '203', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Dot_Off-the-Shoulder_Top/img_00000036.jpg', '45', '114', '181', '177', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Dot_Off-the-Shoulder_Top/img_00000037.jpg', '39', '57', '164', '213', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Dot_Off-the-Shoulder_Top/img_00000038.jpg', '33', '88', '196', '275', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000001.jpg', '86', '62', '150', '127', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000003.jpg', '5', '58', '150', '226', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000004.jpg', '45', '59', '134', '181', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000005.jpg', '50', '55', '145', '117', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000006.jpg', '35', '81', '81', '153', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000008.jpg', '80', '49', '142', '105', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000009.jpg', '53', '55', '135', '133', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000010.jpg', '55', '60', '162', '195', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000011.jpg', '97', '72', '206', '158', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000012.jpg', '68', '92', '153', '212', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000013.jpg', '52', '56', '154', '164', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000014.jpg', '59', '42', '161', '136', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000016.jpg', '48', '96', '161', '183', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000017.jpg', '42', '71', '174', '167', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000018.jpg', '1', '72', '170', '135', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000019.jpg', '51', '54', '111', '117', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000020.jpg', '93', '59', '209', '144', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000021.jpg', '51', '74', '205', '221', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000022.jpg', '101', '72', '193', '152', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000025.jpg', '56', '51', '138', '109', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000026.jpg', '38', '58', '94', '116', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000027.jpg', '31', '67', '182', '290', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000028.jpg', '72', '102', '232', '182', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000031.jpg', '129', '106', '184', '229', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000032.jpg', '48', '79', '139', '198', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000033.jpg', '63', '89', '164', '184', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000034.jpg', '59', '110', '123', '227', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000036.jpg', '60', '68', '130', '130', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000037.jpg', '61', '80', '157', '175', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000038.jpg', '70', '64', '125', '106', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000039.jpg', '57', '85', '145', '185', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000040.jpg', '38', '86', '187', '270', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000042.jpg', '109', '73', '203', '197', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000044.jpg', '1', '68', '205', '300', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000045.jpg', '116', '93', '202', '166', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000047.jpg', '11', '65', '205', '223', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000049.jpg', '21', '83', '189', '250', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000050.jpg', '56', '54', '157', '122', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000051.jpg', '54', '40', '150', '164', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000052.jpg', '54', '65', '139', '116', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000053.jpg', '71', '75', '174', '209', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000054.jpg', '70', '50', '135', '107', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Print_Poncho/img_00000002.jpg', '33', '74', '207', '300', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Print_Poncho/img_00000003.jpg', '37', '76', '191', '245', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Print_Poncho/img_00000004.jpg', '17', '61', '189', '270', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Print_Poncho/img_00000005.jpg', '1', '96', '124', '170', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Print_Poncho/img_00000009.jpg', '41', '39', '209', '217', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Print_Poncho/img_00000010.jpg', '27', '61', '225', '300', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Print_Poncho/img_00000012.jpg', '53', '33', '154', '217', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Print_Poncho/img_00000013.jpg', '97', '32', '206', '245', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Print_Poncho/img_00000015.jpg', '45', '65', '201', '259', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Print_Poncho/img_00000016.jpg', '10', '52', '189', '243', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Print_Poncho/img_00000017.jpg', '31', '63', '240', '300', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Print_Poncho/img_00000018.jpg', '1', '36', '150', '300', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Print_Poncho/img_00000019.jpg', '12', '55', '179', '250', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Print_Poncho/img_00000021.jpg', '27', '83', '230', '252', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Print_Poncho/img_00000022.jpg', '48', '35', '106', '299', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Print_Poncho/img_00000023.jpg', '15', '71', '179', '245', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Print_Poncho/img_00000024.jpg', '13', '83', '239', '258', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Print_Poncho/img_00000025.jpg', '39', '46', '191', '250', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Print_Poncho/img_00000026.jpg', '39', '73', '193', '264', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Print_Poncho/img_00000027.jpg', '35', '78', '210', '296', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Print_Poncho/img_00000028.jpg', '14', '64', '194', '300', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Print_Poncho/img_00000030.jpg', '28', '75', '208', '244', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Print_Poncho/img_00000032.jpg', '61', '53', '204', '249', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Print_Poncho/img_00000033.jpg', '61', '78', '265', '284', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Print_Poncho/img_00000034.jpg', '37', '56', '176', '242', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Print_Poncho/img_00000035.jpg', '22', '91', '183', '287', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Print_Poncho/img_00000036.jpg', '24', '51', '173', '250', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Print_Poncho/img_00000037.jpg', '72', '62', '124', '113', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Print_Poncho/img_00000038.jpg', '40', '63', '190', '300', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000001.jpg', '64', '1', '186', '229', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000003.jpg', '39', '1', '146', '236', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000004.jpg', '49', '1', '185', '279', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000005.jpg', '71', '23', '160', '219', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000006.jpg', '37', '1', '201', '284', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000008.jpg', '32', '1', '160', '255', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000009.jpg', '49', '1', '141', '269', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000011.jpg', '69', '11', '170', '271', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000012.jpg', '39', '1', '159', '276', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000013.jpg', '19', '3', '180', '247', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000015.jpg', '83', '1', '133', '288', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000016.jpg', '92', '1', '213', '270', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000017.jpg', '44', '1', '140', '288', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000018.jpg', '46', '1', '154', '282', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000021.jpg', '43', '1', '207', '255', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000022.jpg', '53', '1', '153', '259', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000023.jpg', '39', '4', '141', '270', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000024.jpg', '54', '1', '149', '239', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000025.jpg', '38', '5', '159', '99', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000027.jpg', '1', '125', '190', '278', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000028.jpg', '65', '1', '175', '256', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000029.jpg', '40', '21', '172', '267', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000030.jpg', '62', '1', '165', '267', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000031.jpg', '77', '6', '205', '81', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000032.jpg', '58', '37', '166', '291', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000035.jpg', '65', '1', '167', '266', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000036.jpg', '51', '29', '160', '260', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000037.jpg', '63', '1', '169', '255', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000038.jpg', '36', '1', '173', '219', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000039.jpg', '40', '1', '171', '220', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000040.jpg', '50', '1', '202', '257', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000041.jpg', '94', '31', '214', '287', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000042.jpg', '23', '41', '166', '267', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000044.jpg', '60', '1', '174', '273', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000045.jpg', '72', '1', '174', '276', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000046.jpg', '33', '1', '155', '300', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000048.jpg', '37', '1', '157', '281', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000049.jpg', '53', '1', '148', '236', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000050.jpg', '46', '1', '162', '243', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000051.jpg', '73', '1', '176', '277', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000052.jpg', '69', '115', '143', '281', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000053.jpg', '60', '1', '178', '273', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000054.jpg', '97', '1', '205', '254', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000055.jpg', '65', '1', '173', '263', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000056.jpg', '96', '1', '206', '267', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000057.jpg', '71', '32', '167', '286', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000058.jpg', '97', '24', '205', '278', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000062.jpg', '92', '1', '217', '243', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000063.jpg', '82', '1', '224', '256', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000064.jpg', '100', '1', '202', '247', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000065.jpg', '49', '1', '158', '276', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000001.jpg', '99', '1', '207', '300', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000002.jpg', '31', '1', '192', '280', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000003.jpg', '57', '1', '182', '260', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000004.jpg', '86', '1', '214', '300', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000005.jpg', '82', '1', '220', '277', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000006.jpg', '85', '1', '219', '300', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000007.jpg', '91', '106', '192', '290', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000009.jpg', '58', '18', '142', '275', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000011.jpg', '42', '1', '144', '264', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000012.jpg', '30', '1', '131', '260', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000013.jpg', '28', '1', '146', '289', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000014.jpg', '66', '1', '174', '279', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000015.jpg', '1', '1', '173', '295', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000017.jpg', '96', '1', '209', '300', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000019.jpg', '52', '1', '174', '260', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000020.jpg', '44', '1', '150', '289', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000021.jpg', '18', '1', '82', '277', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000022.jpg', '85', '1', '239', '277', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000023.jpg', '38', '1', '116', '276', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000025.jpg', '76', '103', '168', '281', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000026.jpg', '103', '8', '205', '246', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000027.jpg', '75', '1', '149', '286', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000028.jpg', '54', '107', '113', '265', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000029.jpg', '59', '1', '186', '300', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000030.jpg', '43', '1', '161', '300', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000031.jpg', '111', '1', '185', '300', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000032.jpg', '109', '1', '194', '236', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000033.jpg', '62', '129', '139', '273', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000035.jpg', '119', '123', '186', '234', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000037.jpg', '42', '89', '178', '284', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000040.jpg', '251', '57', '286', '127', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000041.jpg', '1', '1', '143', '279', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000042.jpg', '77', '1', '101', '277', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000043.jpg', '82', '1', '120', '258', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000044.jpg', '5', '1', '125', '265', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000045.jpg', '1', '1', '110', '300', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000046.jpg', '48', '25', '154', '271', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000048.jpg', '88', '96', '130', '281', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000049.jpg', '70', '1', '152', '280', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000050.jpg', '193', '107', '270', '293', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000051.jpg', '97', '1', '199', '270', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000052.jpg', '36', '1', '130', '272', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000053.jpg', '44', '4', '160', '268', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000054.jpg', '61', '1', '167', '264', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000055.jpg', '94', '1', '212', '300', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000056.jpg', '147', '26', '196', '175', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000058.jpg', '81', '101', '146', '286', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000059.jpg', '45', '9', '161', '275', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000060.jpg', '11', '1', '193', '296', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000061.jpg', '88', '17', '180', '292', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000062.jpg', '54', '1', '155', '275', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000063.jpg', '114', '74', '186', '300', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000064.jpg', '60', '1', '160', '271', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000065.jpg', '56', '1', '133', '253', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000066.jpg', '51', '1', '161', '282', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000067.jpg', '67', '126', '119', '284', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000068.jpg', '88', '1', '174', '279', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000069.jpg', '21', '63', '124', '270', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000071.jpg', '67', '1', '176', '264', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000072.jpg', '59', '115', '136', '273', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Topstitched_Jodhpurs/img_00000001.jpg', '63', '117', '149', '251', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Topstitched_Jodhpurs/img_00000004.jpg', '81', '89', '143', '272', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Topstitched_Jodhpurs/img_00000005.jpg', '1', '127', '200', '300', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Topstitched_Jodhpurs/img_00000006.jpg', '1', '185', '205', '300', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Topstitched_Jodhpurs/img_00000008.jpg', '42', '115', '134', '261', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Topstitched_Jodhpurs/img_00000009.jpg', '87', '97', '110', '290', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Topstitched_Jodhpurs/img_00000012.jpg', '56', '103', '136', '266', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Topstitched_Jodhpurs/img_00000013.jpg', '4', '91', '112', '270', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Topstitched_Jodhpurs/img_00000014.jpg', '57', '101', '160', '270', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Topstitched_Jodhpurs/img_00000016.jpg', '87', '117', '145', '277', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Topstitched_Jodhpurs/img_00000018.jpg', '149', '99', '207', '273', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Topstitched_Jodhpurs/img_00000020.jpg', '50', '19', '150', '278', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Topstitched_Jodhpurs/img_00000022.jpg', '49', '112', '137', '279', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Topstitched_Jodhpurs/img_00000023.jpg', '1', '18', '200', '277', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Topstitched_Jodhpurs/img_00000024.jpg', '133', '121', '170', '281', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Topstitched_Jodhpurs/img_00000025.jpg', '42', '88', '136', '300', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Topstitched_Jodhpurs/img_00000026.jpg', '48', '121', '139', '293', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Topstitched_Jodhpurs/img_00000027.jpg', '119', '111', '177', '278', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Topstitched_Jodhpurs/img_00000028.jpg', '1', '187', '204', '300', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Topstitched_Jodhpurs/img_00000029.jpg', '60', '87', '118', '151', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Topstitched_Jodhpurs/img_00000030.jpg', '1', '121', '106', '170', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Topstitched_Jodhpurs/img_00000031.jpg', '66', '106', '143', '259', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Topstitched_Jodhpurs/img_00000032.jpg', '110', '123', '158', '285', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Topstitched_Jodhpurs/img_00000033.jpg', '1', '141', '200', '300', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Topstitched_Jodhpurs/img_00000034.jpg', '57', '1', '142', '269', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Topstitched_Jodhpurs/img_00000035.jpg', '50', '1', '158', '280', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Topstitched_Jodhpurs/img_00000037.jpg', '87', '106', '101', '279', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Topstitched_Jodhpurs/img_00000038.jpg', '1', '202', '200', '300', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Topstitched_Jodhpurs/img_00000039.jpg', '245', '1', '300', '143', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Topstitched_Jodhpurs/img_00000042.jpg', '77', '85', '125', '227', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Topstitched_Jodhpurs/img_00000043.jpg', '79', '101', '135', '272', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Topstitched_Jodhpurs/img_00000045.jpg', '72', '101', '128', '273', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000001.jpg', '1', '1', '300', '300', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000002.jpg', '258', '188', '300', '295', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000003.jpg', '15', '1', '233', '297', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000004.jpg', '57', '1', '194', '263', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000005.jpg', '34', '48', '179', '286', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000006.jpg', '5', '1', '300', '300', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000007.jpg', '1', '7', '189', '290', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000008.jpg', '27', '1', '279', '300', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000011.jpg', '107', '36', '203', '253', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000012.jpg', '25', '1', '289', '300', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000013.jpg', '42', '32', '150', '296', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000016.jpg', '46', '51', '218', '300', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000018.jpg', '85', '37', '221', '190', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000020.jpg', '54', '21', '205', '300', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000021.jpg', '93', '36', '166', '187', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000022.jpg', '52', '32', '170', '278', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000023.jpg', '20', '3', '148', '210', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000024.jpg', '53', '39', '226', '244', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000030.jpg', '79', '24', '231', '300', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000031.jpg', '63', '36', '202', '267', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000032.jpg', '41', '1', '265', '300', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000033.jpg', '43', '38', '164', '234', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000035.jpg', '1', '39', '106', '204', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000036.jpg', '8', '46', '259', '300', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000037.jpg', '81', '36', '220', '300', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000038.jpg', '58', '28', '146', '163', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000039.jpg', '1', '36', '300', '300', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000040.jpg', '47', '32', '250', '272', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000041.jpg', '115', '25', '187', '300', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000043.jpg', '70', '53', '221', '300', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000044.jpg', '1', '18', '300', '300', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000046.jpg', '59', '32', '244', '197', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000047.jpg', '61', '60', '243', '294', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000048.jpg', '1', '5', '300', '300', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000050.jpg', '77', '19', '213', '300', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000051.jpg', '1', '1', '300', '300', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000052.jpg', '12', '56', '186', '300', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000053.jpg', '85', '29', '207', '292', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Tonal_Plaid_Robe/img_00000001.jpg', '51', '20', '143', '219', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Tonal_Plaid_Robe/img_00000002.jpg', '68', '25', '174', '259', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Tonal_Plaid_Robe/img_00000003.jpg', '21', '30', '142', '278', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Tonal_Plaid_Robe/img_00000004.jpg', '105', '51', '206', '206', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Tonal_Plaid_Robe/img_00000005.jpg', '29', '19', '171', '239', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Tonal_Plaid_Robe/img_00000009.jpg', '60', '41', '211', '282', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Tonal_Plaid_Robe/img_00000012.jpg', '55', '14', '144', '181', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Tonal_Plaid_Robe/img_00000013.jpg', '22', '47', '154', '255', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Tonal_Plaid_Robe/img_00000014.jpg', '49', '86', '158', '249', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Tonal_Plaid_Robe/img_00000015.jpg', '56', '33', '172', '234', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Tonal_Plaid_Robe/img_00000016.jpg', '27', '34', '145', '243', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Tonal_Plaid_Robe/img_00000020.jpg', '4', '51', '176', '274', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Tonal_Plaid_Robe/img_00000021.jpg', '1', '1', '300', '300', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Tonal_Plaid_Robe/img_00000022.jpg', '35', '18', '126', '213', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Tonal_Plaid_Robe/img_00000023.jpg', '1', '1', '200', '300', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Tonal_Plaid_Robe/img_00000024.jpg', '28', '1', '200', '280', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Tonal_Plaid_Robe/img_00000025.jpg', '30', '56', '181', '300', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Tonal_Plaid_Robe/img_00000026.jpg', '86', '27', '144', '220', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Tonal_Plaid_Robe/img_00000027.jpg', '1', '37', '180', '220', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Tonal_Plaid_Robe/img_00000028.jpg', '78', '38', '235', '300', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Tonal_Plaid_Robe/img_00000030.jpg', '1', '43', '180', '220', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Tonal_Plaid_Robe/img_00000031.jpg', '1', '9', '179', '300', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Tonal_Plaid_Robe/img_00000032.jpg', '28', '34', '164', '250', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Training/Tonal_Plaid_Robe/img_00000033.jpg', '36', '15', '247', '300', '3']\n",
            "Data after sorting... [['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000001.jpg', 1, 31, 104, 76, 176], ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000002.jpg', 1, 119, 210, 78, 192], ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000003.jpg', 1, 80, 230, 101, 291], ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000005.jpg', 1, 25, 175, 85, 230], ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000006.jpg', 1, 19, 173, 115, 220], ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000007.jpg', 1, 15, 204, 99, 296], ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000009.jpg', 1, 17, 181, 75, 269], ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000010.jpg', 1, 29, 183, 91, 276], ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000011.jpg', 1, 64, 239, 83, 262], ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000012.jpg', 1, 15, 176, 91, 262]]\n",
            "curr image id... /content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000001\n",
            "current_labels... [1, 31, 104, 76]\n",
            "self.image_ids............. ['/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000001', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000002', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000003', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000005', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000006', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000007', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000009', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000010', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000011', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000012', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000013', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000014', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000016', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000018', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000020', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000023', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000024', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000026', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000027', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000029', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000030', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000031', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000033', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000034', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000035', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000036', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000039', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000040', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000041', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000043', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000044', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Quilted_Drawstring_Hoodie/img_00000001', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Quilted_Drawstring_Hoodie/img_00000002', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Quilted_Drawstring_Hoodie/img_00000003', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Quilted_Drawstring_Hoodie/img_00000004', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Quilted_Drawstring_Hoodie/img_00000005', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Quilted_Drawstring_Hoodie/img_00000008', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Quilted_Drawstring_Hoodie/img_00000009', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Quilted_Drawstring_Hoodie/img_00000010', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Quilted_Drawstring_Hoodie/img_00000012', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Quilted_Drawstring_Hoodie/img_00000013', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Quilted_Drawstring_Hoodie/img_00000015', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Quilted_Drawstring_Hoodie/img_00000017', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Quilted_Drawstring_Hoodie/img_00000020', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Quilted_Drawstring_Hoodie/img_00000022', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Quilted_Drawstring_Hoodie/img_00000024', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Quilted_Drawstring_Hoodie/img_00000025', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Quilted_Drawstring_Hoodie/img_00000028', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Quilted_Drawstring_Hoodie/img_00000029', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Quilted_Drawstring_Hoodie/img_00000030', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Quilted_Drawstring_Hoodie/img_00000032', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Quilted_Drawstring_Hoodie/img_00000033', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Quilted_Drawstring_Hoodie/img_00000034', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Quilted_Drawstring_Hoodie/img_00000036', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Quilted_Drawstring_Hoodie/img_00000037', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Quilted_Drawstring_Hoodie/img_00000039', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Quilted_Drawstring_Hoodie/img_00000041', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Quilted_Drawstring_Hoodie/img_00000042', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Quilted_Drawstring_Hoodie/img_00000044', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Quilted_Drawstring_Hoodie/img_00000045', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract-Quilted_Drawstring_Hoodie/img_00000046', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Dot_Off-the-Shoulder_Top/img_00000003', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Dot_Off-the-Shoulder_Top/img_00000004', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Dot_Off-the-Shoulder_Top/img_00000007', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Dot_Off-the-Shoulder_Top/img_00000008', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Dot_Off-the-Shoulder_Top/img_00000009', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Dot_Off-the-Shoulder_Top/img_00000010', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Dot_Off-the-Shoulder_Top/img_00000011', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Dot_Off-the-Shoulder_Top/img_00000012', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Dot_Off-the-Shoulder_Top/img_00000015', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Dot_Off-the-Shoulder_Top/img_00000016', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Dot_Off-the-Shoulder_Top/img_00000017', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Dot_Off-the-Shoulder_Top/img_00000019', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Dot_Off-the-Shoulder_Top/img_00000020', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Dot_Off-the-Shoulder_Top/img_00000021', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Dot_Off-the-Shoulder_Top/img_00000022', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Dot_Off-the-Shoulder_Top/img_00000023', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Dot_Off-the-Shoulder_Top/img_00000024', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Dot_Off-the-Shoulder_Top/img_00000026', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Dot_Off-the-Shoulder_Top/img_00000027', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Dot_Off-the-Shoulder_Top/img_00000028', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Dot_Off-the-Shoulder_Top/img_00000029', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Dot_Off-the-Shoulder_Top/img_00000030', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Dot_Off-the-Shoulder_Top/img_00000032', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Dot_Off-the-Shoulder_Top/img_00000033', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Dot_Off-the-Shoulder_Top/img_00000034', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Dot_Off-the-Shoulder_Top/img_00000035', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Dot_Off-the-Shoulder_Top/img_00000036', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Dot_Off-the-Shoulder_Top/img_00000037', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Dot_Off-the-Shoulder_Top/img_00000038', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000001', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000003', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000004', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000005', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000006', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000008', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000009', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000010', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000011', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000012', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000013', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000014', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000016', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000017', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000018', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000019', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000020', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000021', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000022', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000025', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000026', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000027', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000028', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000031', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000032', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000033', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000034', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000036', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000037', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000038', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000039', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000040', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000042', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000044', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000045', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000047', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000049', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000050', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000051', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000052', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000053', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Fringe_Crop_Top/img_00000054', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Print_Poncho/img_00000002', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Print_Poncho/img_00000003', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Print_Poncho/img_00000004', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Print_Poncho/img_00000005', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Print_Poncho/img_00000009', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Print_Poncho/img_00000010', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Print_Poncho/img_00000012', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Print_Poncho/img_00000013', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Print_Poncho/img_00000015', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Print_Poncho/img_00000016', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Print_Poncho/img_00000017', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Print_Poncho/img_00000018', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Print_Poncho/img_00000019', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Print_Poncho/img_00000021', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Print_Poncho/img_00000022', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Print_Poncho/img_00000023', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Print_Poncho/img_00000024', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Print_Poncho/img_00000025', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Print_Poncho/img_00000026', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Print_Poncho/img_00000027', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Print_Poncho/img_00000028', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Print_Poncho/img_00000030', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Print_Poncho/img_00000032', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Print_Poncho/img_00000033', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Print_Poncho/img_00000034', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Print_Poncho/img_00000035', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Print_Poncho/img_00000036', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Print_Poncho/img_00000037', '/content/drive/My Drive/Capstone project/SSD/Training/Abstract_Floral_Print_Poncho/img_00000038', '/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000001', '/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000002', '/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000003', '/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000004', '/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000005', '/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000006', '/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000007', '/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000009', '/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000011', '/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000012', '/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000013', '/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000014', '/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000015', '/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000017', '/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000019', '/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000020', '/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000021', '/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000022', '/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000023', '/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000025', '/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000026', '/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000027', '/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000028', '/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000029', '/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000030', '/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000031', '/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000032', '/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000033', '/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000035', '/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000037', '/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000040', '/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000041', '/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000042', '/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000043', '/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000044', '/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000045', '/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000046', '/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000048', '/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000049', '/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000050', '/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000051', '/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000052', '/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000053', '/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000054', '/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000055', '/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000056', '/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000058', '/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000059', '/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000060', '/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000061', '/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000062', '/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000063', '/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000064', '/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000065', '/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000066', '/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000067', '/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000068', '/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000069', '/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000071', '/content/drive/My Drive/Capstone project/SSD/Training/Acid_Wash_Jeggings/img_00000072', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000001', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000003', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000005', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000007', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000008', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000009', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000010', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000013', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000014', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000015', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000016', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000020', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000021', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000022', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000023', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000024', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000027', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000028', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000029', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000030', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000031', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000033', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000034', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000035', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000036', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000037', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000038', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000039', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000041', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000042', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000043', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000044', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000045', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000046', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000047', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000048', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000050', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000052', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000053', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000055', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000056', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000057', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000058', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000061', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000062', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000064', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000065', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000067', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000068', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000070', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000071', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000072', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000073', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000074', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000075', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000076', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000078', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000079', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000080', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000081', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000082', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000084', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000086', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000087', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000088', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000091', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000092', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000093', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000094', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000095', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000096', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000098', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000099', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000100', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000101', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000104', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000105', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000107', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000108', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000109', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000110', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000112', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000113', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000114', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000115', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000116', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000117', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000118', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000119', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000120', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000121', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000122', '/content/drive/My Drive/Capstone project/SSD/Training/Asymmetrical_Open-Front_Blazer/img_00000123', '/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000001', '/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000003', '/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000004', '/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000005', '/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000006', '/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000008', '/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000009', '/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000011', '/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000012', '/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000013', '/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000015', '/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000016', '/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000017', '/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000018', '/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000021', '/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000022', '/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000023', '/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000024', '/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000025', '/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000027', '/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000028', '/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000029', '/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000030', '/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000031', '/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000032', '/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000035', '/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000036', '/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000037', '/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000038', '/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000039', '/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000040', '/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000041', '/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000042', '/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000044', '/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000045', '/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000046', '/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000048', '/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000049', '/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000050', '/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000051', '/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000052', '/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000053', '/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000054', '/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000055', '/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000056', '/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000057', '/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000058', '/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000062', '/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000063', '/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000064', '/content/drive/My Drive/Capstone project/SSD/Training/Button-Tab_Ankle_Chinos/img_00000065', '/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000001', '/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000002', '/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000003', '/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000004', '/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000005', '/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000006', '/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000007', '/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000008', '/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000011', '/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000012', '/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000013', '/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000016', '/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000018', '/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000020', '/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000021', '/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000022', '/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000023', '/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000024', '/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000030', '/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000031', '/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000032', '/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000033', '/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000035', '/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000036', '/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000037', '/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000038', '/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000039', '/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000040', '/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000041', '/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000043', '/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000044', '/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000046', '/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000047', '/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000048', '/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000050', '/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000051', '/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000052', '/content/drive/My Drive/Capstone project/SSD/Training/Tile_Print_Drawstring_Caftan/img_00000053', '/content/drive/My Drive/Capstone project/SSD/Training/Tonal_Plaid_Robe/img_00000001', '/content/drive/My Drive/Capstone project/SSD/Training/Tonal_Plaid_Robe/img_00000002', '/content/drive/My Drive/Capstone project/SSD/Training/Tonal_Plaid_Robe/img_00000003', '/content/drive/My Drive/Capstone project/SSD/Training/Tonal_Plaid_Robe/img_00000004', '/content/drive/My Drive/Capstone project/SSD/Training/Tonal_Plaid_Robe/img_00000005', '/content/drive/My Drive/Capstone project/SSD/Training/Tonal_Plaid_Robe/img_00000009', '/content/drive/My Drive/Capstone project/SSD/Training/Tonal_Plaid_Robe/img_00000012', '/content/drive/My Drive/Capstone project/SSD/Training/Tonal_Plaid_Robe/img_00000013', '/content/drive/My Drive/Capstone project/SSD/Training/Tonal_Plaid_Robe/img_00000014', '/content/drive/My Drive/Capstone project/SSD/Training/Tonal_Plaid_Robe/img_00000015', '/content/drive/My Drive/Capstone project/SSD/Training/Tonal_Plaid_Robe/img_00000016', '/content/drive/My Drive/Capstone project/SSD/Training/Tonal_Plaid_Robe/img_00000020', '/content/drive/My Drive/Capstone project/SSD/Training/Tonal_Plaid_Robe/img_00000021', '/content/drive/My Drive/Capstone project/SSD/Training/Tonal_Plaid_Robe/img_00000022', '/content/drive/My Drive/Capstone project/SSD/Training/Tonal_Plaid_Robe/img_00000023', '/content/drive/My Drive/Capstone project/SSD/Training/Tonal_Plaid_Robe/img_00000024', '/content/drive/My Drive/Capstone project/SSD/Training/Tonal_Plaid_Robe/img_00000025', '/content/drive/My Drive/Capstone project/SSD/Training/Tonal_Plaid_Robe/img_00000026', '/content/drive/My Drive/Capstone project/SSD/Training/Tonal_Plaid_Robe/img_00000027', '/content/drive/My Drive/Capstone project/SSD/Training/Tonal_Plaid_Robe/img_00000028', '/content/drive/My Drive/Capstone project/SSD/Training/Tonal_Plaid_Robe/img_00000030', '/content/drive/My Drive/Capstone project/SSD/Training/Tonal_Plaid_Robe/img_00000031', '/content/drive/My Drive/Capstone project/SSD/Training/Tonal_Plaid_Robe/img_00000032', '/content/drive/My Drive/Capstone project/SSD/Training/Tonal_Plaid_Robe/img_00000033', '/content/drive/My Drive/Capstone project/SSD/Training/Topstitched_Jodhpurs/img_00000001', '/content/drive/My Drive/Capstone project/SSD/Training/Topstitched_Jodhpurs/img_00000004', '/content/drive/My Drive/Capstone project/SSD/Training/Topstitched_Jodhpurs/img_00000005', '/content/drive/My Drive/Capstone project/SSD/Training/Topstitched_Jodhpurs/img_00000006', '/content/drive/My Drive/Capstone project/SSD/Training/Topstitched_Jodhpurs/img_00000008', '/content/drive/My Drive/Capstone project/SSD/Training/Topstitched_Jodhpurs/img_00000009', '/content/drive/My Drive/Capstone project/SSD/Training/Topstitched_Jodhpurs/img_00000012', '/content/drive/My Drive/Capstone project/SSD/Training/Topstitched_Jodhpurs/img_00000013', '/content/drive/My Drive/Capstone project/SSD/Training/Topstitched_Jodhpurs/img_00000014', '/content/drive/My Drive/Capstone project/SSD/Training/Topstitched_Jodhpurs/img_00000016', '/content/drive/My Drive/Capstone project/SSD/Training/Topstitched_Jodhpurs/img_00000018', '/content/drive/My Drive/Capstone project/SSD/Training/Topstitched_Jodhpurs/img_00000020', '/content/drive/My Drive/Capstone project/SSD/Training/Topstitched_Jodhpurs/img_00000022', '/content/drive/My Drive/Capstone project/SSD/Training/Topstitched_Jodhpurs/img_00000023', '/content/drive/My Drive/Capstone project/SSD/Training/Topstitched_Jodhpurs/img_00000024', '/content/drive/My Drive/Capstone project/SSD/Training/Topstitched_Jodhpurs/img_00000025', '/content/drive/My Drive/Capstone project/SSD/Training/Topstitched_Jodhpurs/img_00000026', '/content/drive/My Drive/Capstone project/SSD/Training/Topstitched_Jodhpurs/img_00000027', '/content/drive/My Drive/Capstone project/SSD/Training/Topstitched_Jodhpurs/img_00000028', '/content/drive/My Drive/Capstone project/SSD/Training/Topstitched_Jodhpurs/img_00000029', '/content/drive/My Drive/Capstone project/SSD/Training/Topstitched_Jodhpurs/img_00000030', '/content/drive/My Drive/Capstone project/SSD/Training/Topstitched_Jodhpurs/img_00000031', '/content/drive/My Drive/Capstone project/SSD/Training/Topstitched_Jodhpurs/img_00000032', '/content/drive/My Drive/Capstone project/SSD/Training/Topstitched_Jodhpurs/img_00000033', '/content/drive/My Drive/Capstone project/SSD/Training/Topstitched_Jodhpurs/img_00000034', '/content/drive/My Drive/Capstone project/SSD/Training/Topstitched_Jodhpurs/img_00000035', '/content/drive/My Drive/Capstone project/SSD/Training/Topstitched_Jodhpurs/img_00000037', '/content/drive/My Drive/Capstone project/SSD/Training/Topstitched_Jodhpurs/img_00000038', '/content/drive/My Drive/Capstone project/SSD/Training/Topstitched_Jodhpurs/img_00000039', '/content/drive/My Drive/Capstone project/SSD/Training/Topstitched_Jodhpurs/img_00000042', '/content/drive/My Drive/Capstone project/SSD/Training/Topstitched_Jodhpurs/img_00000043', '/content/drive/My Drive/Capstone project/SSD/Training/Topstitched_Jodhpurs/img_00000045', '/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000002', '/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000003', '/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000004', '/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000005', '/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000006', '/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000007', '/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000008', '/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000009', '/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000010', '/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000011', '/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000013', '/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000014', '/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000015', '/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000016', '/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000018', '/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000019', '/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000020', '/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000021', '/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000022', '/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000025', '/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000026', '/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000028', '/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000030', '/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000031', '/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000032', '/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000034', '/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000037', '/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000038', '/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000039', '/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000040', '/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000042', '/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000043', '/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000044', '/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000046', '/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000047', '/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000048', '/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000049', '/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000050', '/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000051', '/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000052', '/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000053', '/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000056', '/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000057', '/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000058', '/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000059', '/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000060', '/content/drive/My Drive/Capstone project/SSD/Training/Zippered_Sleeveless_Hoodie/img_00000062']\n",
            "INSIDE PARSE_CSV..............\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Asymmetrical_Open-Front_Blazer/img_00000002.jpg', '73', '50', '239', '276', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Asymmetrical_Open-Front_Blazer/img_00000006.jpg', '56', '49', '181', '233', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Asymmetrical_Open-Front_Blazer/img_00000011.jpg', '64', '37', '165', '164', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Asymmetrical_Open-Front_Blazer/img_00000012.jpg', '60', '1', '175', '151', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Asymmetrical_Open-Front_Blazer/img_00000017.jpg', '94', '50', '216', '217', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Asymmetrical_Open-Front_Blazer/img_00000025.jpg', '16', '52', '177', '279', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Asymmetrical_Open-Front_Blazer/img_00000026.jpg', '68', '44', '238', '266', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Asymmetrical_Open-Front_Blazer/img_00000049.jpg', '22', '1', '188', '219', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Asymmetrical_Open-Front_Blazer/img_00000051.jpg', '4', '56', '196', '300', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Asymmetrical_Open-Front_Blazer/img_00000054.jpg', '70', '4', '168', '148', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Asymmetrical_Open-Front_Blazer/img_00000059.jpg', '31', '45', '188', '277', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Asymmetrical_Open-Front_Blazer/img_00000060.jpg', '145', '51', '300', '255', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Asymmetrical_Open-Front_Blazer/img_00000063.jpg', '48', '7', '242', '246', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Asymmetrical_Open-Front_Blazer/img_00000090.jpg', '52', '51', '258', '296', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Asymmetrical_Open-Front_Blazer/img_00000097.jpg', '23', '59', '204', '287', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Asymmetrical_Open-Front_Blazer/img_00000102.jpg', '43', '47', '207', '300', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Asymmetrical_Open-Front_Blazer/img_00000103.jpg', '54', '67', '229', '300', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Asymmetrical_Open-Front_Blazer/img_00000111.jpg', '26', '56', '189', '254', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Zippered_Sleeveless_Hoodie/img_00000012.jpg', '56', '78', '244', '300', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Zippered_Sleeveless_Hoodie/img_00000017.jpg', '63', '40', '152', '160', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Zippered_Sleeveless_Hoodie/img_00000029.jpg', '57', '56', '265', '266', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Zippered_Sleeveless_Hoodie/img_00000054.jpg', '1', '65', '126', '220', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Zippered_Sleeveless_Hoodie/img_00000061.jpg', '1', '45', '214', '170', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000004.jpg', '71', '68', '150', '168', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000008.jpg', '50', '85', '182', '290', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000017.jpg', '31', '77', '197', '252', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000021.jpg', '39', '75', '153', '184', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000025.jpg', '60', '69', '234', '292', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000032.jpg', '65', '70', '255', '199', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Abstract-Quilted_Drawstring_Hoodie/img_00000006.jpg', '1', '29', '300', '300', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Abstract-Quilted_Drawstring_Hoodie/img_00000007.jpg', '19', '69', '189', '278', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Abstract-Quilted_Drawstring_Hoodie/img_00000016.jpg', '32', '71', '222', '300', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Abstract-Quilted_Drawstring_Hoodie/img_00000019.jpg', '16', '69', '195', '273', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Abstract-Quilted_Drawstring_Hoodie/img_00000027.jpg', '22', '35', '136', '165', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Abstract-Quilted_Drawstring_Hoodie/img_00000038.jpg', '9', '53', '154', '228', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Abstract_Dot_Off-the-Shoulder_Top/img_00000013.jpg', '45', '81', '148', '164', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Abstract_Dot_Off-the-Shoulder_Top/img_00000014.jpg', '66', '74', '188', '263', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Abstract_Dot_Off-the-Shoulder_Top/img_00000018.jpg', '11', '114', '203', '300', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Abstract_Dot_Off-the-Shoulder_Top/img_00000025.jpg', '29', '93', '190', '278', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Abstract_Dot_Off-the-Shoulder_Top/img_00000031.jpg', '75', '102', '225', '226', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Abstract_Floral_Fringe_Crop_Top/img_00000007.jpg', '78', '73', '210', '197', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Abstract_Floral_Fringe_Crop_Top/img_00000015.jpg', '81', '81', '223', '214', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Abstract_Floral_Fringe_Crop_Top/img_00000023.jpg', '33', '71', '148', '204', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Abstract_Floral_Fringe_Crop_Top/img_00000024.jpg', '35', '98', '177', '261', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Abstract_Floral_Fringe_Crop_Top/img_00000029.jpg', '74', '56', '165', '185', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Abstract_Floral_Fringe_Crop_Top/img_00000035.jpg', '74', '93', '217', '261', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Abstract_Floral_Fringe_Crop_Top/img_00000041.jpg', '12', '82', '161', '250', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Abstract_Floral_Fringe_Crop_Top/img_00000043.jpg', '52', '43', '150', '135', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Abstract_Floral_Fringe_Crop_Top/img_00000046.jpg', '65', '84', '156', '192', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Abstract_Floral_Print_Poncho/img_00000001.jpg', '1', '69', '202', '300', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Abstract_Floral_Print_Poncho/img_00000008.jpg', '35', '54', '165', '300', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Abstract_Floral_Print_Poncho/img_00000011.jpg', '19', '80', '198', '300', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Abstract_Floral_Print_Poncho/img_00000020.jpg', '136', '75', '279', '295', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Button-Tab_Ankle_Chinos/img_00000007.jpg', '19', '1', '99', '205', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Button-Tab_Ankle_Chinos/img_00000010.jpg', '33', '1', '158', '236', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Button-Tab_Ankle_Chinos/img_00000019.jpg', '54', '10', '154', '206', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Button-Tab_Ankle_Chinos/img_00000026.jpg', '66', '1', '180', '280', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Button-Tab_Ankle_Chinos/img_00000033.jpg', '47', '1', '143', '249', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Button-Tab_Ankle_Chinos/img_00000034.jpg', '43', '1', '168', '220', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Button-Tab_Ankle_Chinos/img_00000043.jpg', '34', '1', '159', '253', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Button-Tab_Ankle_Chinos/img_00000059.jpg', '97', '1', '205', '225', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Acid_Wash_Jeggings/img_00000008.jpg', '72', '1', '188', '268', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Acid_Wash_Jeggings/img_00000016.jpg', '111', '1', '191', '255', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Acid_Wash_Jeggings/img_00000018.jpg', '87', '1', '209', '300', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Acid_Wash_Jeggings/img_00000057.jpg', '56', '1', '138', '281', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Acid_Wash_Jeggings/img_00000070.jpg', '65', '124', '151', '286', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Topstitched_Jodhpurs/img_00000002.jpg', '86', '87', '136', '249', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Topstitched_Jodhpurs/img_00000010.jpg', '11', '146', '189', '293', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Topstitched_Jodhpurs/img_00000011.jpg', '1', '174', '240', '300', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Topstitched_Jodhpurs/img_00000021.jpg', '81', '89', '153', '288', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Topstitched_Jodhpurs/img_00000040.jpg', '73', '131', '132', '275', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Tile_Print_Drawstring_Caftan/img_00000009.jpg', '42', '1', '264', '300', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Tile_Print_Drawstring_Caftan/img_00000010.jpg', '48', '33', '151', '201', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Tile_Print_Drawstring_Caftan/img_00000015.jpg', '78', '32', '157', '217', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Tile_Print_Drawstring_Caftan/img_00000017.jpg', '93', '21', '203', '273', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Tile_Print_Drawstring_Caftan/img_00000025.jpg', '58', '24', '248', '300', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Tile_Print_Drawstring_Caftan/img_00000027.jpg', '63', '30', '190', '271', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Tile_Print_Drawstring_Caftan/img_00000042.jpg', '12', '1', '299', '300', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Tonal_Plaid_Robe/img_00000010.jpg', '41', '66', '179', '300', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Tonal_Plaid_Robe/img_00000011.jpg', '60', '47', '154', '300', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Tonal_Plaid_Robe/img_00000017.jpg', '11', '50', '178', '293', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Validation/Tonal_Plaid_Robe/img_00000029.jpg', '112', '75', '236', '247', '3']\n",
            "Data after sorting... [['/content/drive/My Drive/Capstone project/SSD/Validation/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000004.jpg', 1, 71, 150, 68, 168], ['/content/drive/My Drive/Capstone project/SSD/Validation/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000008.jpg', 1, 50, 182, 85, 290], ['/content/drive/My Drive/Capstone project/SSD/Validation/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000017.jpg', 1, 31, 197, 77, 252], ['/content/drive/My Drive/Capstone project/SSD/Validation/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000021.jpg', 1, 39, 153, 75, 184], ['/content/drive/My Drive/Capstone project/SSD/Validation/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000025.jpg', 1, 60, 234, 69, 292], ['/content/drive/My Drive/Capstone project/SSD/Validation/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000032.jpg', 1, 65, 255, 70, 199], ['/content/drive/My Drive/Capstone project/SSD/Validation/Abstract-Quilted_Drawstring_Hoodie/img_00000006.jpg', 1, 1, 300, 29, 300], ['/content/drive/My Drive/Capstone project/SSD/Validation/Abstract-Quilted_Drawstring_Hoodie/img_00000007.jpg', 1, 19, 189, 69, 278], ['/content/drive/My Drive/Capstone project/SSD/Validation/Abstract-Quilted_Drawstring_Hoodie/img_00000016.jpg', 1, 32, 222, 71, 300], ['/content/drive/My Drive/Capstone project/SSD/Validation/Abstract-Quilted_Drawstring_Hoodie/img_00000019.jpg', 1, 16, 195, 69, 273]]\n",
            "curr image id... /content/drive/My Drive/Capstone project/SSD/Validation/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000004\n",
            "current_labels... [1, 71, 150, 68]\n",
            "self.image_ids............. ['/content/drive/My Drive/Capstone project/SSD/Validation/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000004', '/content/drive/My Drive/Capstone project/SSD/Validation/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000008', '/content/drive/My Drive/Capstone project/SSD/Validation/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000017', '/content/drive/My Drive/Capstone project/SSD/Validation/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000021', '/content/drive/My Drive/Capstone project/SSD/Validation/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000025', '/content/drive/My Drive/Capstone project/SSD/Validation/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000032', '/content/drive/My Drive/Capstone project/SSD/Validation/Abstract-Quilted_Drawstring_Hoodie/img_00000006', '/content/drive/My Drive/Capstone project/SSD/Validation/Abstract-Quilted_Drawstring_Hoodie/img_00000007', '/content/drive/My Drive/Capstone project/SSD/Validation/Abstract-Quilted_Drawstring_Hoodie/img_00000016', '/content/drive/My Drive/Capstone project/SSD/Validation/Abstract-Quilted_Drawstring_Hoodie/img_00000019', '/content/drive/My Drive/Capstone project/SSD/Validation/Abstract-Quilted_Drawstring_Hoodie/img_00000027', '/content/drive/My Drive/Capstone project/SSD/Validation/Abstract-Quilted_Drawstring_Hoodie/img_00000038', '/content/drive/My Drive/Capstone project/SSD/Validation/Abstract_Dot_Off-the-Shoulder_Top/img_00000013', '/content/drive/My Drive/Capstone project/SSD/Validation/Abstract_Dot_Off-the-Shoulder_Top/img_00000014', '/content/drive/My Drive/Capstone project/SSD/Validation/Abstract_Dot_Off-the-Shoulder_Top/img_00000018', '/content/drive/My Drive/Capstone project/SSD/Validation/Abstract_Dot_Off-the-Shoulder_Top/img_00000025', '/content/drive/My Drive/Capstone project/SSD/Validation/Abstract_Dot_Off-the-Shoulder_Top/img_00000031', '/content/drive/My Drive/Capstone project/SSD/Validation/Abstract_Floral_Fringe_Crop_Top/img_00000007', '/content/drive/My Drive/Capstone project/SSD/Validation/Abstract_Floral_Fringe_Crop_Top/img_00000015', '/content/drive/My Drive/Capstone project/SSD/Validation/Abstract_Floral_Fringe_Crop_Top/img_00000023', '/content/drive/My Drive/Capstone project/SSD/Validation/Abstract_Floral_Fringe_Crop_Top/img_00000024', '/content/drive/My Drive/Capstone project/SSD/Validation/Abstract_Floral_Fringe_Crop_Top/img_00000029', '/content/drive/My Drive/Capstone project/SSD/Validation/Abstract_Floral_Fringe_Crop_Top/img_00000035', '/content/drive/My Drive/Capstone project/SSD/Validation/Abstract_Floral_Fringe_Crop_Top/img_00000041', '/content/drive/My Drive/Capstone project/SSD/Validation/Abstract_Floral_Fringe_Crop_Top/img_00000043', '/content/drive/My Drive/Capstone project/SSD/Validation/Abstract_Floral_Fringe_Crop_Top/img_00000046', '/content/drive/My Drive/Capstone project/SSD/Validation/Abstract_Floral_Print_Poncho/img_00000001', '/content/drive/My Drive/Capstone project/SSD/Validation/Abstract_Floral_Print_Poncho/img_00000008', '/content/drive/My Drive/Capstone project/SSD/Validation/Abstract_Floral_Print_Poncho/img_00000011', '/content/drive/My Drive/Capstone project/SSD/Validation/Abstract_Floral_Print_Poncho/img_00000020', '/content/drive/My Drive/Capstone project/SSD/Validation/Acid_Wash_Jeggings/img_00000008', '/content/drive/My Drive/Capstone project/SSD/Validation/Acid_Wash_Jeggings/img_00000016', '/content/drive/My Drive/Capstone project/SSD/Validation/Acid_Wash_Jeggings/img_00000018', '/content/drive/My Drive/Capstone project/SSD/Validation/Acid_Wash_Jeggings/img_00000057', '/content/drive/My Drive/Capstone project/SSD/Validation/Acid_Wash_Jeggings/img_00000070', '/content/drive/My Drive/Capstone project/SSD/Validation/Asymmetrical_Open-Front_Blazer/img_00000002', '/content/drive/My Drive/Capstone project/SSD/Validation/Asymmetrical_Open-Front_Blazer/img_00000006', '/content/drive/My Drive/Capstone project/SSD/Validation/Asymmetrical_Open-Front_Blazer/img_00000011', '/content/drive/My Drive/Capstone project/SSD/Validation/Asymmetrical_Open-Front_Blazer/img_00000012', '/content/drive/My Drive/Capstone project/SSD/Validation/Asymmetrical_Open-Front_Blazer/img_00000017', '/content/drive/My Drive/Capstone project/SSD/Validation/Asymmetrical_Open-Front_Blazer/img_00000025', '/content/drive/My Drive/Capstone project/SSD/Validation/Asymmetrical_Open-Front_Blazer/img_00000026', '/content/drive/My Drive/Capstone project/SSD/Validation/Asymmetrical_Open-Front_Blazer/img_00000049', '/content/drive/My Drive/Capstone project/SSD/Validation/Asymmetrical_Open-Front_Blazer/img_00000051', '/content/drive/My Drive/Capstone project/SSD/Validation/Asymmetrical_Open-Front_Blazer/img_00000054', '/content/drive/My Drive/Capstone project/SSD/Validation/Asymmetrical_Open-Front_Blazer/img_00000059', '/content/drive/My Drive/Capstone project/SSD/Validation/Asymmetrical_Open-Front_Blazer/img_00000060', '/content/drive/My Drive/Capstone project/SSD/Validation/Asymmetrical_Open-Front_Blazer/img_00000063', '/content/drive/My Drive/Capstone project/SSD/Validation/Asymmetrical_Open-Front_Blazer/img_00000090', '/content/drive/My Drive/Capstone project/SSD/Validation/Asymmetrical_Open-Front_Blazer/img_00000097', '/content/drive/My Drive/Capstone project/SSD/Validation/Asymmetrical_Open-Front_Blazer/img_00000102', '/content/drive/My Drive/Capstone project/SSD/Validation/Asymmetrical_Open-Front_Blazer/img_00000103', '/content/drive/My Drive/Capstone project/SSD/Validation/Asymmetrical_Open-Front_Blazer/img_00000111', '/content/drive/My Drive/Capstone project/SSD/Validation/Button-Tab_Ankle_Chinos/img_00000007', '/content/drive/My Drive/Capstone project/SSD/Validation/Button-Tab_Ankle_Chinos/img_00000010', '/content/drive/My Drive/Capstone project/SSD/Validation/Button-Tab_Ankle_Chinos/img_00000019', '/content/drive/My Drive/Capstone project/SSD/Validation/Button-Tab_Ankle_Chinos/img_00000026', '/content/drive/My Drive/Capstone project/SSD/Validation/Button-Tab_Ankle_Chinos/img_00000033', '/content/drive/My Drive/Capstone project/SSD/Validation/Button-Tab_Ankle_Chinos/img_00000034', '/content/drive/My Drive/Capstone project/SSD/Validation/Button-Tab_Ankle_Chinos/img_00000043', '/content/drive/My Drive/Capstone project/SSD/Validation/Button-Tab_Ankle_Chinos/img_00000059', '/content/drive/My Drive/Capstone project/SSD/Validation/Tile_Print_Drawstring_Caftan/img_00000009', '/content/drive/My Drive/Capstone project/SSD/Validation/Tile_Print_Drawstring_Caftan/img_00000010', '/content/drive/My Drive/Capstone project/SSD/Validation/Tile_Print_Drawstring_Caftan/img_00000015', '/content/drive/My Drive/Capstone project/SSD/Validation/Tile_Print_Drawstring_Caftan/img_00000017', '/content/drive/My Drive/Capstone project/SSD/Validation/Tile_Print_Drawstring_Caftan/img_00000025', '/content/drive/My Drive/Capstone project/SSD/Validation/Tile_Print_Drawstring_Caftan/img_00000027', '/content/drive/My Drive/Capstone project/SSD/Validation/Tile_Print_Drawstring_Caftan/img_00000042', '/content/drive/My Drive/Capstone project/SSD/Validation/Tonal_Plaid_Robe/img_00000010', '/content/drive/My Drive/Capstone project/SSD/Validation/Tonal_Plaid_Robe/img_00000011', '/content/drive/My Drive/Capstone project/SSD/Validation/Tonal_Plaid_Robe/img_00000017', '/content/drive/My Drive/Capstone project/SSD/Validation/Tonal_Plaid_Robe/img_00000029', '/content/drive/My Drive/Capstone project/SSD/Validation/Topstitched_Jodhpurs/img_00000002', '/content/drive/My Drive/Capstone project/SSD/Validation/Topstitched_Jodhpurs/img_00000010', '/content/drive/My Drive/Capstone project/SSD/Validation/Topstitched_Jodhpurs/img_00000011', '/content/drive/My Drive/Capstone project/SSD/Validation/Topstitched_Jodhpurs/img_00000021', '/content/drive/My Drive/Capstone project/SSD/Validation/Topstitched_Jodhpurs/img_00000040', '/content/drive/My Drive/Capstone project/SSD/Validation/Zippered_Sleeveless_Hoodie/img_00000012', '/content/drive/My Drive/Capstone project/SSD/Validation/Zippered_Sleeveless_Hoodie/img_00000017', '/content/drive/My Drive/Capstone project/SSD/Validation/Zippered_Sleeveless_Hoodie/img_00000029', '/content/drive/My Drive/Capstone project/SSD/Validation/Zippered_Sleeveless_Hoodie/img_00000054', '/content/drive/My Drive/Capstone project/SSD/Validation/Zippered_Sleeveless_Hoodie/img_00000061']\n",
            "INSIDE PARSE_CSV..............\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Asymmetrical_Open-Front_Blazer/img_00000004.jpg', '6', '92', '166', '297', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Asymmetrical_Open-Front_Blazer/img_00000018.jpg', '1', '8', '187', '218', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Asymmetrical_Open-Front_Blazer/img_00000019.jpg', '93', '43', '215', '149', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Asymmetrical_Open-Front_Blazer/img_00000032.jpg', '39', '67', '207', '250', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Asymmetrical_Open-Front_Blazer/img_00000040.jpg', '88', '49', '227', '282', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Asymmetrical_Open-Front_Blazer/img_00000066.jpg', '6', '1', '222', '278', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Asymmetrical_Open-Front_Blazer/img_00000069.jpg', '53', '38', '137', '162', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Asymmetrical_Open-Front_Blazer/img_00000077.jpg', '1', '1', '226', '300', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Asymmetrical_Open-Front_Blazer/img_00000083.jpg', '47', '57', '203', '273', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Asymmetrical_Open-Front_Blazer/img_00000085.jpg', '1', '1', '224', '287', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Asymmetrical_Open-Front_Blazer/img_00000089.jpg', '17', '52', '204', '281', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Asymmetrical_Open-Front_Blazer/img_00000106.jpg', '60', '57', '211', '274', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Zippered_Sleeveless_Hoodie/img_00000001.jpg', '64', '52', '243', '275', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Zippered_Sleeveless_Hoodie/img_00000023.jpg', '61', '1', '243', '252', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Zippered_Sleeveless_Hoodie/img_00000024.jpg', '36', '75', '188', '300', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Zippered_Sleeveless_Hoodie/img_00000027.jpg', '1', '10', '300', '300', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Zippered_Sleeveless_Hoodie/img_00000033.jpg', '27', '34', '181', '255', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Zippered_Sleeveless_Hoodie/img_00000035.jpg', '145', '49', '227', '232', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Zippered_Sleeveless_Hoodie/img_00000036.jpg', '72', '98', '222', '284', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Zippered_Sleeveless_Hoodie/img_00000041.jpg', '75', '17', '171', '195', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Zippered_Sleeveless_Hoodie/img_00000045.jpg', '94', '69', '245', '271', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Zippered_Sleeveless_Hoodie/img_00000055.jpg', '34', '18', '171', '215', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000015.jpg', '53', '74', '155', '240', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000019.jpg', '51', '55', '130', '188', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000022.jpg', '24', '82', '196', '244', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000028.jpg', '38', '73', '154', '178', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000037.jpg', '1', '53', '139', '202', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000038.jpg', '58', '48', '186', '160', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000042.jpg', '28', '85', '209', '250', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Abstract-Quilted_Drawstring_Hoodie/img_00000011.jpg', '21', '67', '182', '286', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Abstract-Quilted_Drawstring_Hoodie/img_00000014.jpg', '21', '55', '137', '284', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Abstract-Quilted_Drawstring_Hoodie/img_00000018.jpg', '29', '63', '219', '295', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Abstract-Quilted_Drawstring_Hoodie/img_00000021.jpg', '82', '86', '216', '300', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Abstract-Quilted_Drawstring_Hoodie/img_00000023.jpg', '16', '50', '143', '200', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Abstract-Quilted_Drawstring_Hoodie/img_00000026.jpg', '46', '54', '191', '236', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Abstract-Quilted_Drawstring_Hoodie/img_00000031.jpg', '23', '53', '155', '220', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Abstract-Quilted_Drawstring_Hoodie/img_00000035.jpg', '15', '61', '161', '245', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Abstract-Quilted_Drawstring_Hoodie/img_00000040.jpg', '32', '49', '120', '185', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Abstract-Quilted_Drawstring_Hoodie/img_00000043.jpg', '33', '63', '160', '228', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Abstract_Dot_Off-the-Shoulder_Top/img_00000002.jpg', '14', '87', '204', '294', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Abstract_Dot_Off-the-Shoulder_Top/img_00000005.jpg', '47', '92', '174', '211', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Abstract_Dot_Off-the-Shoulder_Top/img_00000006.jpg', '33', '83', '158', '207', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Abstract_Floral_Fringe_Crop_Top/img_00000002.jpg', '91', '58', '191', '218', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Abstract_Floral_Fringe_Crop_Top/img_00000030.jpg', '27', '104', '119', '188', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Abstract_Floral_Fringe_Crop_Top/img_00000048.jpg', '27', '70', '140', '144', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Abstract_Floral_Print_Poncho/img_00000006.jpg', '72', '71', '196', '292', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Abstract_Floral_Print_Poncho/img_00000007.jpg', '1', '55', '200', '200', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Abstract_Floral_Print_Poncho/img_00000014.jpg', '42', '49', '265', '300', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Abstract_Floral_Print_Poncho/img_00000029.jpg', '13', '109', '180', '298', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Abstract_Floral_Print_Poncho/img_00000031.jpg', '43', '56', '275', '300', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Abstract_Floral_Print_Poncho/img_00000039.jpg', '85', '26', '221', '255', '1']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Button-Tab_Ankle_Chinos/img_00000002.jpg', '71', '104', '127', '269', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Button-Tab_Ankle_Chinos/img_00000014.jpg', '8', '5', '97', '155', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Button-Tab_Ankle_Chinos/img_00000020.jpg', '74', '117', '151', '197', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Button-Tab_Ankle_Chinos/img_00000047.jpg', '80', '1', '218', '300', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Button-Tab_Ankle_Chinos/img_00000060.jpg', '46', '9', '147', '281', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Button-Tab_Ankle_Chinos/img_00000061.jpg', '28', '1', '171', '269', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Acid_Wash_Jeggings/img_00000010.jpg', '97', '1', '236', '278', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Acid_Wash_Jeggings/img_00000024.jpg', '18', '13', '272', '300', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Acid_Wash_Jeggings/img_00000034.jpg', '74', '119', '170', '265', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Acid_Wash_Jeggings/img_00000036.jpg', '65', '1', '154', '272', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Acid_Wash_Jeggings/img_00000038.jpg', '55', '1', '176', '255', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Acid_Wash_Jeggings/img_00000039.jpg', '66', '1', '166', '267', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Acid_Wash_Jeggings/img_00000047.jpg', '69', '124', '139', '279', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Topstitched_Jodhpurs/img_00000003.jpg', '59', '97', '125', '255', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Topstitched_Jodhpurs/img_00000007.jpg', '54', '97', '164', '287', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Topstitched_Jodhpurs/img_00000015.jpg', '1', '137', '233', '233', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Topstitched_Jodhpurs/img_00000017.jpg', '1', '197', '200', '300', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Topstitched_Jodhpurs/img_00000019.jpg', '71', '120', '149', '287', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Topstitched_Jodhpurs/img_00000036.jpg', '57', '115', '158', '286', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Topstitched_Jodhpurs/img_00000041.jpg', '100', '98', '185', '282', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Topstitched_Jodhpurs/img_00000044.jpg', '39', '108', '104', '273', '2']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Tile_Print_Drawstring_Caftan/img_00000014.jpg', '126', '23', '239', '300', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Tile_Print_Drawstring_Caftan/img_00000019.jpg', '1', '33', '200', '300', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Tile_Print_Drawstring_Caftan/img_00000026.jpg', '41', '34', '151', '170', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Tile_Print_Drawstring_Caftan/img_00000028.jpg', '40', '47', '191', '276', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Tile_Print_Drawstring_Caftan/img_00000029.jpg', '63', '44', '158', '228', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Tile_Print_Drawstring_Caftan/img_00000034.jpg', '55', '48', '241', '300', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Tile_Print_Drawstring_Caftan/img_00000045.jpg', '46', '52', '234', '297', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Tile_Print_Drawstring_Caftan/img_00000049.jpg', '86', '30', '180', '174', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Tile_Print_Drawstring_Caftan/img_00000054.jpg', '36', '61', '196', '300', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Tonal_Plaid_Robe/img_00000006.jpg', '51', '32', '128', '164', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Tonal_Plaid_Robe/img_00000007.jpg', '56', '17', '148', '214', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Tonal_Plaid_Robe/img_00000008.jpg', '27', '1', '173', '250', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Tonal_Plaid_Robe/img_00000018.jpg', '35', '24', '137', '241', '3']\n",
            "Row : ['/content/drive/My Drive/Capstone project/SSD/Testing/Tonal_Plaid_Robe/img_00000019.jpg', '19', '47', '146', '248', '3']\n",
            "Data after sorting... [['/content/drive/My Drive/Capstone project/SSD/Testing/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000015.jpg', 1, 53, 155, 74, 240], ['/content/drive/My Drive/Capstone project/SSD/Testing/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000019.jpg', 1, 51, 130, 55, 188], ['/content/drive/My Drive/Capstone project/SSD/Testing/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000022.jpg', 1, 24, 196, 82, 244], ['/content/drive/My Drive/Capstone project/SSD/Testing/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000028.jpg', 1, 38, 154, 73, 178], ['/content/drive/My Drive/Capstone project/SSD/Testing/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000037.jpg', 1, 1, 139, 53, 202], ['/content/drive/My Drive/Capstone project/SSD/Testing/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000038.jpg', 1, 58, 186, 48, 160], ['/content/drive/My Drive/Capstone project/SSD/Testing/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000042.jpg', 1, 28, 209, 85, 250], ['/content/drive/My Drive/Capstone project/SSD/Testing/Abstract-Quilted_Drawstring_Hoodie/img_00000011.jpg', 1, 21, 182, 67, 286], ['/content/drive/My Drive/Capstone project/SSD/Testing/Abstract-Quilted_Drawstring_Hoodie/img_00000014.jpg', 1, 21, 137, 55, 284], ['/content/drive/My Drive/Capstone project/SSD/Testing/Abstract-Quilted_Drawstring_Hoodie/img_00000018.jpg', 1, 29, 219, 63, 295]]\n",
            "curr image id... /content/drive/My Drive/Capstone project/SSD/Testing/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000015\n",
            "current_labels... [1, 53, 155, 74]\n",
            "self.image_ids............. ['/content/drive/My Drive/Capstone project/SSD/Testing/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000015', '/content/drive/My Drive/Capstone project/SSD/Testing/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000019', '/content/drive/My Drive/Capstone project/SSD/Testing/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000022', '/content/drive/My Drive/Capstone project/SSD/Testing/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000028', '/content/drive/My Drive/Capstone project/SSD/Testing/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000037', '/content/drive/My Drive/Capstone project/SSD/Testing/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000038', '/content/drive/My Drive/Capstone project/SSD/Testing/Abstract-Plaid_Ruffled_Bell_Sleeve_Top/img_00000042', '/content/drive/My Drive/Capstone project/SSD/Testing/Abstract-Quilted_Drawstring_Hoodie/img_00000011', '/content/drive/My Drive/Capstone project/SSD/Testing/Abstract-Quilted_Drawstring_Hoodie/img_00000014', '/content/drive/My Drive/Capstone project/SSD/Testing/Abstract-Quilted_Drawstring_Hoodie/img_00000018', '/content/drive/My Drive/Capstone project/SSD/Testing/Abstract-Quilted_Drawstring_Hoodie/img_00000021', '/content/drive/My Drive/Capstone project/SSD/Testing/Abstract-Quilted_Drawstring_Hoodie/img_00000023', '/content/drive/My Drive/Capstone project/SSD/Testing/Abstract-Quilted_Drawstring_Hoodie/img_00000026', '/content/drive/My Drive/Capstone project/SSD/Testing/Abstract-Quilted_Drawstring_Hoodie/img_00000031', '/content/drive/My Drive/Capstone project/SSD/Testing/Abstract-Quilted_Drawstring_Hoodie/img_00000035', '/content/drive/My Drive/Capstone project/SSD/Testing/Abstract-Quilted_Drawstring_Hoodie/img_00000040', '/content/drive/My Drive/Capstone project/SSD/Testing/Abstract-Quilted_Drawstring_Hoodie/img_00000043', '/content/drive/My Drive/Capstone project/SSD/Testing/Abstract_Dot_Off-the-Shoulder_Top/img_00000002', '/content/drive/My Drive/Capstone project/SSD/Testing/Abstract_Dot_Off-the-Shoulder_Top/img_00000005', '/content/drive/My Drive/Capstone project/SSD/Testing/Abstract_Dot_Off-the-Shoulder_Top/img_00000006', '/content/drive/My Drive/Capstone project/SSD/Testing/Abstract_Floral_Fringe_Crop_Top/img_00000002', '/content/drive/My Drive/Capstone project/SSD/Testing/Abstract_Floral_Fringe_Crop_Top/img_00000030', '/content/drive/My Drive/Capstone project/SSD/Testing/Abstract_Floral_Fringe_Crop_Top/img_00000048', '/content/drive/My Drive/Capstone project/SSD/Testing/Abstract_Floral_Print_Poncho/img_00000006', '/content/drive/My Drive/Capstone project/SSD/Testing/Abstract_Floral_Print_Poncho/img_00000007', '/content/drive/My Drive/Capstone project/SSD/Testing/Abstract_Floral_Print_Poncho/img_00000014', '/content/drive/My Drive/Capstone project/SSD/Testing/Abstract_Floral_Print_Poncho/img_00000029', '/content/drive/My Drive/Capstone project/SSD/Testing/Abstract_Floral_Print_Poncho/img_00000031', '/content/drive/My Drive/Capstone project/SSD/Testing/Abstract_Floral_Print_Poncho/img_00000039', '/content/drive/My Drive/Capstone project/SSD/Testing/Acid_Wash_Jeggings/img_00000010', '/content/drive/My Drive/Capstone project/SSD/Testing/Acid_Wash_Jeggings/img_00000024', '/content/drive/My Drive/Capstone project/SSD/Testing/Acid_Wash_Jeggings/img_00000034', '/content/drive/My Drive/Capstone project/SSD/Testing/Acid_Wash_Jeggings/img_00000036', '/content/drive/My Drive/Capstone project/SSD/Testing/Acid_Wash_Jeggings/img_00000038', '/content/drive/My Drive/Capstone project/SSD/Testing/Acid_Wash_Jeggings/img_00000039', '/content/drive/My Drive/Capstone project/SSD/Testing/Acid_Wash_Jeggings/img_00000047', '/content/drive/My Drive/Capstone project/SSD/Testing/Asymmetrical_Open-Front_Blazer/img_00000004', '/content/drive/My Drive/Capstone project/SSD/Testing/Asymmetrical_Open-Front_Blazer/img_00000018', '/content/drive/My Drive/Capstone project/SSD/Testing/Asymmetrical_Open-Front_Blazer/img_00000019', '/content/drive/My Drive/Capstone project/SSD/Testing/Asymmetrical_Open-Front_Blazer/img_00000032', '/content/drive/My Drive/Capstone project/SSD/Testing/Asymmetrical_Open-Front_Blazer/img_00000040', '/content/drive/My Drive/Capstone project/SSD/Testing/Asymmetrical_Open-Front_Blazer/img_00000066', '/content/drive/My Drive/Capstone project/SSD/Testing/Asymmetrical_Open-Front_Blazer/img_00000069', '/content/drive/My Drive/Capstone project/SSD/Testing/Asymmetrical_Open-Front_Blazer/img_00000077', '/content/drive/My Drive/Capstone project/SSD/Testing/Asymmetrical_Open-Front_Blazer/img_00000083', '/content/drive/My Drive/Capstone project/SSD/Testing/Asymmetrical_Open-Front_Blazer/img_00000085', '/content/drive/My Drive/Capstone project/SSD/Testing/Asymmetrical_Open-Front_Blazer/img_00000089', '/content/drive/My Drive/Capstone project/SSD/Testing/Asymmetrical_Open-Front_Blazer/img_00000106', '/content/drive/My Drive/Capstone project/SSD/Testing/Button-Tab_Ankle_Chinos/img_00000002', '/content/drive/My Drive/Capstone project/SSD/Testing/Button-Tab_Ankle_Chinos/img_00000014', '/content/drive/My Drive/Capstone project/SSD/Testing/Button-Tab_Ankle_Chinos/img_00000020', '/content/drive/My Drive/Capstone project/SSD/Testing/Button-Tab_Ankle_Chinos/img_00000047', '/content/drive/My Drive/Capstone project/SSD/Testing/Button-Tab_Ankle_Chinos/img_00000060', '/content/drive/My Drive/Capstone project/SSD/Testing/Button-Tab_Ankle_Chinos/img_00000061', '/content/drive/My Drive/Capstone project/SSD/Testing/Tile_Print_Drawstring_Caftan/img_00000014', '/content/drive/My Drive/Capstone project/SSD/Testing/Tile_Print_Drawstring_Caftan/img_00000019', '/content/drive/My Drive/Capstone project/SSD/Testing/Tile_Print_Drawstring_Caftan/img_00000026', '/content/drive/My Drive/Capstone project/SSD/Testing/Tile_Print_Drawstring_Caftan/img_00000028', '/content/drive/My Drive/Capstone project/SSD/Testing/Tile_Print_Drawstring_Caftan/img_00000029', '/content/drive/My Drive/Capstone project/SSD/Testing/Tile_Print_Drawstring_Caftan/img_00000034', '/content/drive/My Drive/Capstone project/SSD/Testing/Tile_Print_Drawstring_Caftan/img_00000045', '/content/drive/My Drive/Capstone project/SSD/Testing/Tile_Print_Drawstring_Caftan/img_00000049', '/content/drive/My Drive/Capstone project/SSD/Testing/Tile_Print_Drawstring_Caftan/img_00000054', '/content/drive/My Drive/Capstone project/SSD/Testing/Tonal_Plaid_Robe/img_00000006', '/content/drive/My Drive/Capstone project/SSD/Testing/Tonal_Plaid_Robe/img_00000007', '/content/drive/My Drive/Capstone project/SSD/Testing/Tonal_Plaid_Robe/img_00000008', '/content/drive/My Drive/Capstone project/SSD/Testing/Tonal_Plaid_Robe/img_00000018', '/content/drive/My Drive/Capstone project/SSD/Testing/Tonal_Plaid_Robe/img_00000019', '/content/drive/My Drive/Capstone project/SSD/Testing/Topstitched_Jodhpurs/img_00000003', '/content/drive/My Drive/Capstone project/SSD/Testing/Topstitched_Jodhpurs/img_00000007', '/content/drive/My Drive/Capstone project/SSD/Testing/Topstitched_Jodhpurs/img_00000015', '/content/drive/My Drive/Capstone project/SSD/Testing/Topstitched_Jodhpurs/img_00000017', '/content/drive/My Drive/Capstone project/SSD/Testing/Topstitched_Jodhpurs/img_00000019', '/content/drive/My Drive/Capstone project/SSD/Testing/Topstitched_Jodhpurs/img_00000036', '/content/drive/My Drive/Capstone project/SSD/Testing/Topstitched_Jodhpurs/img_00000041', '/content/drive/My Drive/Capstone project/SSD/Testing/Topstitched_Jodhpurs/img_00000044', '/content/drive/My Drive/Capstone project/SSD/Testing/Zippered_Sleeveless_Hoodie/img_00000001', '/content/drive/My Drive/Capstone project/SSD/Testing/Zippered_Sleeveless_Hoodie/img_00000023', '/content/drive/My Drive/Capstone project/SSD/Testing/Zippered_Sleeveless_Hoodie/img_00000024', '/content/drive/My Drive/Capstone project/SSD/Testing/Zippered_Sleeveless_Hoodie/img_00000027', '/content/drive/My Drive/Capstone project/SSD/Testing/Zippered_Sleeveless_Hoodie/img_00000033', '/content/drive/My Drive/Capstone project/SSD/Testing/Zippered_Sleeveless_Hoodie/img_00000035', '/content/drive/My Drive/Capstone project/SSD/Testing/Zippered_Sleeveless_Hoodie/img_00000036', '/content/drive/My Drive/Capstone project/SSD/Testing/Zippered_Sleeveless_Hoodie/img_00000041', '/content/drive/My Drive/Capstone project/SSD/Testing/Zippered_Sleeveless_Hoodie/img_00000045', '/content/drive/My Drive/Capstone project/SSD/Testing/Zippered_Sleeveless_Hoodie/img_00000055']\n",
            "Number of images in the training dataset:\t   506\n",
            "Number of images in the validation dataset:\t    82\n",
            "Number of images in the testing dataset:\t    86\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGgFVWTysFCV",
        "colab_type": "code",
        "outputId": "c7e83c97-b296-4091-cf28-b4c575870709",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# 3: Set the batch size.\n",
        "\n",
        "batch_size = 10 #32 # Change the batch size if you like, or if you run into GPU memory issues.\n",
        "\n",
        "# 4: Set the image transformations for pre-processing and data augmentation options.\n",
        "\n",
        "# For the training generator:\n",
        "ssd_data_augmentation = SSDDataAugmentation(img_height=img_height,\n",
        "                                            img_width=img_width,\n",
        "                                            background=mean_color)\n",
        "\n",
        "ssd_data_augmentation"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.SSDDataAugmentation at 0x7f5860be95f8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnL8KIHrtekK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# For the validation generator:\n",
        "convert_to_3_channels = ConvertTo3Channels()\n",
        "resize = Resize(height=img_height, width=img_width)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjOeJiOKtoHk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 5: Instantiate an encoder that can encode ground truth labels into the format needed by the SSD loss function.\n",
        "\n",
        "# The encoder constructor needs the spatial dimensions of the model's predictor layers to create the anchor boxes.\n",
        "predictor_sizes = [model.get_layer('conv4_3_norm_mbox_conf').output_shape[1:3],\n",
        "                   model.get_layer('fc7_mbox_conf').output_shape[1:3],\n",
        "                   model.get_layer('conv6_2_mbox_conf').output_shape[1:3],\n",
        "                   model.get_layer('conv7_2_mbox_conf').output_shape[1:3],\n",
        "                   model.get_layer('conv8_2_mbox_conf').output_shape[1:3],\n",
        "                   model.get_layer('conv9_2_mbox_conf').output_shape[1:3]]\n",
        "\n",
        "ssd_input_encoder = SSDInputEncoder(img_height=img_height,\n",
        "                                    img_width=img_width,\n",
        "                                    n_classes=n_classes,\n",
        "                                    predictor_sizes=predictor_sizes,\n",
        "                                    scales=scales,\n",
        "                                    aspect_ratios_per_layer=aspect_ratios,\n",
        "                                    two_boxes_for_ar1=two_boxes_for_ar1,\n",
        "                                    steps=steps,\n",
        "                                    offsets=offsets,\n",
        "                                    clip_boxes=clip_boxes,\n",
        "                                    variances=variances,\n",
        "                                    matching_type='multi',\n",
        "                                    pos_iou_threshold=0.5,\n",
        "                                    neg_iou_limit=0.5,\n",
        "                                    normalize_coords=normalize_coords)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkDB-3X0EeiM",
        "colab_type": "code",
        "outputId": "9d671965-d6ef-4ac4-b3d4-dc599c9b7ada",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "\n",
        "\n",
        "# 6: Create the generator handles that will be passed to Keras' `fit_generator()` function.\n",
        "\n",
        "train_generator = train_dataset.generate(batch_size=batch_size,\n",
        "                                         shuffle=True,\n",
        "                                         transformations=[ssd_data_augmentation],\n",
        "                                         label_encoder=ssd_input_encoder,\n",
        "                                         returns={'processed_images',\n",
        "                                                  'encoded_labels'},\n",
        "                                         keep_images_without_gt=False)\n",
        "\n",
        "val_generator = val_dataset.generate(batch_size=batch_size,\n",
        "                                     shuffle=False,\n",
        "                                     transformations=[convert_to_3_channels,\n",
        "                                                      resize],\n",
        "                                     #transformations=[resize,ssd_data_augmentation],\n",
        "                                     label_encoder=ssd_input_encoder,\n",
        "                                     returns={'processed_images',\n",
        "                                              'encoded_labels'},\n",
        "                                     keep_images_without_gt=False)\n",
        "\n",
        "test_generator = test_dataset.generate(batch_size=batch_size,\n",
        "                                     shuffle=False,\n",
        "                                     transformations=[convert_to_3_channels,\n",
        "                                                      resize],\n",
        "                                     label_encoder=ssd_input_encoder,\n",
        "                                     returns={'processed_images',\n",
        "                                              'encoded_labels'},\n",
        "                                     keep_images_without_gt=False)\n",
        "\n",
        "# Get the number of samples in the training and validations datasets.\n",
        "train_dataset_size = train_dataset.get_dataset_size()\n",
        "val_dataset_size   = val_dataset.get_dataset_size()\n",
        "test_dataset_size   = test_dataset.get_dataset_size()\n",
        "\n",
        "\n",
        "print(\"Number of images in the training dataset:\\t{:>6}\".format(train_dataset_size))\n",
        "print(\"Number of images in the validation dataset:\\t{:>6}\".format(val_dataset_size))\n",
        "print(\"Number of images in the test dataset:\\t{:>6}\".format(test_dataset_size))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of images in the training dataset:\t   506\n",
            "Number of images in the validation dataset:\t    82\n",
            "Number of images in the test dataset:\t    86\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sh5uxXaEefk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define a learning rate schedule.\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "    if epoch < 80:\n",
        "        return 0.001\n",
        "    elif epoch < 100:\n",
        "        return 0.0001\n",
        "    else:\n",
        "        return 0.00001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_5UZpUNEecu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define model callbacks.\n",
        "\n",
        "# TODO: Set the filepath under which you want to save the model.\n",
        "model_checkpoint = ModelCheckpoint(filepath='ssd300_deepfashion_epoch-{epoch:02d}_loss-{loss:.4f}_val_loss-{val_loss:.4f}.h5',\n",
        "                                   monitor='val_loss',\n",
        "                                   verbose=1,\n",
        "                                   save_best_only=True,\n",
        "                                   save_weights_only=False,\n",
        "                                   mode='auto',\n",
        "                                   period=1)\n",
        "#model_checkpoint.best = \n",
        "\n",
        "csv_logger = CSVLogger(filename='ssd300_deepfashion_training_log.csv',\n",
        "                       separator=',',\n",
        "                       append=True)\n",
        "\n",
        "learning_rate_scheduler = LearningRateScheduler(schedule=lr_schedule,\n",
        "                                                verbose=1)\n",
        "\n",
        "terminate_on_nan = TerminateOnNaN()\n",
        "\n",
        "callbacks = [model_checkpoint,\n",
        "             csv_logger,\n",
        "             learning_rate_scheduler,\n",
        "             terminate_on_nan]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElkDzAP2Eeav",
        "colab_type": "code",
        "outputId": "71d80f4a-c2ee-45a3-c067-fa055b1c1a44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# If you're resuming a previous training, set `initial_epoch` and `final_epoch` accordingly.\n",
        "initial_epoch   = 0\n",
        "#final_epoch     = 20 #120 10:20 pm,oct 6\n",
        "final_epoch     = 10 #120\n",
        "steps_per_epoch = 50 #1000 10:36 pm ,oct 6\n",
        "\n",
        "history = model.fit_generator(generator=train_generator,\n",
        "                              steps_per_epoch=steps_per_epoch,\n",
        "                              epochs=final_epoch,\n",
        "                              callbacks=callbacks,\n",
        "                              validation_data=val_generator,\n",
        "                              validation_steps=ceil(val_dataset_size/batch_size),\n",
        "                              initial_epoch=initial_epoch)\n",
        "\n",
        "history"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.001.\n",
            "50/50 [==============================] - 48s 966ms/step - loss: 5227.5400 - acc: 0.2199 - val_loss: 9.8831 - val_acc: 0.3398\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 9.88307, saving model to ssd300_deepfashion_epoch-01_loss-5227.5400_val_loss-9.8831.h5\n",
            "Epoch 2/10\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.001.\n",
            "50/50 [==============================] - 40s 808ms/step - loss: 9.8834 - acc: 0.3401 - val_loss: 8.5557 - val_acc: 0.5083\n",
            "\n",
            "Epoch 00002: val_loss improved from 9.88307 to 8.55574, saving model to ssd300_deepfashion_epoch-02_loss-9.8902_val_loss-8.5557.h5\n",
            "Epoch 3/10\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.001.\n",
            "50/50 [==============================] - 40s 804ms/step - loss: 8.9784 - acc: 0.4401 - val_loss: 8.0812 - val_acc: 0.5031\n",
            "\n",
            "Epoch 00003: val_loss improved from 8.55574 to 8.08122, saving model to ssd300_deepfashion_epoch-03_loss-8.9866_val_loss-8.0812.h5\n",
            "Epoch 4/10\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.001.\n",
            "50/50 [==============================] - 40s 808ms/step - loss: 8.2806 - acc: 0.4352 - val_loss: 7.4696 - val_acc: 0.2968\n",
            "\n",
            "Epoch 00004: val_loss improved from 8.08122 to 7.46956, saving model to ssd300_deepfashion_epoch-04_loss-8.2862_val_loss-7.4696.h5\n",
            "Epoch 5/10\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.001.\n",
            "50/50 [==============================] - 40s 802ms/step - loss: 7.5288 - acc: 0.4375 - val_loss: 7.2543 - val_acc: 0.5355\n",
            "\n",
            "Epoch 00005: val_loss improved from 7.46956 to 7.25427, saving model to ssd300_deepfashion_epoch-05_loss-7.5295_val_loss-7.2543.h5\n",
            "Epoch 6/10\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.001.\n",
            "50/50 [==============================] - 40s 801ms/step - loss: 7.5735 - acc: 0.5017 - val_loss: 7.1325 - val_acc: 0.2758\n",
            "\n",
            "Epoch 00006: val_loss improved from 7.25427 to 7.13249, saving model to ssd300_deepfashion_epoch-06_loss-7.5522_val_loss-7.1325.h5\n",
            "Epoch 7/10\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.001.\n",
            "50/50 [==============================] - 40s 805ms/step - loss: 7.6663 - acc: 0.4397 - val_loss: 6.9866 - val_acc: 0.5433\n",
            "\n",
            "Epoch 00007: val_loss improved from 7.13249 to 6.98664, saving model to ssd300_deepfashion_epoch-07_loss-7.6654_val_loss-6.9866.h5\n",
            "Epoch 8/10\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.001.\n",
            "50/50 [==============================] - 40s 802ms/step - loss: 7.2064 - acc: 0.4404 - val_loss: 6.8506 - val_acc: 0.3092\n",
            "\n",
            "Epoch 00008: val_loss improved from 6.98664 to 6.85060, saving model to ssd300_deepfashion_epoch-08_loss-7.2128_val_loss-6.8506.h5\n",
            "Epoch 9/10\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.001.\n",
            "50/50 [==============================] - 40s 803ms/step - loss: 7.3009 - acc: 0.4010 - val_loss: 6.8897 - val_acc: 0.3989\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 6.85060\n",
            "Epoch 10/10\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.001.\n",
            "20/50 [===========>..................] - ETA: 23s - loss: 7.3510 - acc: 0.2874"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:364: RuntimeWarning: invalid value encountered in true_divide\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "50/50 [==============================] - 40s 802ms/step - loss: 7.3780 - acc: 0.3492 - val_loss: 6.9320 - val_acc: 0.4346\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 6.85060\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5860bf6208>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hySN0zTVQHv3",
        "colab_type": "text"
      },
      "source": [
        "6. Make predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzVT5dzQnWc4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('my_modelssd.h5')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnlpjZdInxWA",
        "colab_type": "code",
        "outputId": "b3dfc570-e9f5-4571-c1e0-695077d8618c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('my_modelssd.h5')# download the csv\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------\n",
            "Exception happened during processing of request from ('::ffff:127.0.0.1', 56826, 0, 0)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 320, in _handle_request_noblock\n",
            "    self.process_request(request, client_address)\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 351, in process_request\n",
            "    self.finish_request(request, client_address)\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 364, in finish_request\n",
            "    self.RequestHandlerClass(request, client_address, self)\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 724, in __init__\n",
            "    self.handle()\n",
            "  File \"/usr/lib/python3.6/http/server.py\", line 418, in handle\n",
            "    self.handle_one_request()\n",
            "  File \"/usr/lib/python3.6/http/server.py\", line 406, in handle_one_request\n",
            "    method()\n",
            "  File \"/usr/lib/python3.6/http/server.py\", line 639, in do_GET\n",
            "    self.copyfile(f, self.wfile)\n",
            "  File \"/usr/lib/python3.6/http/server.py\", line 800, in copyfile\n",
            "    shutil.copyfileobj(source, outputfile)\n",
            "  File \"/usr/lib/python3.6/shutil.py\", line 82, in copyfileobj\n",
            "    fdst.write(buf)\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 803, in write\n",
            "    self._sock.sendall(b)\n",
            "ConnectionResetError: [Errno 104] Connection reset by peer\n",
            "----------------------------------------\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAvv-FpsEeYn",
        "colab_type": "code",
        "outputId": "d312cc9b-5f8e-45a9-bf8e-0b131978598f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# 1: Set the generator for the predictions.\n",
        "\n",
        "predict_generator = test_dataset.generate(batch_size=2,\n",
        "                                         shuffle=True,\n",
        "                                         transformations=[convert_to_3_channels,\n",
        "                                                          resize],\n",
        "                                         label_encoder=None,\n",
        "                                         returns={'processed_images',\n",
        "                                                  'filenames',\n",
        "                                                  'inverse_transform',\n",
        "                                                  'original_images',\n",
        "                                                  'original_labels'},\n",
        "                                         keep_images_without_gt=False)\n",
        "\n",
        "predict_generator"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object DataGenerator.generate at 0x7f5860245d58>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVoOyWf1EeWb",
        "colab_type": "code",
        "outputId": "5737249f-d39c-433b-f5a1-64bd505d5076",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "# 2: Generate samples.\n",
        "\n",
        "batch_images, batch_filenames, batch_inverse_transforms, batch_original_images, batch_original_labels = next(predict_generator)\n",
        "\n",
        "i = 0 # Which batch item to look at\n",
        "\n",
        "\n",
        "print(\"Image:\", batch_filenames[i])\n",
        "print()\n",
        "print(\"Ground truth boxes:\\n\")\n",
        "print(np.array(batch_original_labels[i]))\n",
        "print(len(batch_original_labels))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Image: /content/drive/My Drive/Capstone project/SSD/Testing/Asymmetrical_Open-Front_Blazer/img_00000083.jpg\n",
            "\n",
            "Ground truth boxes:\n",
            "\n",
            "[[  1  47 203  57 273]]\n",
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McUtAc9zEeTH",
        "colab_type": "code",
        "outputId": "a9790db3-2f83-4d1c-c02a-11bb4819e7c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "# 3: Make predictions.\n",
        "\n",
        "y_pred = model.predict(batch_images)\n",
        "print(len(y_pred))\n",
        "y_pred[0][0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.8715678 ,  0.10205435,  0.01033146,  0.01604639,  0.74546003,\n",
              "       -0.30303344, -0.586821  , -0.26634675,  0.01333333,  0.01333333,\n",
              "        0.1       ,  0.1       ,  0.1       ,  0.1       ,  0.2       ,\n",
              "        0.2       ], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIyicFBJEeQx",
        "colab_type": "code",
        "outputId": "4a2536dd-08ad-4081-c350-900d39094b03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# 4: Decode the raw predictions in `y_pred`.\n",
        "\n",
        "y_pred_decoded = decode_detections(y_pred,\n",
        "                                   confidence_thresh=0.5,\n",
        "                                   iou_threshold=0.4,\n",
        "                                   top_k=200,\n",
        "                                   normalize_coords=normalize_coords,\n",
        "                                   img_height=img_height,\n",
        "                                   img_width=img_width)\n",
        "print(len(y_pred_decoded))\n",
        "y_pred_decoded"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([], dtype=float64), array([], dtype=float64)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rja68tIEeOn",
        "colab_type": "code",
        "outputId": "784eaab1-fd42-4943-f04d-6441658774b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "# 5: Convert the predictions for the original image.\n",
        "\n",
        "y_pred_decoded_inv = apply_inverse_transforms(y_pred_decoded, batch_inverse_transforms)\n",
        "\n",
        "np.set_printoptions(precision=2, suppress=True, linewidth=90)\n",
        "print(\"Predicted boxes:\\n\")\n",
        "print('   class   conf xmin   ymin   xmax   ymax')\n",
        "print(y_pred_decoded_inv[i])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted boxes:\n",
            "\n",
            "   class   conf xmin   ymin   xmax   ymax\n",
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RleskUmEeMJ",
        "colab_type": "code",
        "outputId": "7ef1fe61-ebcb-4698-ac26-117321f3b502",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 720
        }
      },
      "source": [
        "# 5: Draw the predicted boxes onto the image\n",
        "\n",
        "# Set the colors for the bounding boxes\n",
        "colors = plt.cm.hsv(np.linspace(0, 1, n_classes+1)).tolist()\n",
        "##\n",
        "\"\"\"\n",
        "classes = ['Abstract_Dot_Off-the-Shoulder_Top', 'Abstract_Floral_Fringe_Crop_Top', 'Abstract_Floral_Print_Poncho', 'Abstract-Plaid_Ruffled_Bell_Sleeve_Top',\n",
        "             'Abstract-Quilted_Drawstring_Hoodie','Acid_Wash_Jeggings', 'Asymmetrical_Open-Front_Blazer', 'Button-Tab_Ankle_Chinos', 'Tile_Print_Drawstring_Caftan', 'Tonal_Plaid_Robe',\n",
        "               'Topstitched_Jodhpurs', 'Zippered_Sleeveless_Hoodie']\"\"\"\n",
        "\n",
        "classes = ['TOP','BOTTOM','FULL DRESS'] # 1 is Top, 2 is Bottom, 3 is Full dress\n",
        "\n",
        "plt.figure(figsize=(20,12))\n",
        "plt.imshow(batch_original_images[i])\n",
        "\n",
        "current_axis = plt.gca()\n",
        "\n",
        "for box in batch_original_labels[i]:\n",
        "    xmin = box[1]\n",
        "    ymin = box[3]\n",
        "    xmax = box[2]\n",
        "   \n",
        "    #ymin = box[2]\n",
        "    #xmax = box[3]\n",
        "    ymax = box[4]\n",
        "    print(\"original\",xmin,ymin,xmax,ymax)\n",
        "    label = '{}'.format(classes[int(box[0])])\n",
        "    current_axis.add_patch(plt.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, color='green', fill=False, linewidth=2)) \n",
        "    #current_axis.add_patch(plt.Rectangle((xmin, xmax), xmax-xmin, ymax-ymin, color='green', fill=False, linewidth=2)) \n",
        "    current_axis.text(xmin, ymin, label, size='x-large', color='white', bbox={'facecolor':'green', 'alpha':1.0})\n",
        "\n",
        "for box in y_pred_decoded_inv[i]:\n",
        "    xmin = box[2]\n",
        "    ymin = box[3]\n",
        "    xmax = box[4]\n",
        "    ymax = box[5]\n",
        "    print(\"decoded\",box[2:5])\n",
        "    color = colors[int(box[0])]\n",
        "    label = '{}: {:.2f}'.format(classes[int(box[0])], box[1])\n",
        "    current_axis.add_patch(plt.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, color='red', fill=False, linewidth=2))  \n",
        "    current_axis.text(xmin, ymin, label, size='x-large', color='white', bbox={'facecolor':'red', 'alpha':1.0})"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original 47 57 203 273\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAksAAAKvCAYAAAB6aIuWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4f8DycrGnTA4WsIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1440x864 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLjBj4n0bNmp",
        "colab_type": "text"
      },
      "source": [
        "MAKE INFERENCE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMZ-2SGPbNTO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "K.clear_session() # Clear previous models from memory.\n",
        "\n",
        "model = ssd_300(image_size=(img_height, img_width, 3),\n",
        "                n_classes=12,\n",
        "                mode='inference',\n",
        "                l2_regularization=0.0005,\n",
        "                scales=[0.1, 0.2, 0.37, 0.54, 0.71, 0.88, 1.05], # The scales for MS COCO are [0.07, 0.15, 0.33, 0.51, 0.69, 0.87, 1.05]\n",
        "                aspect_ratios_per_layer=[[1.0, 2.0, 0.5],\n",
        "                                         [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
        "                                         [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
        "                                         [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
        "                                         [1.0, 2.0, 0.5],\n",
        "                                         [1.0, 2.0, 0.5]],\n",
        "                two_boxes_for_ar1=True,\n",
        "                steps=[8, 16, 32, 64, 100, 300],\n",
        "                offsets=[0.5, 0.5, 0.5, 0.5, 0.5, 0.5],\n",
        "                clip_boxes=False,\n",
        "                variances=[0.1, 0.1, 0.2, 0.2],\n",
        "                normalize_coords=True,\n",
        "                subtract_mean=[123, 117, 104],\n",
        "                swap_channels=[2, 1, 0],\n",
        "                confidence_thresh=0.5,\n",
        "                iou_threshold=0.45,\n",
        "                top_k=200,\n",
        "                nms_max_output_size=400)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0eckuhKbNQp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "import glob\n",
        "from keras.preprocessing import image\n",
        "\n",
        "orig_images = [] # Store the images here.\n",
        "input_images = [] # Store resized versions of the images here.\n",
        "\n",
        "# We'll only load one image in this example.\n",
        "img_path = '/content/drive/My Drive/Capstone project/SSD/Testing/Topstitched_Jodhpurs/img_00000007.jpg'\n",
        "\n",
        "orig_images.append(imread(img_path))\n",
        "img = image.load_img(img_path, target_size=(img_height, img_width))\n",
        "img = image.img_to_array(img) \n",
        "input_images.append(img)\n",
        "input_images = np.array(input_images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZszjyBdojGP_",
        "colab_type": "code",
        "outputId": "4407c51e-7357-4548-bdbd-09ff5d11e92f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 720
        }
      },
      "source": [
        "plt.figure(figsize=(20,12))\n",
        "plt.imshow(orig_images[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f58540a7080>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 147
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAd4AAAKvCAYAAADECK46AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 1440x864 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtVoyEH2bNN8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = model.predict(input_images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xx9klKTFbNLR",
        "colab_type": "code",
        "outputId": "5e48324b-8ef0-4215-a35a-1a6930ada53c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "confidence_threshold = 0.5\n",
        "\n",
        "y_pred_thresh = [y_pred[k][y_pred[k,:,1] > confidence_threshold] for k in range(y_pred.shape[0])]\n",
        "\n",
        "np.set_printoptions(precision=2, suppress=True, linewidth=90)\n",
        "print(\"Predicted boxes:\\n\")\n",
        "print('   class   conf xmin   ymin   xmax   ymax')\n",
        "print(y_pred_thresh[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted boxes:\n",
            "\n",
            "   class   conf xmin   ymin   xmax   ymax\n",
            "[[ 1.00e+00  1.00e+00  6.51e+02  4.02e+02  6.51e+02  4.07e+02]\n",
            " [ 1.00e+00  1.00e+00  5.57e+02 -3.45e+06  5.57e+02  3.45e+06]\n",
            " [ 1.00e+00  1.00e+00  5.18e+02 -1.68e+07  5.18e+02  1.68e+07]\n",
            " ...\n",
            " [ 3.00e+00  1.00e+00 -9.00e+18  3.84e+01  9.00e+18  3.84e+01]\n",
            " [ 3.00e+00  1.00e+00 -7.35e+02  8.20e+01 -7.12e+02  8.24e+01]\n",
            " [ 3.00e+00  1.00e+00 -4.88e+16  1.78e+02  4.88e+16  1.78e+02]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqSZGbqmbNIM",
        "colab_type": "code",
        "outputId": "1c7915cf-f83f-418c-ab2f-c6d544612184",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 604
        }
      },
      "source": [
        "# Display the image and draw the predicted boxes onto it.\n",
        "\n",
        "# Set the colors for the bounding boxes\n",
        "colors = plt.cm.hsv(np.linspace(0, 1, 21)).tolist()\n",
        "\n",
        "classes = ['TOP','BOTTOM','FULL DRESS']\n",
        "\n",
        "plt.figure(figsize=(20,12))\n",
        "plt.imshow(orig_images[0])\n",
        "\n",
        "current_axis = plt.gca()\n",
        "\n",
        "for box in y_pred_thresh[0]:\n",
        "    # Transform the predicted bounding boxes for the 300x300 image to the original image dimensions.\n",
        "    xmin = box[2] * orig_images[0].shape[1] / img_width\n",
        "    #print(\"Xmin:\",xmin)\n",
        "    ymin = box[3] * orig_images[0].shape[0] / img_height\n",
        "    #print(\"ymin:\",ymin)\n",
        "    xmax = box[4] * orig_images[0].shape[1] / img_width\n",
        "    #print(\"Xmax:\",xmax)\n",
        "    ymax = box[5] * orig_images[0].shape[0] / img_height\n",
        "    #print(\"ymax:\",ymax)\n",
        "    color = colors[int(box[0])]\n",
        "    label = '{}: {:.2f}'.format(classes[int(box[0])], box[1])\n",
        "    current_axis.add_patch(plt.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, color=color, fill=False, linewidth=2))  \n",
        "    current_axis.text(xmin, ymin, label, size='x-large', color='white', bbox={'facecolor':color, 'alpha':1.0})"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-150-679099abb692>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m#print(\"ymax:\",ymax)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'{}: {:.2f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mcurrent_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_patch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRectangle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mymin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxmax\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mxmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mymax\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mymin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mcurrent_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mymin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'x-large'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'white'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'facecolor'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'alpha'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    332\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m             \u001b[0;31m# Finally look for special method names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(fig)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'png'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'retina'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'png2x'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mretina_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0mbytes_io\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_io\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfmt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'svg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m   2073\u001b[0m                     \u001b[0morientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2074\u001b[0m                     \u001b[0mbbox_inches_restore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_bbox_inches_restore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2075\u001b[0;31m                     **kwargs)\n\u001b[0m\u001b[1;32m   2076\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2077\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbbox_inches\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mrestore_bbox\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m         \"\"\"\n\u001b[0;32m--> 510\u001b[0;31m         \u001b[0mFigureCanvasAgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m         \u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_renderer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0mDraw\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfigure\u001b[0m \u001b[0musing\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \"\"\"\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_renderer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleared\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m         \u001b[0;31m# acquire a lock on the shared font cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0mRendererAgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mget_renderer\u001b[0;34m(self, cleared)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mneed_new_renderer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRendererAgg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lastKey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mcleared\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, width, height, dpi)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_renderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_RendererAgg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filter_renderers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Image size of 0x1965387776 pixels is too large. It must be less than 2^16 in each direction."
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x864 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgNAlOOAbNFA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hh4AY3d8EeJb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kkqmw0s5EeGv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}